{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ba2e640d-2908-4fd0-bdf3-78f0c301dd65",
   "metadata": {},
   "source": [
    "# MODEL TESTING\n",
    "---\n",
    "---\n",
    "### *Commented out models to prevent run times; results are in the commented out portions.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b4b83d0-7973-42c4-8713-7c6cc5dc39bb",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    "---\n",
    "## [Average Cycling Model Testing](#Average-Cycling-Performance-Model-Testing)\n",
    "> ### [Average Cycling Best Model](#Average-Cycling-Performance-Best-Model---XGBoost)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3237b7f-7fc2-4010-b60b-c7d880f28dd7",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Sources and Adaptations From\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9e3e2a61-2500-4f10-9091-992cc2485733",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Various modeling lessons and breakfast hours from DSI 523\n",
    "# https://medium.com/towards-data-science/loss-functions-and-their-use-in-neural-networks-a470e703f1e9\n",
    "# https://www.tensorflow.org/api_docs/python/tf/keras/losses/MeanAbsoluteError\n",
    "# https://www.tensorflow.org/api_docs/python/tf/keras/losses/Huber\n",
    "# https://towardsdatascience.com/what-is-batch-normalization-46058b4f583\n",
    "# https://www.tensorflow.org/tutorials/keras/save_and_load#save_the_entire_model\n",
    "# https://machinelearningmastery.com/xgboost-for-regression/\n",
    "# https://towardsdatascience.com/getting-started-with-xgboost-in-scikit-learn-f69f5f470a97"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9509e54f-20d0-4638-aebd-93d7a93be9f1",
   "metadata": {},
   "source": [
    "# Imports\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b828ad9a-87a1-4fcc-ab0b-1a27c6ba6159",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "pd.set_option('display.max_columns', None)\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import PolynomialFeatures, StandardScaler\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import Ridge, Lasso, ElasticNet\n",
    "from sklearn.ensemble import GradientBoostingRegressor, AdaBoostRegressor, VotingRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import AdaBoostRegressor, RandomForestRegressor\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.regularizers import l1, l2, l1_l2\n",
    "from tensorflow.keras.losses import MeanAbsoluteError, Huber\n",
    "\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ca0ce27d-3d8f-42de-9ea2-616dbf621a46",
   "metadata": {},
   "outputs": [],
   "source": [
    "a_df = pd.read_csv('../data/average/a_df.csv')\n",
    "h_df = pd.read_csv('../data/high/h_df.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16db770f-e93d-4bfb-9395-8176710ffbd9",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Average Cycling Performance Model Testing\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6ab7c15d-bf3f-4083-b508-3cd326ce0bd1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>elevation</th>\n",
       "      <th>dt</th>\n",
       "      <th>heart_rate</th>\n",
       "      <th>bearing</th>\n",
       "      <th>time_diff_s</th>\n",
       "      <th>total_time_s</th>\n",
       "      <th>ele_diff_m</th>\n",
       "      <th>total_ele_change_m</th>\n",
       "      <th>lat_lon</th>\n",
       "      <th>dist_diff_km</th>\n",
       "      <th>total_dist_km</th>\n",
       "      <th>temp</th>\n",
       "      <th>feels_like</th>\n",
       "      <th>pressure</th>\n",
       "      <th>humidity</th>\n",
       "      <th>dew_point</th>\n",
       "      <th>clouds</th>\n",
       "      <th>wind_speed</th>\n",
       "      <th>wind_deg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2022-07-20 16:07:45+00:00</td>\n",
       "      <td>38.773466</td>\n",
       "      <td>-121.363686</td>\n",
       "      <td>35.799999</td>\n",
       "      <td>1658333265</td>\n",
       "      <td>78</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>(38.77346634864807, -121.36368582956493)</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>297.65</td>\n",
       "      <td>297.17</td>\n",
       "      <td>1019</td>\n",
       "      <td>39</td>\n",
       "      <td>282.80</td>\n",
       "      <td>1</td>\n",
       "      <td>0.45</td>\n",
       "      <td>177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2022-07-20 16:07:46+00:00</td>\n",
       "      <td>38.773542</td>\n",
       "      <td>-121.363672</td>\n",
       "      <td>35.599998</td>\n",
       "      <td>1658333266</td>\n",
       "      <td>79</td>\n",
       "      <td>8.292053</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.200001</td>\n",
       "      <td>-0.2</td>\n",
       "      <td>(38.77354153431952, -121.36367183178663)</td>\n",
       "      <td>0.0084</td>\n",
       "      <td>0.0084</td>\n",
       "      <td>297.65</td>\n",
       "      <td>297.17</td>\n",
       "      <td>1019</td>\n",
       "      <td>39</td>\n",
       "      <td>282.80</td>\n",
       "      <td>1</td>\n",
       "      <td>0.45</td>\n",
       "      <td>177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2022-07-20 16:07:49+00:00</td>\n",
       "      <td>38.773630</td>\n",
       "      <td>-121.363682</td>\n",
       "      <td>35.200001</td>\n",
       "      <td>1658333269</td>\n",
       "      <td>82</td>\n",
       "      <td>-5.321180</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>-0.399998</td>\n",
       "      <td>-0.6</td>\n",
       "      <td>(38.77363029867411, -121.36368239298463)</td>\n",
       "      <td>0.0099</td>\n",
       "      <td>0.0183</td>\n",
       "      <td>297.65</td>\n",
       "      <td>297.17</td>\n",
       "      <td>1019</td>\n",
       "      <td>39</td>\n",
       "      <td>282.80</td>\n",
       "      <td>1</td>\n",
       "      <td>0.45</td>\n",
       "      <td>177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2022-07-20 16:07:51+00:00</td>\n",
       "      <td>38.773789</td>\n",
       "      <td>-121.363733</td>\n",
       "      <td>35.000000</td>\n",
       "      <td>1658333271</td>\n",
       "      <td>83</td>\n",
       "      <td>-13.956066</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>-0.200001</td>\n",
       "      <td>-0.8</td>\n",
       "      <td>(38.77378871664405, -121.36373268440366)</td>\n",
       "      <td>0.0181</td>\n",
       "      <td>0.0364</td>\n",
       "      <td>297.67</td>\n",
       "      <td>297.17</td>\n",
       "      <td>1019</td>\n",
       "      <td>38</td>\n",
       "      <td>282.43</td>\n",
       "      <td>1</td>\n",
       "      <td>0.45</td>\n",
       "      <td>177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2022-07-20 16:07:52+00:00</td>\n",
       "      <td>38.773786</td>\n",
       "      <td>-121.363766</td>\n",
       "      <td>35.000000</td>\n",
       "      <td>1658333272</td>\n",
       "      <td>83</td>\n",
       "      <td>-96.936537</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.8</td>\n",
       "      <td>(38.77378553152084, -121.36376612819731)</td>\n",
       "      <td>0.0029</td>\n",
       "      <td>0.0393</td>\n",
       "      <td>297.67</td>\n",
       "      <td>297.17</td>\n",
       "      <td>1019</td>\n",
       "      <td>38</td>\n",
       "      <td>282.43</td>\n",
       "      <td>1</td>\n",
       "      <td>0.45</td>\n",
       "      <td>177</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   timestamp   latitude   longitude  elevation          dt  \\\n",
       "0  2022-07-20 16:07:45+00:00  38.773466 -121.363686  35.799999  1658333265   \n",
       "1  2022-07-20 16:07:46+00:00  38.773542 -121.363672  35.599998  1658333266   \n",
       "2  2022-07-20 16:07:49+00:00  38.773630 -121.363682  35.200001  1658333269   \n",
       "3  2022-07-20 16:07:51+00:00  38.773789 -121.363733  35.000000  1658333271   \n",
       "4  2022-07-20 16:07:52+00:00  38.773786 -121.363766  35.000000  1658333272   \n",
       "\n",
       "   heart_rate    bearing  time_diff_s  total_time_s  ele_diff_m  \\\n",
       "0          78   0.000000            0             0    0.000000   \n",
       "1          79   8.292053            1             1   -0.200001   \n",
       "2          82  -5.321180            3             4   -0.399998   \n",
       "3          83 -13.956066            2             6   -0.200001   \n",
       "4          83 -96.936537            1             7    0.000000   \n",
       "\n",
       "   total_ele_change_m                                   lat_lon  dist_diff_km  \\\n",
       "0                 0.0  (38.77346634864807, -121.36368582956493)        0.0000   \n",
       "1                -0.2  (38.77354153431952, -121.36367183178663)        0.0084   \n",
       "2                -0.6  (38.77363029867411, -121.36368239298463)        0.0099   \n",
       "3                -0.8  (38.77378871664405, -121.36373268440366)        0.0181   \n",
       "4                -0.8  (38.77378553152084, -121.36376612819731)        0.0029   \n",
       "\n",
       "   total_dist_km    temp  feels_like  pressure  humidity  dew_point  clouds  \\\n",
       "0         0.0000  297.65      297.17      1019        39     282.80       1   \n",
       "1         0.0084  297.65      297.17      1019        39     282.80       1   \n",
       "2         0.0183  297.65      297.17      1019        39     282.80       1   \n",
       "3         0.0364  297.67      297.17      1019        38     282.43       1   \n",
       "4         0.0393  297.67      297.17      1019        38     282.43       1   \n",
       "\n",
       "   wind_speed  wind_deg  \n",
       "0        0.45       177  \n",
       "1        0.45       177  \n",
       "2        0.45       177  \n",
       "3        0.45       177  \n",
       "4        0.45       177  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4acc5f0b-46be-4d35-87ce-5e50e80274f6",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## X, y, train_test_split\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1422d14a-0130-44c4-afe3-74ee147cd7a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['timestamp', 'latitude', 'longitude', 'elevation', 'dt', 'heart_rate',\n",
       "       'bearing', 'time_diff_s', 'total_time_s', 'ele_diff_m',\n",
       "       'total_ele_change_m', 'lat_lon', 'dist_diff_km', 'total_dist_km',\n",
       "       'temp', 'feels_like', 'pressure', 'humidity', 'dew_point', 'clouds',\n",
       "       'wind_speed', 'wind_deg'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a7112d65-747f-47d3-a69d-5a905518bddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "a_features = ['elevation', 'bearing', 'time_diff_s', 'total_time_s', 'ele_diff_m',\n",
    "       'total_ele_change_m', 'dist_diff_km', 'total_dist_km',\n",
    "       'temp', 'feels_like', 'pressure', 'humidity', 'dew_point', 'clouds',\n",
    "       'wind_speed', 'wind_deg']\n",
    "a_X = a_df[a_features]\n",
    "a_y = a_df['heart_rate']\n",
    "\n",
    "a_X_train, a_X_test, a_y_train, a_y_test = train_test_split(a_X, a_y, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "58b0ddc5-1911-439c-89ec-342d06ab9dfe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "130.21652421652422"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a_y.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5eab97a-12cc-46e7-b1bb-b0222df5cd69",
   "metadata": {},
   "source": [
    "### StandardScaler X_train and X_test for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8902c045-7211-4157-9248-2fb9e0426418",
   "metadata": {},
   "outputs": [],
   "source": [
    "a_ss = StandardScaler()\n",
    "a_X_train_sc = a_ss.fit_transform(a_X_train)\n",
    "a_X_test_sc = a_ss.transform(a_X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04b67ac0-6d16-4efe-b05c-b03bcea45e88",
   "metadata": {},
   "source": [
    "### Polynomial X_train and X_test for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4f429dcf-6b97-424d-ac99-0f12184d3266",
   "metadata": {},
   "outputs": [],
   "source": [
    "a_poly = PolynomialFeatures()\n",
    "a_X_train_sc_p = a_poly.fit_transform(a_X_train_sc)\n",
    "a_X_test_sc_p = a_poly.fit_transform(a_X_test_sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e10c0a5a-9580-4cb1-8a6b-d50c38e4879a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Linear Regression\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2308b699-e547-4eb1-914d-5d06f13712bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nLinear Regression Train R2 Score: 0.5260233155015783\\nLinear Regression Test R2 Score: 0.5322513167399172\\n'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "a_lr = LinearRegression()\n",
    "a_lr.fit(a_X_train_sc, a_y_train)\n",
    "print(f'Linear Regression Train R2 Score: {a_lr.score(a_X_train_sc, a_y_train)}')\n",
    "print(f'Linear Regression Test R2 Score: {a_lr.score(a_X_test_sc, a_y_test)}')\n",
    "'''\n",
    "'''\n",
    "Linear Regression Train R2 Score: 0.5260233155015783\n",
    "Linear Regression Test R2 Score: 0.5322513167399172\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a3eba4a-d843-40fe-894f-437fc51e4da6",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Linear Regression Pipeline (StandardScaler, Polynomial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7dbc010d-fd67-4801-ad74-b6108a94f87b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nLR Pipe Train R2 Score: 0.8082482755481244\\nLR Pipe Test R2 Score: 0.7906546450250773\\n'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "a_lr_pipe = Pipeline([\n",
    "    ('a_ss', StandardScaler()),\n",
    "    ('a_poly', PolynomialFeatures()),\n",
    "    ('a_lr', LinearRegression())\n",
    "])\n",
    "\n",
    "a_lr_pipe.fit(a_X_train, a_y_train)\n",
    "print(f'LR Pipe Train R2 Score: {a_lr_pipe.score(a_X_train, a_y_train)}')\n",
    "print(f'LR Pipe Test R2 Score: {a_lr_pipe.score(a_X_test, a_y_test)}')\n",
    "'''\n",
    "'''\n",
    "LR Pipe Train R2 Score: 0.8082482755481244\n",
    "LR Pipe Test R2 Score: 0.7906546450250773\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe5836ba-85f8-42db-af96-34667942de0f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Regressor Boosting\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c3242f8-96db-41b8-9fca-e0b32a8aa4cc",
   "metadata": {
    "tags": []
   },
   "source": [
    "### AdaBoost with Linear Regression Base Estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "968bf2bf-2df7-48f1-9bfc-0f59053ef06c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nAda LR Train R2 Score: 0.8046423913777858\\nAda LR Test R2 Score: 0.776694380832758\\n'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "a_ada_lr = AdaBoostRegressor(base_estimator = LinearRegression(), random_state = 42)\n",
    "\n",
    "a_ada_lr.fit(a_X_train_sc_p, a_y_train)\n",
    "print(f'Ada LR Train R2 Score: {a_ada_lr.score(a_X_train_sc_p, a_y_train)}')\n",
    "print(f'Ada LR Test R2 Score: {a_ada_lr.score(a_X_test_sc_p, a_y_test)}')\n",
    "'''\n",
    "'''\n",
    "Ada LR Train R2 Score: 0.8046423913777858\n",
    "Ada LR Test R2 Score: 0.776694380832758\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4dcc69d-c9c3-47cb-a6e9-a1e24411836d",
   "metadata": {
    "tags": []
   },
   "source": [
    "### AdaBoost with Decision Tree Base Estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fc0afdc7-652b-4526-a213-bfca4d321bb9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n0.868164587298871\\n{'base_estimator__max_depth': 5, 'n_estimators': 200}\\nAda DT Train R2 Score: 0.8756108134812447\\nAda DT Test R2 Score: 0.8730399805443708\\n\""
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "a_ada_dt = AdaBoostRegressor(base_estimator = DecisionTreeRegressor(), random_state = 42)\n",
    "\n",
    "a_ada_dt_params = {\n",
    "    'n_estimators': [50, 100, 150, 200],\n",
    "    'base_estimator__max_depth': [1, 2, 5]\n",
    "}\n",
    "\n",
    "a_gs_ada_dt = GridSearchCV(a_ada_dt, param_grid = a_ada_dt_params, cv = 5)\n",
    "a_gs_ada_dt.fit(a_X_train_sc, a_y_train)\n",
    "print(a_gs_ada_dt.best_score_)\n",
    "print(a_gs_ada_dt.best_params_)\n",
    "print(f'Ada DT Train R2 Score: {a_gs_ada_dt.score(a_X_train_sc, a_y_train)}')\n",
    "print(f'Ada DT Test R2 Score: {a_gs_ada_dt.score(a_X_test_sc, a_y_test)}')\n",
    "'''\n",
    "\n",
    "'''\n",
    "0.868164587298871\n",
    "{'base_estimator__max_depth': 5, 'n_estimators': 200}\n",
    "Ada DT Train R2 Score: 0.8756108134812447\n",
    "Ada DT Test R2 Score: 0.8730399805443708\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6155407f-7539-4cbe-bca7-8b766fd70b94",
   "metadata": {
    "tags": []
   },
   "source": [
    "### AdaBoost with Random Forest Base Estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5de35077-25ad-44a6-bb79-dc018699bd4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n0.844253394135492\\n{'base_estimator__max_depth': 5, 'n_estimators': 50}\\nAda DT Train R2 Score: 0.8526320077631239\\nAda DT Test R2 Score: 0.8512220020729748\\n\""
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "a_ada_rf = AdaBoostRegressor(base_estimator = RandomForestRegressor(), random_state = 42)\n",
    "\n",
    "a_ada_rf_params = {\n",
    "    'n_estimators': [50, 100, 150, 200],\n",
    "    'base_estimator__max_depth': [1, 2, 5]\n",
    "}\n",
    "\n",
    "a_gs_ada_rf = GridSearchCV(a_ada_rf, param_grid = a_ada_rf_params, cv = 5)\n",
    "a_gs_ada_rf.fit(a_X_train_sc, a_y_train)\n",
    "print(a_gs_ada_rf.best_score_)\n",
    "print(a_gs_ada_rf.best_params_)\n",
    "print(f'Ada DT Train R2 Score: {a_gs_ada_rf.score(a_X_train_sc, a_y_train)}')\n",
    "print(f'Ada DT Test R2 Score: {a_gs_ada_rf.score(a_X_test_sc, a_y_test)}')\n",
    "'''\n",
    "'''\n",
    "0.844253394135492\n",
    "{'base_estimator__max_depth': 5, 'n_estimators': 50}\n",
    "Ada DT Train R2 Score: 0.8526320077631239\n",
    "Ada DT Test R2 Score: 0.8512220020729748\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c920337-6425-47be-9aaa-60769c0e1f00",
   "metadata": {},
   "source": [
    "### Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1f14bbf6-8fe7-4203-ba97-93d56b5a5234",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nGridSearch Best Score: 0.964003581281308\\n{'learning_rate': 0.8, 'max_depth': 4, 'n_estimators': 150}\\nGBoost Train R2 Score: 0.9888934084696182\\nGBoost Test R2 Score: 0.9709606783777225\\nMean Absolute Error: 2.7824393977965642\\nMean Squared Error: 14.658521217024555\\n\""
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "# Instantiate\n",
    "a_gboost = GradientBoostingRegressor()\n",
    "\n",
    "a_gboost_params = {\n",
    "    'max_depth': [2, 3, 4],\n",
    "    'n_estimators': [100, 125, 150],\n",
    "    'learning_rate': [0.8, .1, .12]\n",
    "}\n",
    "\n",
    "# Gridsearch\n",
    "a_gb_gs = GridSearchCV(a_gboost, param_grid = a_gboost_params, cv = 5)\n",
    "a_gb_gs.fit(a_X_train_sc, a_y_train)\n",
    "\n",
    "# Metrics\n",
    "print(f'GridSearch Best Score: {a_gb_gs.best_score_}')\n",
    "print(a_gb_gs.best_params_)\n",
    "print(f'GBoost Train R2 Score: {a_gb_gs.score(a_X_train_sc, a_y_train)}')\n",
    "print(f'GBoost Test R2 Score: {a_gb_gs.score(a_X_test_sc, a_y_test)}')\n",
    "\n",
    "a_gb_y_true = a_y_test\n",
    "a_gb_y_pred = a_gb_gs.predict(a_X_test_sc)\n",
    "\n",
    "# MAE\n",
    "print(f'Mean Absolute Error: {mean_absolute_error(a_gb_y_true, a_gb_y_pred)}')\n",
    "\n",
    "# MSE\n",
    "print(f'Mean Squared Error: {mean_squared_error(a_gb_y_true, a_gb_y_pred)}')\n",
    "'''\n",
    "'''\n",
    "GridSearch Best Score: 0.964003581281308\n",
    "{'learning_rate': 0.8, 'max_depth': 4, 'n_estimators': 150}\n",
    "GBoost Train R2 Score: 0.9888934084696182\n",
    "GBoost Test R2 Score: 0.9709606783777225\n",
    "Mean Absolute Error: 2.7824393977965642\n",
    "Mean Squared Error: 14.658521217024555\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4181f707-d03f-4ee2-b37b-1118d1248c1f",
   "metadata": {},
   "source": [
    "### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bbad08d0-11cc-4eec-8800-6c20b255c9d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nGridSearch Best Score: 0.9659966627636546\\n{'learning_rate': 0.8, 'max_depth': 4, 'n_estimators': 150}\\nXGBoost Train R2 Score: 0.9888381686104007\\nXGBoost Test R2 Score: 0.9719382323708556\\nMean Absolute Error: 2.7619688805285496\\nMean Squared Error: 14.165069746789941\\n\""
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "# Instantiate\n",
    "a_xgboost = XGBRegressor()\n",
    "\n",
    "# Gridsearch\n",
    "a_xgboost_params = {\n",
    "    'max_depth': [2, 3, 4],\n",
    "    'n_estimators': [100, 125, 150],\n",
    "    'learning_rate': [0.8, .1, .12]\n",
    "}\n",
    "\n",
    "a_xgb_gs = GridSearchCV(a_xgboost, param_grid = a_xgboost_params, cv = 5)\n",
    "a_xgb_gs.fit(a_X_train_sc, a_y_train)\n",
    "\n",
    "# Metrics\n",
    "print(f'GridSearch Best Score: {a_xgb_gs.best_score_}')\n",
    "print(a_xgb_gs.best_params_)\n",
    "print(f'XGBoost Train R2 Score: {a_xgb_gs.score(a_X_train_sc, a_y_train)}')\n",
    "print(f'XGBoost Test R2 Score: {a_xgb_gs.score(a_X_test_sc, a_y_test)}')\n",
    "\n",
    "a_xgb_y_true = a_y_test\n",
    "a_xgb_y_pred = a_xgb_gs.predict(a_X_test_sc)\n",
    "\n",
    "# MAE\n",
    "print(f'Mean Absolute Error: {mean_absolute_error(a_xgb_y_true, a_xgb_y_pred)}')\n",
    "\n",
    "# MSE\n",
    "print(f'Mean Squared Error: {mean_squared_error(a_xgb_y_true, a_xgb_y_pred)}')\n",
    "'''\n",
    "'''\n",
    "GridSearch Best Score: 0.9659966627636546\n",
    "{'learning_rate': 0.8, 'max_depth': 4, 'n_estimators': 150}\n",
    "XGBoost Train R2 Score: 0.9888381686104007\n",
    "XGBoost Test R2 Score: 0.9719382323708556\n",
    "Mean Absolute Error: 2.7619688805285496\n",
    "Mean Squared Error: 14.165069746789941\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d8ca1c8-b384-47d3-996b-6f29e207438b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Neural Net Regressor\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22cacd20-80c3-48ac-bea0-c84c0c8cd227",
   "metadata": {},
   "source": [
    "### X, y, train_test_split, StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "72562d7c-5ea8-442a-9d86-9e02f13c5004",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\na_X_nn = a_df[a_features]\\na_y_nn = a_df['heart_rate']\\n\\na_X_nn = np.array(a_X_nn)\\na_y_nn = np.array(a_y_nn)\\n\\na_X_nn_train, a_X_nn_test, a_y_nn_train, a_y_nn_test = train_test_split(a_X_nn, a_y_nn, random_state = 42)\\n\\na_ss_nn = StandardScaler()\\na_X_nn_train_sc = a_ss_nn.fit_transform(a_X_nn_train)\\na_X_nn_test_sc = a_ss.transform(a_X_nn_test)\\n\""
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "a_X_nn = a_df[a_features]\n",
    "a_y_nn = a_df['heart_rate']\n",
    "\n",
    "a_X_nn = np.array(a_X_nn)\n",
    "a_y_nn = np.array(a_y_nn)\n",
    "\n",
    "a_X_nn_train, a_X_nn_test, a_y_nn_train, a_y_nn_test = train_test_split(a_X_nn, a_y_nn, random_state = 42)\n",
    "\n",
    "a_ss_nn = StandardScaler()\n",
    "a_X_nn_train_sc = a_ss_nn.fit_transform(a_X_nn_train)\n",
    "a_X_nn_test_sc = a_ss.transform(a_X_nn_test)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2407b7a2-6e73-4b0e-a006-93ffbc16cbe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a_X_nn[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ef54f2ba-d2bd-4c98-b754-af095d7e5d76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n# Instantiate\\na_model_nn = Sequential()\\n\\n# Layers\\na_model_nn.add(Dense(128, input_dim = 16, activation = 'relu'))\\n\\na_model_nn.add(BatchNormalization())\\na_model_nn.add(Dense(64, activation = 'relu', kernel_regularizer = l2(.1)))\\na_model_nn.add(Dense(128, activation = 'relu', kernel_regularizer = l2(.5)))\\na_model_nn.add(Dense(64, activation = 'relu', kernel_regularizer = l2(.1)))\\na_model_nn.add(Dense(128, activation = 'relu', kernel_regularizer = l2(.5))) \\na_model_nn.add(Dense(64, activation = 'relu', kernel_regularizer = l2(.1))) \\na_model_nn.add(Dense(1, kernel_regularizer = l2(.5)))\\n\""
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "# Instantiate\n",
    "a_model_nn = Sequential()\n",
    "\n",
    "# Layers\n",
    "a_model_nn.add(Dense(128, input_dim = 16, activation = 'relu'))\n",
    "\n",
    "a_model_nn.add(BatchNormalization())\n",
    "a_model_nn.add(Dense(64, activation = 'relu', kernel_regularizer = l2(.1)))\n",
    "a_model_nn.add(Dense(128, activation = 'relu', kernel_regularizer = l2(.5)))\n",
    "a_model_nn.add(Dense(64, activation = 'relu', kernel_regularizer = l2(.1)))\n",
    "a_model_nn.add(Dense(128, activation = 'relu', kernel_regularizer = l2(.5))) \n",
    "a_model_nn.add(Dense(64, activation = 'relu', kernel_regularizer = l2(.1))) \n",
    "a_model_nn.add(Dense(1, kernel_regularizer = l2(.5)))\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d60e4577-0412-443b-94e0-ea16b121a107",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n# compile\\na_model_nn.compile(optimizer = 'adam', loss = 'mse', metrics = ['mse'])\\n\""
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "# compile\n",
    "a_model_nn.compile(optimizer = 'adam', loss = 'mse', metrics = ['mse'])\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b676e67c-ea0c-4e85-88ea-8c8361d2fc68",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# fit\\na_history_nn = a_model_nn.fit(a_X_nn_train_sc, a_y_nn_train, epochs = 500, verbose = 1, \\n                          validation_data = (a_X_nn_test_sc, a_y_nn_test))\\n'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "# fit\n",
    "a_history_nn = a_model_nn.fit(a_X_nn_train_sc, a_y_nn_train, epochs = 500, verbose = 1, \n",
    "                          validation_data = (a_X_nn_test_sc, a_y_nn_test))\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b6adc2a6-f40b-4657-a166-7c4c4ddec36d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n#breakfast hour\\n# plot train and test loss (mse)\\nfig, ax = plt.subplots(1, 2, figsize=(15, 5))\\n\\nax[0].plot(a_history_nn.history['loss'], color = 'green')\\nax[1].plot(a_history_nn.history['val_loss'], color = 'blue')\\n\\nax[0].set_title('Train')\\nax[0].set_xlabel('epochs')\\nax[0].set_ylabel('Loss = MSE')\\n\\nax[1].set_title('Test')\\nax[1].set_xlabel('epochs')\\nax[1].set_ylabel('Loss = MSE');\\n\""
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "#breakfast hour\n",
    "# plot train and test loss (mse)\n",
    "fig, ax = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "ax[0].plot(a_history_nn.history['loss'], color = 'green')\n",
    "ax[1].plot(a_history_nn.history['val_loss'], color = 'blue')\n",
    "\n",
    "ax[0].set_title('Train')\n",
    "ax[0].set_xlabel('epochs')\n",
    "ax[0].set_ylabel('Loss = MSE')\n",
    "\n",
    "ax[1].set_title('Test')\n",
    "ax[1].set_xlabel('epochs')\n",
    "ax[1].set_ylabel('Loss = MSE');\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f7a59700-0e54-4489-b2d9-6ac968862dab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n118/118 [==============================] - 0s 1ms/step\\nTest R2: 0.9330577294184806\\nMean Absolute Error: 24.59176254272461\\nHuber Loss: 24.096338272094727\\n'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "# https://medium.com/towards-data-science/loss-functions-and-their-use-in-neural-networks-a470e703f1e9\n",
    "a_nn_y_true = a_y_nn_test\n",
    "a_nn_y_pred = a_model_nn.predict(a_X_nn_test_sc)\n",
    "\n",
    "# R2\n",
    "print(f'Test R2: {r2_score(a_nn_y_true, a_nn_y_pred)}')\n",
    "\n",
    "# MAE\n",
    "mae = MeanAbsoluteError()\n",
    "print(f'Mean Absolute Error: {mae(a_nn_y_true, a_nn_y_pred)}')\n",
    "\n",
    "# Huber\n",
    "huber = Huber()\n",
    "print(f'Huber Loss: {huber(a_nn_y_true, a_nn_y_pred)}')\n",
    "'''\n",
    "'''\n",
    "118/118 [==============================] - 0s 1ms/step\n",
    "Test R2: 0.9330577294184806\n",
    "Mean Absolute Error: 24.59176254272461\n",
    "Huber Loss: 24.096338272094727\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d0f289d-1569-471e-a074-5c8321fcb722",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Average Cycling Performance Best Model - XGBoost\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8b1eb6d-d573-44ac-a1d7-565d143fe1d1",
   "metadata": {},
   "source": [
    "## Pickle That"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a227fa4d-a3fb-4bbe-8fdb-4839b2e441de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n# Pickle a_NN\\nwith open('../models/a_model_nn.pkl', 'wb') as f:\\n    pickle.dump(a_model_nn, f)\\n\""
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "# Pickle a_NN\n",
    "with open('../models/a_model_nn.pkl', 'wb') as f:\n",
    "    pickle.dump(a_model_nn, f)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c8211e8a-a0f1-4c9e-a52e-e92a534c9b69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n# Pickle a_XGB\\nwith open('../models/a_model_xgb.pkl', 'wb') as f:\\n    pickle.dump(a_xgb_gs, f)\\n\""
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "# Pickle a_XGB\n",
    "with open('../models/a_model_xgb.pkl', 'wb') as f:\n",
    "    pickle.dump(a_xgb_gs, f)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a9cd977-53bf-42e7-965a-e201deef56bc",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# High Cycling Performance Modeling\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a24e074-bd9d-45a1-92fe-2535d9ebfb43",
   "metadata": {
    "tags": []
   },
   "source": [
    "## X, y, train_test_split\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f7921df7-2cf4-42ba-b4b1-6a0f230578be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['timestamp', 'latitude', 'longitude', 'elevation', 'dt', 'heart_rate',\n",
       "       'bearing', 'time_diff_s', 'total_time_s', 'ele_diff_m',\n",
       "       'total_ele_change_m', 'lat_lon', 'dist_diff_km', 'total_dist_km',\n",
       "       'temp', 'feels_like', 'pressure', 'humidity', 'dew_point', 'clouds',\n",
       "       'wind_speed', 'wind_deg'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "969387f3-9c8e-4c11-8be3-5222b2df2a2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "h_features = ['elevation', 'bearing', 'time_diff_s', 'total_time_s', 'ele_diff_m',\n",
    "       'total_ele_change_m', 'dist_diff_km', 'total_dist_km',\n",
    "       'temp', 'feels_like', 'pressure', 'humidity', 'dew_point', 'clouds',\n",
    "       'wind_speed', 'wind_deg']\n",
    "h_X = h_df[h_features]\n",
    "h_y = h_df['heart_rate']\n",
    "\n",
    "h_X_train, h_X_test, h_y_train, h_y_test = train_test_split(h_X, h_y, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "cc8982b8-f0bf-41e6-9a9f-dc8948dfff99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "130.53650259067356"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h_y.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e872296-5e47-4fb1-8f37-5672dea404e4",
   "metadata": {},
   "source": [
    "### StandardScaler X_train and X_test for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "253e8153-5c95-4066-b6be-eabe265fd2a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "h_ss = StandardScaler()\n",
    "h_X_train_sc = h_ss.fit_transform(h_X_train)\n",
    "h_X_test_sc = h_ss.transform(h_X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d829a19f-5c0c-4853-b565-34f35c44ae00",
   "metadata": {},
   "source": [
    "### Polynomial X_train and X_test for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e3047e1a-7a7e-47eb-a725-6c7d27c4227d",
   "metadata": {},
   "outputs": [],
   "source": [
    "h_poly = PolynomialFeatures()\n",
    "h_X_train_sc_p = h_poly.fit_transform(h_X_train_sc)\n",
    "h_X_test_sc_p = h_poly.fit_transform(h_X_test_sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63639075-3182-4879-9cdd-1470c2eebcbd",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Linear Regression\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a6d44b31-0e30-46e6-a56e-989200e36618",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nLinear Regression Train R2 Score: 0.5260233155015783\\nLinear Regression Test R2 Score: 0.5322513167399172\\n'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "a_lr = LinearRegression()\n",
    "a_lr.fit(a_X_train_sc, a_y_train)\n",
    "print(f'Linear Regression Train R2 Score: {a_lr.score(a_X_train_sc, a_y_train)}')\n",
    "print(f'Linear Regression Test R2 Score: {a_lr.score(a_X_test_sc, a_y_test)}')\n",
    "'''\n",
    "'''\n",
    "Linear Regression Train R2 Score: 0.5260233155015783\n",
    "Linear Regression Test R2 Score: 0.5322513167399172\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7686ed97-1c84-469e-8af8-219427411d79",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Linear Regression Pipeline (StandardScaler, Polynomial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f7775cef-3418-4343-a182-1abcb77617cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nLR Pipe Train R2 Score: 0.8082482755481244\\nLR Pipe Test R2 Score: 0.7906546450250773\\n'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "a_lr_pipe = Pipeline([\n",
    "    ('a_ss', StandardScaler()),\n",
    "    ('a_poly', PolynomialFeatures()),\n",
    "    ('a_lr', LinearRegression())\n",
    "])\n",
    "\n",
    "a_lr_pipe.fit(a_X_train, a_y_train)\n",
    "print(f'LR Pipe Train R2 Score: {a_lr_pipe.score(a_X_train, a_y_train)}')\n",
    "print(f'LR Pipe Test R2 Score: {a_lr_pipe.score(a_X_test, a_y_test)}')\n",
    "'''\n",
    "'''\n",
    "LR Pipe Train R2 Score: 0.8082482755481244\n",
    "LR Pipe Test R2 Score: 0.7906546450250773\n",
    "'''\n",
    "'''\n",
    "LR Pipe Train R2 Score: 0.8082482755481244\n",
    "LR Pipe Test R2 Score: 0.7906546450250773\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab2ca0b9-2095-456c-8b25-59d83d916ae2",
   "metadata": {},
   "source": [
    "## Regressor Boosting\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "863d38da-24aa-40b5-a1d4-b9093305aeb1",
   "metadata": {
    "tags": []
   },
   "source": [
    "### AdaBoost with Linear Regression Base Estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "29b7fae1-c7fe-47bf-afee-8d72c2755105",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nAda LR Train R2 Score: 0.8046423913777858\\nAda LR Test R2 Score: 0.776694380832758\\n'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "a_ada_lr = AdaBoostRegressor(base_estimator = LinearRegression(), random_state = 42)\n",
    "\n",
    "a_ada_lr.fit(a_X_train_sc_p, a_y_train)\n",
    "print(f'Ada LR Train R2 Score: {a_ada_lr.score(a_X_train_sc_p, a_y_train)}')\n",
    "print(f'Ada LR Test R2 Score: {a_ada_lr.score(a_X_test_sc_p, a_y_test)}')\n",
    "'''\n",
    "'''\n",
    "Ada LR Train R2 Score: 0.8046423913777858\n",
    "Ada LR Test R2 Score: 0.776694380832758\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a806004b-cfa6-4c4d-a665-c747c5a1f0a8",
   "metadata": {
    "tags": []
   },
   "source": [
    "### AdaBoost with Decision Tree Base Estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6b5c2aa0-b1b7-47b4-84fa-c3ccd862cac3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n0.868164587298871\\n{'base_estimator__max_depth': 5, 'n_estimators': 200}\\nAda DT Train R2 Score: 0.8756108134812447\\nAda DT Test R2 Score: 0.8730399805443708\\n\""
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "a_ada_dt = AdaBoostRegressor(base_estimator = DecisionTreeRegressor(), random_state = 42)\n",
    "\n",
    "a_ada_dt_params = {\n",
    "    'n_estimators': [50, 100, 150, 200],\n",
    "    'base_estimator__max_depth': [1, 2, 5]\n",
    "}\n",
    "\n",
    "a_gs_ada_dt = GridSearchCV(a_ada_dt, param_grid = a_ada_dt_params, cv = 5)\n",
    "a_gs_ada_dt.fit(a_X_train_sc, a_y_train)\n",
    "print(a_gs_ada_dt.best_score_)\n",
    "print(a_gs_ada_dt.best_params_)\n",
    "print(f'Ada DT Train R2 Score: {a_gs_ada_dt.score(a_X_train_sc, a_y_train)}')\n",
    "print(f'Ada DT Test R2 Score: {a_gs_ada_dt.score(a_X_test_sc, a_y_test)}')\n",
    "'''\n",
    "\n",
    "'''\n",
    "0.868164587298871\n",
    "{'base_estimator__max_depth': 5, 'n_estimators': 200}\n",
    "Ada DT Train R2 Score: 0.8756108134812447\n",
    "Ada DT Test R2 Score: 0.8730399805443708\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "326c2e4e-7857-4237-9e40-4f9a81df1aa0",
   "metadata": {
    "tags": []
   },
   "source": [
    "### AdaBoost with Random Forest Base Estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "97de7994-ecee-4a43-81f7-c06a3e000acd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n0.844253394135492\\n{'base_estimator__max_depth': 5, 'n_estimators': 50}\\nAda DT Train R2 Score: 0.8526320077631239\\nAda DT Test R2 Score: 0.8512220020729748\\n\""
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "a_ada_rf = AdaBoostRegressor(base_estimator = RandomForestRegressor(), random_state = 42)\n",
    "\n",
    "a_ada_rf_params = {\n",
    "    'n_estimators': [50, 100, 150, 200],\n",
    "    'base_estimator__max_depth': [1, 2, 5]\n",
    "}\n",
    "\n",
    "a_gs_ada_rf = GridSearchCV(a_ada_rf, param_grid = a_ada_rf_params, cv = 5)\n",
    "a_gs_ada_rf.fit(a_X_train_sc, a_y_train)\n",
    "print(a_gs_ada_rf.best_score_)\n",
    "print(a_gs_ada_rf.best_params_)\n",
    "print(f'Ada DT Train R2 Score: {a_gs_ada_rf.score(a_X_train_sc, a_y_train)}')\n",
    "print(f'Ada DT Test R2 Score: {a_gs_ada_rf.score(a_X_test_sc, a_y_test)}')\n",
    "'''\n",
    "'''\n",
    "0.844253394135492\n",
    "{'base_estimator__max_depth': 5, 'n_estimators': 50}\n",
    "Ada DT Train R2 Score: 0.8526320077631239\n",
    "Ada DT Test R2 Score: 0.8512220020729748\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "351263ad-bdf0-40fd-9333-5f2f8af3dd58",
   "metadata": {},
   "source": [
    "### Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0e3968af-6d12-49ba-80ac-778c655ccf20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nGridSearch Best Score: 0.8839302929148015\\n{'learning_rate': 0.8, 'max_depth': 4, 'n_estimators': 150}\\nGBoost Train R2 Score: 0.9313990799272799\\nGBoost Test R2 Score: 0.8849043647905527\\nMean Absolute Error: 4.056626449058407\\nMean Squared Error: 30.153280288091132\\n\""
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "# Instantiate\n",
    "h_gboost = GradientBoostingRegressor()\n",
    "\n",
    "h_gboost_params = {\n",
    "    'max_depth': [2, 3, 4],\n",
    "    'n_estimators': [100, 125, 150],\n",
    "    'learning_rate': [0.8, .1, .12]\n",
    "}\n",
    "\n",
    "# Gridsearch\n",
    "h_gb_gs = GridSearchCV(h_gboost, param_grid = h_gboost_params, cv = 5)\n",
    "h_gb_gs.fit(h_X_train_sc, h_y_train)\n",
    "\n",
    "# Metrics\n",
    "print(f'GridSearch Best Score: {h_gb_gs.best_score_}')\n",
    "print(h_gb_gs.best_params_)\n",
    "print(f'GBoost Train R2 Score: {h_gb_gs.score(h_X_train_sc, h_y_train)}')\n",
    "print(f'GBoost Test R2 Score: {h_gb_gs.score(h_X_test_sc, h_y_test)}')\n",
    "\n",
    "h_gb_y_true = h_y_test\n",
    "h_gb_y_pred = h_gb_gs.predict(h_X_test_sc)\n",
    "\n",
    "# MAE\n",
    "print(f'Mean Absolute Error: {mean_absolute_error(h_gb_y_true, h_gb_y_pred)}')\n",
    "\n",
    "# MSE\n",
    "print(f'Mean Squared Error: {mean_squared_error(h_gb_y_true, h_gb_y_pred)}')\n",
    "'''\n",
    "'''\n",
    "GridSearch Best Score: 0.8839302929148015\n",
    "{'learning_rate': 0.8, 'max_depth': 4, 'n_estimators': 150}\n",
    "GBoost Train R2 Score: 0.9313990799272799\n",
    "GBoost Test R2 Score: 0.8849043647905527\n",
    "Mean Absolute Error: 4.056626449058407\n",
    "Mean Squared Error: 30.153280288091132\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38228abd-c78d-492d-bf62-e810d1449682",
   "metadata": {},
   "source": [
    "### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "361f95ab-bfda-4cb9-8ba3-2a28aff106c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nGridSearch Best Score: 0.8861220006738151\\n{'learning_rate': 0.8, 'max_depth': 4, 'n_estimators': 150}\\nXGBoost Train R2 Score: 0.9303844407794039\\nXGBoost Test R2 Score: 0.8895029087565683\\nMean Absolute Error: 3.9739194626882286\\nMean Squared Error: 28.948532732964043\\n\""
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "# Instantiate\n",
    "h_xgboost = XGBRegressor()\n",
    "\n",
    "# Gridsearch\n",
    "h_xgboost_params = {\n",
    "    'max_depth': [2, 3, 4],\n",
    "    'n_estimators': [100, 125, 150],\n",
    "    'learning_rate': [0.8, .1, .12]\n",
    "}\n",
    "\n",
    "h_xgb_gs = GridSearchCV(h_xgboost, param_grid = h_xgboost_params, cv = 5)\n",
    "h_xgb_gs.fit(h_X_train_sc, h_y_train)\n",
    "\n",
    "# Metrics\n",
    "print(f'GridSearch Best Score: {h_xgb_gs.best_score_}')\n",
    "print(h_xgb_gs.best_params_)\n",
    "print(f'XGBoost Train R2 Score: {h_xgb_gs.score(h_X_train_sc, h_y_train)}')\n",
    "print(f'XGBoost Test R2 Score: {h_xgb_gs.score(h_X_test_sc, h_y_test)}')\n",
    "\n",
    "h_xgb_y_true = h_y_test\n",
    "h_xgb_y_pred = h_xgb_gs.predict(h_X_test_sc)\n",
    "\n",
    "# MAE\n",
    "print(f'Mean Absolute Error: {mean_absolute_error(h_xgb_y_true, h_xgb_y_pred)}')\n",
    "\n",
    "# MSE\n",
    "print(f'Mean Squared Error: {mean_squared_error(h_xgb_y_true, h_xgb_y_pred)}')\n",
    "'''\n",
    "'''\n",
    "GridSearch Best Score: 0.8861220006738151\n",
    "{'learning_rate': 0.8, 'max_depth': 4, 'n_estimators': 150}\n",
    "XGBoost Train R2 Score: 0.9303844407794039\n",
    "XGBoost Test R2 Score: 0.8895029087565683\n",
    "Mean Absolute Error: 3.9739194626882286\n",
    "Mean Squared Error: 28.948532732964043\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42951d10-b39e-443f-a36a-e766a664d496",
   "metadata": {},
   "source": [
    "## Neural Net Regressor\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c11b9530-ac08-4da0-a6cf-5c75fc7eab57",
   "metadata": {},
   "source": [
    "### X, y, train_test_split, StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "546830b3-b4ef-40e7-9a72-ed98f8a51ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "h_X_nn = h_df[h_features]\n",
    "h_y_nn = h_df['heart_rate']\n",
    "\n",
    "h_X_nn = np.array(h_X_nn)\n",
    "h_y_nn = np.array(h_y_nn)\n",
    "\n",
    "h_X_nn_train, h_X_nn_test, h_y_nn_train, h_y_nn_test = train_test_split(h_X_nn, h_y_nn, random_state = 42)\n",
    "\n",
    "h_ss_nn = StandardScaler()\n",
    "h_X_nn_train_sc = h_ss_nn.fit_transform(h_X_nn_train)\n",
    "h_X_nn_test_sc = h_ss.transform(h_X_nn_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c5fe911f-9448-4b16-bc94-270ee4531a05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16,)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h_X_nn[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ecf6188a-a85f-403f-8c24-523eed885157",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Instantiate\n",
    "h_model_nn = Sequential()\n",
    "\n",
    "# Layers\n",
    "h_model_nn.add(Dense(128, input_dim = 16, activation = 'relu'))\n",
    "\n",
    "h_model_nn.add(BatchNormalization())\n",
    "h_model_nn.add(Dense(64, activation = 'relu', kernel_regularizer = l2(.1)))\n",
    "h_model_nn.add(Dense(128, activation = 'relu', kernel_regularizer = l2(.5))) \n",
    "h_model_nn.add(Dense(64, activation = 'relu', kernel_regularizer = l2(.1))) \n",
    "h_model_nn.add(Dense(1, kernel_regularizer = l2(.5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c56569da-5f33-4e0a-9262-29899d45bbea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile\n",
    "h_model_nn.compile(optimizer = 'adam', loss = 'mse', metrics = ['mse'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "5d926489-6913-4c2b-9715-6d6c1f8f69f5",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "905/905 [==============================] - 3s 2ms/step - loss: 784.1671 - mse: 631.1639 - val_loss: 342.3654 - val_mse: 216.6667\n",
      "Epoch 2/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 282.0534 - mse: 171.1889 - val_loss: 285.4284 - val_mse: 187.6171\n",
      "Epoch 3/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 243.0020 - mse: 155.3402 - val_loss: 257.0897 - val_mse: 178.7236\n",
      "Epoch 4/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 214.0899 - mse: 143.1501 - val_loss: 247.0578 - val_mse: 183.1942\n",
      "Epoch 5/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 197.8345 - mse: 139.3699 - val_loss: 236.5539 - val_mse: 183.2388\n",
      "Epoch 6/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 179.3881 - mse: 130.2984 - val_loss: 189.7656 - val_mse: 144.7719\n",
      "Epoch 7/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 170.8244 - mse: 128.9628 - val_loss: 194.3819 - val_mse: 155.5633\n",
      "Epoch 8/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 161.0761 - mse: 124.6720 - val_loss: 151.7924 - val_mse: 117.6564\n",
      "Epoch 9/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 154.6753 - mse: 122.4250 - val_loss: 154.9718 - val_mse: 124.5175\n",
      "Epoch 10/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 148.2454 - mse: 119.1927 - val_loss: 160.9859 - val_mse: 133.3988\n",
      "Epoch 11/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 143.1029 - mse: 116.6096 - val_loss: 141.8698 - val_mse: 116.4810\n",
      "Epoch 12/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 138.8850 - mse: 114.3921 - val_loss: 140.4711 - val_mse: 116.8136\n",
      "Epoch 13/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 137.5608 - mse: 114.6328 - val_loss: 133.2957 - val_mse: 110.9885\n",
      "Epoch 14/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 135.0295 - mse: 113.3105 - val_loss: 130.4852 - val_mse: 109.3226\n",
      "Epoch 15/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 132.5524 - mse: 111.7745 - val_loss: 139.4652 - val_mse: 119.1172\n",
      "Epoch 16/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 129.0040 - mse: 109.0247 - val_loss: 135.7282 - val_mse: 116.2512\n",
      "Epoch 17/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 127.7231 - mse: 108.4318 - val_loss: 125.6088 - val_mse: 106.5929\n",
      "Epoch 18/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 125.5967 - mse: 106.9217 - val_loss: 127.7455 - val_mse: 109.3731\n",
      "Epoch 19/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 121.5853 - mse: 103.4037 - val_loss: 127.1976 - val_mse: 109.3344\n",
      "Epoch 20/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 123.6245 - mse: 105.8763 - val_loss: 132.5589 - val_mse: 114.8857\n",
      "Epoch 21/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 119.2693 - mse: 101.8225 - val_loss: 126.0140 - val_mse: 108.7964\n",
      "Epoch 22/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 118.2550 - mse: 101.1244 - val_loss: 119.6543 - val_mse: 102.6227\n",
      "Epoch 23/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 116.6740 - mse: 99.7720 - val_loss: 115.8606 - val_mse: 99.1644\n",
      "Epoch 24/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 116.1740 - mse: 99.5031 - val_loss: 115.3024 - val_mse: 98.7021\n",
      "Epoch 25/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 114.9733 - mse: 98.4513 - val_loss: 115.4740 - val_mse: 98.9916\n",
      "Epoch 26/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 116.3312 - mse: 99.8664 - val_loss: 118.6324 - val_mse: 102.1704\n",
      "Epoch 27/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 112.3054 - mse: 95.9516 - val_loss: 121.1516 - val_mse: 104.9420\n",
      "Epoch 28/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 112.2874 - mse: 96.0432 - val_loss: 137.8617 - val_mse: 121.6954\n",
      "Epoch 29/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 113.9114 - mse: 97.5601 - val_loss: 123.3057 - val_mse: 107.0540\n",
      "Epoch 30/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 110.9631 - mse: 94.5663 - val_loss: 119.5226 - val_mse: 103.1095\n",
      "Epoch 31/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 111.9030 - mse: 95.4122 - val_loss: 111.1836 - val_mse: 94.6828\n",
      "Epoch 32/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 108.6964 - mse: 92.2599 - val_loss: 109.1192 - val_mse: 92.7340\n",
      "Epoch 33/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 110.6791 - mse: 94.2339 - val_loss: 119.6780 - val_mse: 103.2891\n",
      "Epoch 34/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 108.3418 - mse: 91.8834 - val_loss: 124.3213 - val_mse: 107.8815\n",
      "Epoch 35/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 109.4244 - mse: 92.9467 - val_loss: 150.4561 - val_mse: 134.2421\n",
      "Epoch 36/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 109.2337 - mse: 92.7627 - val_loss: 108.7244 - val_mse: 92.2239\n",
      "Epoch 37/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 106.0359 - mse: 89.5122 - val_loss: 106.7155 - val_mse: 90.1218\n",
      "Epoch 38/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 105.9989 - mse: 89.4952 - val_loss: 110.1459 - val_mse: 93.5258\n",
      "Epoch 39/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 106.0597 - mse: 89.5139 - val_loss: 105.3433 - val_mse: 88.7908\n",
      "Epoch 40/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 105.3656 - mse: 88.8622 - val_loss: 103.6669 - val_mse: 87.2180\n",
      "Epoch 41/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 104.8006 - mse: 88.3031 - val_loss: 110.4201 - val_mse: 93.9551\n",
      "Epoch 42/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 105.2574 - mse: 88.7373 - val_loss: 110.2218 - val_mse: 93.7646\n",
      "Epoch 43/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 104.1164 - mse: 87.5646 - val_loss: 120.3960 - val_mse: 103.6158\n",
      "Epoch 44/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 103.3751 - mse: 86.7659 - val_loss: 103.0345 - val_mse: 86.4524\n",
      "Epoch 45/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 103.1370 - mse: 86.5127 - val_loss: 122.7483 - val_mse: 106.4018\n",
      "Epoch 46/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 103.8700 - mse: 87.3074 - val_loss: 108.7789 - val_mse: 92.2248\n",
      "Epoch 47/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 104.9238 - mse: 88.2939 - val_loss: 107.5596 - val_mse: 90.8687\n",
      "Epoch 48/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 102.3616 - mse: 85.6628 - val_loss: 103.5260 - val_mse: 86.8531\n",
      "Epoch 49/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 103.2893 - mse: 86.5955 - val_loss: 102.3741 - val_mse: 85.7116\n",
      "Epoch 50/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 101.4192 - mse: 84.7326 - val_loss: 111.0259 - val_mse: 94.4593\n",
      "Epoch 51/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 101.6977 - mse: 84.9738 - val_loss: 99.6961 - val_mse: 83.0052\n",
      "Epoch 52/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 102.5447 - mse: 85.8810 - val_loss: 125.7807 - val_mse: 109.0442\n",
      "Epoch 53/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 101.1253 - mse: 84.4358 - val_loss: 100.6469 - val_mse: 83.9302\n",
      "Epoch 54/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 100.3466 - mse: 83.7010 - val_loss: 95.2855 - val_mse: 78.5482\n",
      "Epoch 55/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 100.4726 - mse: 83.8014 - val_loss: 107.7111 - val_mse: 90.9604\n",
      "Epoch 56/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 100.6750 - mse: 83.9554 - val_loss: 98.2261 - val_mse: 81.5380\n",
      "Epoch 57/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 99.9833 - mse: 83.2802 - val_loss: 100.0871 - val_mse: 83.4143\n",
      "Epoch 58/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 98.6227 - mse: 81.9400 - val_loss: 99.7238 - val_mse: 83.1190\n",
      "Epoch 59/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 101.0526 - mse: 84.3372 - val_loss: 99.3369 - val_mse: 82.6987\n",
      "Epoch 60/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 99.5648 - mse: 82.8505 - val_loss: 105.1922 - val_mse: 88.5402\n",
      "Epoch 61/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 98.7227 - mse: 81.9936 - val_loss: 96.7627 - val_mse: 80.0340\n",
      "Epoch 62/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 98.1721 - mse: 81.4921 - val_loss: 122.9540 - val_mse: 106.1052\n",
      "Epoch 63/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 98.4753 - mse: 81.7521 - val_loss: 109.1501 - val_mse: 92.4406\n",
      "Epoch 64/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 97.5704 - mse: 80.8302 - val_loss: 95.1634 - val_mse: 78.5623\n",
      "Epoch 65/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 99.1410 - mse: 82.4736 - val_loss: 96.8631 - val_mse: 80.1943\n",
      "Epoch 66/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 99.1296 - mse: 82.4741 - val_loss: 98.4924 - val_mse: 81.7829\n",
      "Epoch 67/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 97.9663 - mse: 81.2671 - val_loss: 95.6902 - val_mse: 78.9791\n",
      "Epoch 68/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 98.0735 - mse: 81.3063 - val_loss: 97.9260 - val_mse: 80.9897\n",
      "Epoch 69/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 98.0122 - mse: 81.1816 - val_loss: 112.7999 - val_mse: 96.0431\n",
      "Epoch 70/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 98.6931 - mse: 81.9450 - val_loss: 105.6219 - val_mse: 88.7324\n",
      "Epoch 71/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 98.2763 - mse: 81.5120 - val_loss: 98.4761 - val_mse: 81.7199\n",
      "Epoch 72/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 97.3546 - mse: 80.5258 - val_loss: 97.5198 - val_mse: 80.7529\n",
      "Epoch 73/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 96.4836 - mse: 79.6593 - val_loss: 100.0230 - val_mse: 83.1039\n",
      "Epoch 74/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 96.1089 - mse: 79.3402 - val_loss: 99.1969 - val_mse: 82.5996\n",
      "Epoch 75/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 97.7304 - mse: 81.0124 - val_loss: 100.6689 - val_mse: 83.7069\n",
      "Epoch 76/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 96.6757 - mse: 79.9473 - val_loss: 101.0390 - val_mse: 84.4052\n",
      "Epoch 77/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 96.1427 - mse: 79.3392 - val_loss: 97.5823 - val_mse: 80.7679\n",
      "Epoch 78/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 95.8616 - mse: 78.9938 - val_loss: 93.2044 - val_mse: 76.4298\n",
      "Epoch 79/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 95.8441 - mse: 78.9672 - val_loss: 98.7268 - val_mse: 81.8758\n",
      "Epoch 80/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 96.2234 - mse: 79.3180 - val_loss: 94.9328 - val_mse: 78.1236\n",
      "Epoch 81/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 96.4540 - mse: 79.5007 - val_loss: 96.5628 - val_mse: 79.6368\n",
      "Epoch 82/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 96.2431 - mse: 79.2451 - val_loss: 95.4999 - val_mse: 78.4163\n",
      "Epoch 83/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 96.2513 - mse: 79.2006 - val_loss: 95.7544 - val_mse: 78.8294\n",
      "Epoch 84/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 95.0706 - mse: 78.0880 - val_loss: 96.3765 - val_mse: 79.4839\n",
      "Epoch 85/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 96.4697 - mse: 79.5148 - val_loss: 93.4232 - val_mse: 76.4318\n",
      "Epoch 86/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 94.3161 - mse: 77.3950 - val_loss: 93.9599 - val_mse: 77.0894\n",
      "Epoch 87/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 95.8241 - mse: 78.9372 - val_loss: 94.8602 - val_mse: 77.8372\n",
      "Epoch 88/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 95.0135 - mse: 78.1227 - val_loss: 96.1107 - val_mse: 79.2509\n",
      "Epoch 89/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 94.7754 - mse: 77.8270 - val_loss: 96.0475 - val_mse: 79.1330\n",
      "Epoch 90/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 95.3812 - mse: 78.4553 - val_loss: 96.6363 - val_mse: 79.5531\n",
      "Epoch 91/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 95.4897 - mse: 78.5626 - val_loss: 92.9020 - val_mse: 76.0216\n",
      "Epoch 92/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 93.6590 - mse: 76.7942 - val_loss: 90.6169 - val_mse: 73.7766\n",
      "Epoch 93/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 94.5971 - mse: 77.6897 - val_loss: 91.6396 - val_mse: 74.8155\n",
      "Epoch 94/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 94.7899 - mse: 77.8784 - val_loss: 95.3235 - val_mse: 78.3184\n",
      "Epoch 95/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 93.8658 - mse: 76.9622 - val_loss: 93.8036 - val_mse: 76.9880\n",
      "Epoch 96/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 95.3427 - mse: 78.4524 - val_loss: 94.7940 - val_mse: 77.8091\n",
      "Epoch 97/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 93.7844 - mse: 76.8666 - val_loss: 94.8533 - val_mse: 77.9671\n",
      "Epoch 98/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 93.9288 - mse: 77.0367 - val_loss: 90.0952 - val_mse: 73.1827\n",
      "Epoch 99/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 92.9575 - mse: 76.1372 - val_loss: 95.8630 - val_mse: 79.0217\n",
      "Epoch 100/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 94.8037 - mse: 77.9640 - val_loss: 90.1756 - val_mse: 73.3829\n",
      "Epoch 101/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 93.4810 - mse: 76.7118 - val_loss: 93.7757 - val_mse: 77.0345\n",
      "Epoch 102/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 93.3629 - mse: 76.5880 - val_loss: 93.2546 - val_mse: 76.3146\n",
      "Epoch 103/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 92.7636 - mse: 75.9141 - val_loss: 89.7724 - val_mse: 72.9337\n",
      "Epoch 104/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 93.0410 - mse: 76.2042 - val_loss: 94.5447 - val_mse: 77.6477\n",
      "Epoch 105/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 93.4361 - mse: 76.5294 - val_loss: 101.8812 - val_mse: 85.0350\n",
      "Epoch 106/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 94.1344 - mse: 77.3094 - val_loss: 106.9487 - val_mse: 90.3348\n",
      "Epoch 107/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 92.9254 - mse: 76.1003 - val_loss: 94.0468 - val_mse: 77.3289\n",
      "Epoch 108/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 92.8085 - mse: 76.0016 - val_loss: 95.7143 - val_mse: 78.8454\n",
      "Epoch 109/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 93.1018 - mse: 76.2940 - val_loss: 89.7577 - val_mse: 72.8547\n",
      "Epoch 110/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 93.0403 - mse: 76.1701 - val_loss: 93.2417 - val_mse: 76.4177\n",
      "Epoch 111/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 92.5798 - mse: 75.7241 - val_loss: 95.2959 - val_mse: 78.3033\n",
      "Epoch 112/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 92.1778 - mse: 75.3108 - val_loss: 100.8481 - val_mse: 84.1537\n",
      "Epoch 113/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 92.3781 - mse: 75.5308 - val_loss: 96.0282 - val_mse: 79.4118\n",
      "Epoch 114/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 92.0486 - mse: 75.2837 - val_loss: 95.7018 - val_mse: 78.8154\n",
      "Epoch 115/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 93.4847 - mse: 76.6251 - val_loss: 89.8379 - val_mse: 72.9308\n",
      "Epoch 116/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 91.6789 - mse: 74.7488 - val_loss: 93.4969 - val_mse: 76.7403\n",
      "Epoch 117/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 93.0667 - mse: 76.2091 - val_loss: 93.0142 - val_mse: 76.1399\n",
      "Epoch 118/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 91.0775 - mse: 74.2223 - val_loss: 90.1265 - val_mse: 73.1917\n",
      "Epoch 119/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 92.2267 - mse: 75.3202 - val_loss: 92.1845 - val_mse: 75.3985\n",
      "Epoch 120/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 92.1461 - mse: 75.2921 - val_loss: 89.5983 - val_mse: 72.7873\n",
      "Epoch 121/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 91.8429 - mse: 74.9652 - val_loss: 91.1455 - val_mse: 74.1853\n",
      "Epoch 122/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 91.6122 - mse: 74.7478 - val_loss: 93.3256 - val_mse: 76.4246\n",
      "Epoch 123/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 91.9988 - mse: 75.0784 - val_loss: 93.8313 - val_mse: 76.9520\n",
      "Epoch 124/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 90.9675 - mse: 74.0644 - val_loss: 103.1209 - val_mse: 86.4193\n",
      "Epoch 125/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 91.3256 - mse: 74.4583 - val_loss: 91.5363 - val_mse: 74.6717\n",
      "Epoch 126/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 90.0893 - mse: 73.2148 - val_loss: 89.8544 - val_mse: 72.9789\n",
      "Epoch 127/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 91.4654 - mse: 74.5838 - val_loss: 92.9855 - val_mse: 76.1841\n",
      "Epoch 128/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 91.7828 - mse: 74.9501 - val_loss: 92.9985 - val_mse: 76.2901\n",
      "Epoch 129/500\n",
      "905/905 [==============================] - 2s 3ms/step - loss: 90.3649 - mse: 73.5280 - val_loss: 94.2526 - val_mse: 77.5172\n",
      "Epoch 130/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 90.5870 - mse: 73.7914 - val_loss: 88.9675 - val_mse: 71.9803\n",
      "Epoch 131/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 90.9900 - mse: 74.1205 - val_loss: 92.7629 - val_mse: 75.8633\n",
      "Epoch 132/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 90.6947 - mse: 73.8243 - val_loss: 89.9039 - val_mse: 73.0677\n",
      "Epoch 133/500\n",
      "905/905 [==============================] - 3s 3ms/step - loss: 90.8982 - mse: 74.0505 - val_loss: 96.2598 - val_mse: 79.2904\n",
      "Epoch 134/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 90.8110 - mse: 73.9100 - val_loss: 90.0730 - val_mse: 73.2140\n",
      "Epoch 135/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 90.4224 - mse: 73.5761 - val_loss: 91.7618 - val_mse: 74.8284\n",
      "Epoch 136/500\n",
      "905/905 [==============================] - 2s 3ms/step - loss: 90.3925 - mse: 73.5581 - val_loss: 90.4388 - val_mse: 73.7079\n",
      "Epoch 137/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 90.4656 - mse: 73.6187 - val_loss: 91.9084 - val_mse: 75.0859\n",
      "Epoch 138/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 90.1294 - mse: 73.3254 - val_loss: 90.6205 - val_mse: 73.9602\n",
      "Epoch 139/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 90.6868 - mse: 73.9551 - val_loss: 88.1215 - val_mse: 71.3409\n",
      "Epoch 140/500\n",
      "905/905 [==============================] - 2s 3ms/step - loss: 90.1174 - mse: 73.3121 - val_loss: 88.9477 - val_mse: 72.1734\n",
      "Epoch 141/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 91.0060 - mse: 74.2213 - val_loss: 89.4721 - val_mse: 72.6190\n",
      "Epoch 142/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 89.3679 - mse: 72.5350 - val_loss: 87.0074 - val_mse: 70.2775\n",
      "Epoch 143/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 90.1356 - mse: 73.3072 - val_loss: 97.7981 - val_mse: 80.9676\n",
      "Epoch 144/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 89.1234 - mse: 72.3216 - val_loss: 89.3090 - val_mse: 72.5084\n",
      "Epoch 145/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 89.4098 - mse: 72.6974 - val_loss: 93.1331 - val_mse: 76.3549\n",
      "Epoch 146/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 90.5766 - mse: 73.8652 - val_loss: 96.8774 - val_mse: 80.1763\n",
      "Epoch 147/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 90.5065 - mse: 73.8547 - val_loss: 89.8003 - val_mse: 73.0985\n",
      "Epoch 148/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 89.9571 - mse: 73.2865 - val_loss: 89.9575 - val_mse: 73.3302\n",
      "Epoch 149/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 88.9203 - mse: 72.1985 - val_loss: 98.0035 - val_mse: 81.2167\n",
      "Epoch 150/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 89.2695 - mse: 72.5835 - val_loss: 90.6362 - val_mse: 74.0257\n",
      "Epoch 151/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 90.6069 - mse: 73.9274 - val_loss: 91.9057 - val_mse: 75.1748\n",
      "Epoch 152/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 89.1665 - mse: 72.4461 - val_loss: 90.0379 - val_mse: 73.2675\n",
      "Epoch 153/500\n",
      "905/905 [==============================] - 2s 3ms/step - loss: 89.4081 - mse: 72.6755 - val_loss: 96.5072 - val_mse: 79.8406\n",
      "Epoch 154/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 89.5141 - mse: 72.8143 - val_loss: 92.6774 - val_mse: 75.9870\n",
      "Epoch 155/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 89.1741 - mse: 72.4553 - val_loss: 89.4663 - val_mse: 72.6488\n",
      "Epoch 156/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 88.5876 - mse: 71.8244 - val_loss: 88.5289 - val_mse: 71.8722\n",
      "Epoch 157/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 89.6199 - mse: 72.9137 - val_loss: 90.2127 - val_mse: 73.4791\n",
      "Epoch 158/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 88.2416 - mse: 71.5520 - val_loss: 91.8174 - val_mse: 75.1628\n",
      "Epoch 159/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 88.6814 - mse: 72.0499 - val_loss: 94.4162 - val_mse: 77.8913\n",
      "Epoch 160/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 89.0711 - mse: 72.4694 - val_loss: 87.1159 - val_mse: 70.4539\n",
      "Epoch 161/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 88.4779 - mse: 71.8632 - val_loss: 88.2858 - val_mse: 71.7116\n",
      "Epoch 162/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 89.4849 - mse: 72.8787 - val_loss: 90.0188 - val_mse: 73.4827\n",
      "Epoch 163/500\n",
      "905/905 [==============================] - 2s 3ms/step - loss: 88.3317 - mse: 71.7832 - val_loss: 89.9302 - val_mse: 73.3548\n",
      "Epoch 164/500\n",
      "905/905 [==============================] - 3s 3ms/step - loss: 87.9606 - mse: 71.3885 - val_loss: 87.6109 - val_mse: 70.9565\n",
      "Epoch 165/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 88.5489 - mse: 71.9011 - val_loss: 89.5151 - val_mse: 72.8300\n",
      "Epoch 166/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 88.1071 - mse: 71.4288 - val_loss: 99.9990 - val_mse: 83.2180\n",
      "Epoch 167/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 88.4517 - mse: 71.7807 - val_loss: 90.2671 - val_mse: 73.7128\n",
      "Epoch 168/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 87.7760 - mse: 71.1609 - val_loss: 90.0439 - val_mse: 73.4747\n",
      "Epoch 169/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 88.7712 - mse: 72.1291 - val_loss: 86.9285 - val_mse: 70.3231\n",
      "Epoch 170/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 88.4552 - mse: 71.8691 - val_loss: 92.7576 - val_mse: 76.1436\n",
      "Epoch 171/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 88.4022 - mse: 71.8430 - val_loss: 89.8051 - val_mse: 73.2423\n",
      "Epoch 172/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 87.4819 - mse: 70.9648 - val_loss: 88.6089 - val_mse: 71.9032\n",
      "Epoch 173/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 88.3303 - mse: 71.6895 - val_loss: 91.8533 - val_mse: 75.3539\n",
      "Epoch 174/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 87.9130 - mse: 71.2880 - val_loss: 91.2785 - val_mse: 74.7871\n",
      "Epoch 175/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 88.0682 - mse: 71.4307 - val_loss: 88.2800 - val_mse: 71.8138\n",
      "Epoch 176/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 87.1529 - mse: 70.5575 - val_loss: 87.9164 - val_mse: 71.3335\n",
      "Epoch 177/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 88.2381 - mse: 71.6021 - val_loss: 90.4144 - val_mse: 73.6866\n",
      "Epoch 178/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 88.5373 - mse: 71.9220 - val_loss: 87.0601 - val_mse: 70.4426\n",
      "Epoch 179/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 87.7942 - mse: 71.2015 - val_loss: 87.3351 - val_mse: 70.8130\n",
      "Epoch 180/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 88.1153 - mse: 71.5081 - val_loss: 91.2226 - val_mse: 74.4514\n",
      "Epoch 181/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 87.7044 - mse: 71.0669 - val_loss: 87.5996 - val_mse: 71.0206\n",
      "Epoch 182/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 87.0942 - mse: 70.4584 - val_loss: 85.3153 - val_mse: 68.7033\n",
      "Epoch 183/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 87.3843 - mse: 70.7696 - val_loss: 91.6141 - val_mse: 75.0243\n",
      "Epoch 184/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 86.4052 - mse: 69.8136 - val_loss: 87.8158 - val_mse: 71.2505\n",
      "Epoch 185/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 87.1591 - mse: 70.6207 - val_loss: 83.7456 - val_mse: 67.1779\n",
      "Epoch 186/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 87.0389 - mse: 70.4850 - val_loss: 89.5821 - val_mse: 72.9907\n",
      "Epoch 187/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 86.9280 - mse: 70.3733 - val_loss: 108.7593 - val_mse: 91.9989\n",
      "Epoch 188/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 87.8786 - mse: 71.3728 - val_loss: 84.2130 - val_mse: 67.7180\n",
      "Epoch 189/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 87.3920 - mse: 70.8981 - val_loss: 87.4821 - val_mse: 71.0205\n",
      "Epoch 190/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 86.4082 - mse: 69.8475 - val_loss: 91.8628 - val_mse: 75.4350\n",
      "Epoch 191/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 87.7430 - mse: 71.2035 - val_loss: 85.6727 - val_mse: 69.1431\n",
      "Epoch 192/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 86.6592 - mse: 70.1830 - val_loss: 85.9311 - val_mse: 69.3996\n",
      "Epoch 193/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 85.7825 - mse: 69.3571 - val_loss: 88.4693 - val_mse: 72.0782\n",
      "Epoch 194/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 86.9019 - mse: 70.4810 - val_loss: 94.3323 - val_mse: 77.8718\n",
      "Epoch 195/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 86.4187 - mse: 69.9669 - val_loss: 84.4703 - val_mse: 67.9569\n",
      "Epoch 196/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 86.6867 - mse: 70.2479 - val_loss: 86.8114 - val_mse: 70.3257\n",
      "Epoch 197/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 86.9962 - mse: 70.5382 - val_loss: 86.9974 - val_mse: 70.5271\n",
      "Epoch 198/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 86.1685 - mse: 69.7822 - val_loss: 87.7457 - val_mse: 71.3326\n",
      "Epoch 199/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 86.2279 - mse: 69.8698 - val_loss: 87.7770 - val_mse: 71.3783\n",
      "Epoch 200/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 86.0025 - mse: 69.6995 - val_loss: 86.1546 - val_mse: 70.0064\n",
      "Epoch 201/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 86.1324 - mse: 69.8377 - val_loss: 86.2952 - val_mse: 69.9996\n",
      "Epoch 202/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 86.1552 - mse: 69.8353 - val_loss: 89.3739 - val_mse: 72.9608\n",
      "Epoch 203/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 86.9693 - mse: 70.6377 - val_loss: 93.8017 - val_mse: 77.4188\n",
      "Epoch 204/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 86.1645 - mse: 69.8605 - val_loss: 89.3811 - val_mse: 73.2234\n",
      "Epoch 205/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 85.1562 - mse: 68.8134 - val_loss: 90.0851 - val_mse: 73.7642\n",
      "Epoch 206/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 85.5956 - mse: 69.2589 - val_loss: 86.9872 - val_mse: 70.7982\n",
      "Epoch 207/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 86.6663 - mse: 70.3717 - val_loss: 87.2043 - val_mse: 70.8790\n",
      "Epoch 208/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 84.8272 - mse: 68.4898 - val_loss: 85.4150 - val_mse: 69.0523\n",
      "Epoch 209/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 85.5421 - mse: 69.2018 - val_loss: 90.2928 - val_mse: 74.0598\n",
      "Epoch 210/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 85.2887 - mse: 68.9782 - val_loss: 86.2918 - val_mse: 70.0770\n",
      "Epoch 211/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 85.4903 - mse: 69.2181 - val_loss: 90.1023 - val_mse: 73.9936\n",
      "Epoch 212/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 84.7777 - mse: 68.5112 - val_loss: 88.8473 - val_mse: 72.6757\n",
      "Epoch 213/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 84.9995 - mse: 68.8244 - val_loss: 85.1557 - val_mse: 68.9540\n",
      "Epoch 214/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 86.2798 - mse: 70.0092 - val_loss: 83.7885 - val_mse: 67.6753\n",
      "Epoch 215/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 85.9378 - mse: 69.7243 - val_loss: 86.3452 - val_mse: 70.1942\n",
      "Epoch 216/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 85.6622 - mse: 69.4217 - val_loss: 99.6146 - val_mse: 83.5228\n",
      "Epoch 217/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 85.7482 - mse: 69.4309 - val_loss: 84.5247 - val_mse: 68.2212\n",
      "Epoch 218/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 85.5538 - mse: 69.2179 - val_loss: 86.3439 - val_mse: 69.9335\n",
      "Epoch 219/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 85.4392 - mse: 69.1396 - val_loss: 89.7193 - val_mse: 73.4213\n",
      "Epoch 220/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 85.5762 - mse: 69.3328 - val_loss: 86.4068 - val_mse: 70.0953\n",
      "Epoch 221/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 85.9485 - mse: 69.6552 - val_loss: 90.3029 - val_mse: 73.9410\n",
      "Epoch 222/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 84.4119 - mse: 68.1300 - val_loss: 85.5492 - val_mse: 69.2961\n",
      "Epoch 223/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 85.4282 - mse: 69.1288 - val_loss: 97.9029 - val_mse: 81.5091\n",
      "Epoch 224/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 85.0127 - mse: 68.7123 - val_loss: 88.8295 - val_mse: 72.5892\n",
      "Epoch 225/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 84.4652 - mse: 68.1677 - val_loss: 92.9392 - val_mse: 76.7353\n",
      "Epoch 226/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 84.3570 - mse: 68.0398 - val_loss: 81.9547 - val_mse: 65.6438\n",
      "Epoch 227/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 85.3921 - mse: 69.1053 - val_loss: 82.6466 - val_mse: 66.3325\n",
      "Epoch 228/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 84.2234 - mse: 68.0013 - val_loss: 86.8189 - val_mse: 70.6350\n",
      "Epoch 229/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 85.7413 - mse: 69.5279 - val_loss: 83.8119 - val_mse: 67.4670\n",
      "Epoch 230/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 84.9060 - mse: 68.6584 - val_loss: 86.3434 - val_mse: 70.1320\n",
      "Epoch 231/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 83.5430 - mse: 67.3049 - val_loss: 94.4396 - val_mse: 78.1225\n",
      "Epoch 232/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 84.2016 - mse: 67.9947 - val_loss: 85.1508 - val_mse: 69.0177\n",
      "Epoch 233/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 84.2492 - mse: 68.0616 - val_loss: 86.9346 - val_mse: 70.8610\n",
      "Epoch 234/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 84.5342 - mse: 68.3425 - val_loss: 86.0112 - val_mse: 69.8624\n",
      "Epoch 235/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 84.4716 - mse: 68.2473 - val_loss: 87.7955 - val_mse: 71.5276\n",
      "Epoch 236/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 84.6302 - mse: 68.4685 - val_loss: 87.2712 - val_mse: 71.1325\n",
      "Epoch 237/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 84.5014 - mse: 68.3761 - val_loss: 83.6990 - val_mse: 67.5862\n",
      "Epoch 238/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 84.7383 - mse: 68.6498 - val_loss: 85.3890 - val_mse: 69.3769\n",
      "Epoch 239/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 84.0660 - mse: 67.9910 - val_loss: 85.0791 - val_mse: 68.9721\n",
      "Epoch 240/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 84.6045 - mse: 68.4406 - val_loss: 85.2155 - val_mse: 69.0312\n",
      "Epoch 241/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 84.0573 - mse: 67.8898 - val_loss: 82.7004 - val_mse: 66.6393\n",
      "Epoch 242/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 83.9879 - mse: 67.8867 - val_loss: 85.3699 - val_mse: 69.2471\n",
      "Epoch 243/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 84.3692 - mse: 68.2913 - val_loss: 86.5070 - val_mse: 70.4044\n",
      "Epoch 244/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 84.3040 - mse: 68.2788 - val_loss: 90.4212 - val_mse: 74.3563\n",
      "Epoch 245/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 83.7407 - mse: 67.6849 - val_loss: 81.8505 - val_mse: 65.6388\n",
      "Epoch 246/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 83.7423 - mse: 67.6328 - val_loss: 85.6935 - val_mse: 69.6023\n",
      "Epoch 247/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 84.8850 - mse: 68.7657 - val_loss: 89.0977 - val_mse: 73.0094\n",
      "Epoch 248/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 83.2295 - mse: 67.1710 - val_loss: 84.8249 - val_mse: 68.6902\n",
      "Epoch 249/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 83.4332 - mse: 67.3447 - val_loss: 83.6102 - val_mse: 67.5750\n",
      "Epoch 250/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 83.6712 - mse: 67.6334 - val_loss: 86.3739 - val_mse: 70.2330\n",
      "Epoch 251/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 83.9684 - mse: 67.9056 - val_loss: 101.1406 - val_mse: 84.9556\n",
      "Epoch 252/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 84.2148 - mse: 68.1817 - val_loss: 85.6334 - val_mse: 69.6695\n",
      "Epoch 253/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 83.6970 - mse: 67.6590 - val_loss: 86.7371 - val_mse: 70.6984\n",
      "Epoch 254/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 83.8197 - mse: 67.7307 - val_loss: 83.9258 - val_mse: 67.9211\n",
      "Epoch 255/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 84.0147 - mse: 67.9530 - val_loss: 83.3834 - val_mse: 67.3808\n",
      "Epoch 256/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 83.0328 - mse: 66.9819 - val_loss: 84.3017 - val_mse: 68.2615\n",
      "Epoch 257/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 84.3298 - mse: 68.2934 - val_loss: 86.1753 - val_mse: 70.0855\n",
      "Epoch 258/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 83.8294 - mse: 67.7662 - val_loss: 81.3504 - val_mse: 65.1795\n",
      "Epoch 259/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 83.6838 - mse: 67.6594 - val_loss: 81.1833 - val_mse: 65.2071\n",
      "Epoch 260/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 83.1813 - mse: 67.1730 - val_loss: 87.6756 - val_mse: 71.6245\n",
      "Epoch 261/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 83.8478 - mse: 67.9077 - val_loss: 82.4060 - val_mse: 66.4129\n",
      "Epoch 262/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 82.7124 - mse: 66.8014 - val_loss: 82.9638 - val_mse: 66.9968\n",
      "Epoch 263/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 83.7993 - mse: 67.9018 - val_loss: 82.4379 - val_mse: 66.4785\n",
      "Epoch 264/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 83.1659 - mse: 67.2711 - val_loss: 87.1142 - val_mse: 71.2820\n",
      "Epoch 265/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 84.3075 - mse: 68.4306 - val_loss: 97.9129 - val_mse: 81.8164\n",
      "Epoch 266/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 84.0248 - mse: 68.1201 - val_loss: 87.5480 - val_mse: 71.6947\n",
      "Epoch 267/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 83.0753 - mse: 67.1697 - val_loss: 86.8836 - val_mse: 71.1603\n",
      "Epoch 268/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 82.9785 - mse: 67.0585 - val_loss: 84.3944 - val_mse: 68.3614\n",
      "Epoch 269/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 84.1043 - mse: 68.1342 - val_loss: 88.1986 - val_mse: 72.4206\n",
      "Epoch 270/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 83.0997 - mse: 67.1873 - val_loss: 81.2914 - val_mse: 65.4228\n",
      "Epoch 271/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 84.6678 - mse: 68.7256 - val_loss: 83.5926 - val_mse: 67.6941\n",
      "Epoch 272/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 83.5613 - mse: 67.5808 - val_loss: 83.1096 - val_mse: 67.1155\n",
      "Epoch 273/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 81.9990 - mse: 66.0901 - val_loss: 83.7409 - val_mse: 67.8798\n",
      "Epoch 274/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 82.1771 - mse: 66.2904 - val_loss: 86.0918 - val_mse: 70.1625\n",
      "Epoch 275/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 82.5627 - mse: 66.6528 - val_loss: 85.2174 - val_mse: 69.4566\n",
      "Epoch 276/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 82.3048 - mse: 66.4580 - val_loss: 82.4659 - val_mse: 66.5833\n",
      "Epoch 277/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 82.7462 - mse: 66.8744 - val_loss: 85.1136 - val_mse: 69.2801\n",
      "Epoch 278/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 83.0749 - mse: 67.1572 - val_loss: 86.8705 - val_mse: 70.9220\n",
      "Epoch 279/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 83.1559 - mse: 67.2926 - val_loss: 85.2369 - val_mse: 69.3718\n",
      "Epoch 280/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 82.2942 - mse: 66.4229 - val_loss: 85.3272 - val_mse: 69.4668\n",
      "Epoch 281/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 82.4996 - mse: 66.6797 - val_loss: 80.8711 - val_mse: 65.1303\n",
      "Epoch 282/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 82.1761 - mse: 66.3359 - val_loss: 82.8265 - val_mse: 66.9886\n",
      "Epoch 283/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 83.1720 - mse: 67.3488 - val_loss: 85.0781 - val_mse: 69.3062\n",
      "Epoch 284/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 81.4308 - mse: 65.5720 - val_loss: 85.0278 - val_mse: 69.2044\n",
      "Epoch 285/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 83.2228 - mse: 67.4403 - val_loss: 83.9760 - val_mse: 68.3162\n",
      "Epoch 286/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 81.5937 - mse: 65.8652 - val_loss: 83.8739 - val_mse: 68.1434\n",
      "Epoch 287/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 82.0436 - mse: 66.3148 - val_loss: 86.7091 - val_mse: 71.0497\n",
      "Epoch 288/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 83.1875 - mse: 67.4235 - val_loss: 89.6615 - val_mse: 73.8948\n",
      "Epoch 289/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 82.2644 - mse: 66.4929 - val_loss: 86.4391 - val_mse: 70.7047\n",
      "Epoch 290/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 80.9050 - mse: 65.1100 - val_loss: 86.8927 - val_mse: 71.1884\n",
      "Epoch 291/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 81.3098 - mse: 65.5343 - val_loss: 82.7623 - val_mse: 67.0101\n",
      "Epoch 292/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 83.1249 - mse: 67.3204 - val_loss: 83.9050 - val_mse: 67.9827\n",
      "Epoch 293/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 81.9622 - mse: 66.1379 - val_loss: 83.4763 - val_mse: 67.7805\n",
      "Epoch 294/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 81.5180 - mse: 65.7830 - val_loss: 87.0678 - val_mse: 71.3218\n",
      "Epoch 295/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 81.3136 - mse: 65.5495 - val_loss: 81.4925 - val_mse: 65.8269\n",
      "Epoch 296/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 82.0931 - mse: 66.3627 - val_loss: 81.1489 - val_mse: 65.4690\n",
      "Epoch 297/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 81.2424 - mse: 65.4956 - val_loss: 83.0327 - val_mse: 67.3389\n",
      "Epoch 298/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 82.4076 - mse: 66.6749 - val_loss: 82.7622 - val_mse: 67.0219\n",
      "Epoch 299/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 81.7161 - mse: 65.9638 - val_loss: 81.3447 - val_mse: 65.6191\n",
      "Epoch 300/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 82.3729 - mse: 66.5984 - val_loss: 85.6405 - val_mse: 69.9074\n",
      "Epoch 301/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 81.9357 - mse: 66.1858 - val_loss: 86.4814 - val_mse: 70.6416\n",
      "Epoch 302/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 82.1621 - mse: 66.4229 - val_loss: 90.0996 - val_mse: 74.4915\n",
      "Epoch 303/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 81.9979 - mse: 66.2718 - val_loss: 81.6718 - val_mse: 65.8578\n",
      "Epoch 304/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 82.3451 - mse: 66.5429 - val_loss: 91.5871 - val_mse: 75.9439\n",
      "Epoch 305/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 82.4619 - mse: 66.7085 - val_loss: 78.9876 - val_mse: 63.2333\n",
      "Epoch 306/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 81.7773 - mse: 66.0203 - val_loss: 82.1321 - val_mse: 66.3519\n",
      "Epoch 307/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 81.0164 - mse: 65.2670 - val_loss: 85.5022 - val_mse: 69.8183\n",
      "Epoch 308/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 82.2054 - mse: 66.4707 - val_loss: 83.5342 - val_mse: 67.7941\n",
      "Epoch 309/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 81.6815 - mse: 65.9811 - val_loss: 83.8398 - val_mse: 68.1631\n",
      "Epoch 310/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 81.3537 - mse: 65.6480 - val_loss: 83.2397 - val_mse: 67.5335\n",
      "Epoch 311/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 80.4297 - mse: 64.7544 - val_loss: 84.6069 - val_mse: 68.9486\n",
      "Epoch 312/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 82.3097 - mse: 66.6503 - val_loss: 85.0068 - val_mse: 69.3033\n",
      "Epoch 313/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 81.9009 - mse: 66.2761 - val_loss: 79.9196 - val_mse: 64.3293\n",
      "Epoch 314/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 81.3435 - mse: 65.7708 - val_loss: 88.3207 - val_mse: 72.6122\n",
      "Epoch 315/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 81.2055 - mse: 65.5731 - val_loss: 84.3620 - val_mse: 68.7045\n",
      "Epoch 316/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 81.4702 - mse: 65.8218 - val_loss: 85.5834 - val_mse: 69.8536\n",
      "Epoch 317/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 81.5910 - mse: 65.9880 - val_loss: 78.6993 - val_mse: 63.1449\n",
      "Epoch 318/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 81.6974 - mse: 66.1011 - val_loss: 84.8085 - val_mse: 69.1487\n",
      "Epoch 319/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 81.6584 - mse: 66.0420 - val_loss: 80.9412 - val_mse: 65.3864\n",
      "Epoch 320/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 81.0806 - mse: 65.5227 - val_loss: 84.3319 - val_mse: 68.7441\n",
      "Epoch 321/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 81.4402 - mse: 65.8004 - val_loss: 79.4121 - val_mse: 63.7848\n",
      "Epoch 322/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 81.1060 - mse: 65.5004 - val_loss: 82.4253 - val_mse: 66.7798\n",
      "Epoch 323/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 80.7305 - mse: 65.0642 - val_loss: 79.9771 - val_mse: 64.1852\n",
      "Epoch 324/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 80.7041 - mse: 65.0383 - val_loss: 85.7182 - val_mse: 70.0558\n",
      "Epoch 325/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 80.5963 - mse: 65.0225 - val_loss: 82.2412 - val_mse: 66.7707\n",
      "Epoch 326/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 81.7074 - mse: 66.1777 - val_loss: 82.6860 - val_mse: 67.1352\n",
      "Epoch 327/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 80.9498 - mse: 65.3553 - val_loss: 82.0828 - val_mse: 66.4795\n",
      "Epoch 328/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 82.1446 - mse: 66.5722 - val_loss: 80.9959 - val_mse: 65.5670\n",
      "Epoch 329/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 80.2686 - mse: 64.7653 - val_loss: 79.2748 - val_mse: 63.8850\n",
      "Epoch 330/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 80.8893 - mse: 65.3510 - val_loss: 83.8207 - val_mse: 68.1621\n",
      "Epoch 331/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 80.5934 - mse: 65.0344 - val_loss: 86.1178 - val_mse: 70.6673\n",
      "Epoch 332/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 80.4549 - mse: 64.9372 - val_loss: 86.5747 - val_mse: 71.1046\n",
      "Epoch 333/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 81.0553 - mse: 65.5177 - val_loss: 82.8526 - val_mse: 67.2895\n",
      "Epoch 334/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 81.1974 - mse: 65.6875 - val_loss: 81.0560 - val_mse: 65.6073\n",
      "Epoch 335/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 82.1481 - mse: 66.6920 - val_loss: 93.1556 - val_mse: 77.6010\n",
      "Epoch 336/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 81.1960 - mse: 65.6951 - val_loss: 83.2494 - val_mse: 67.7537\n",
      "Epoch 337/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 81.2168 - mse: 65.7234 - val_loss: 83.9396 - val_mse: 68.4838\n",
      "Epoch 338/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 80.3430 - mse: 64.8136 - val_loss: 84.7468 - val_mse: 69.1385\n",
      "Epoch 339/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 80.9139 - mse: 65.3933 - val_loss: 81.8813 - val_mse: 66.3361\n",
      "Epoch 340/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 79.8028 - mse: 64.2394 - val_loss: 84.8948 - val_mse: 69.4579\n",
      "Epoch 341/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 80.0038 - mse: 64.5000 - val_loss: 83.0757 - val_mse: 67.5932\n",
      "Epoch 342/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 80.9969 - mse: 65.5663 - val_loss: 80.7854 - val_mse: 65.3242\n",
      "Epoch 343/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 80.8305 - mse: 65.3670 - val_loss: 79.1843 - val_mse: 63.8295\n",
      "Epoch 344/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 80.1585 - mse: 64.6971 - val_loss: 82.9264 - val_mse: 67.4412\n",
      "Epoch 345/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 79.8717 - mse: 64.4064 - val_loss: 82.0356 - val_mse: 66.5569\n",
      "Epoch 346/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 81.2554 - mse: 65.8094 - val_loss: 87.3450 - val_mse: 71.8522\n",
      "Epoch 347/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 80.5744 - mse: 65.1003 - val_loss: 85.0731 - val_mse: 69.6969\n",
      "Epoch 348/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 80.3874 - mse: 64.9934 - val_loss: 80.7177 - val_mse: 65.2713\n",
      "Epoch 349/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 80.5123 - mse: 65.1217 - val_loss: 79.1063 - val_mse: 63.6976\n",
      "Epoch 350/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 80.0936 - mse: 64.6806 - val_loss: 80.6267 - val_mse: 65.2423\n",
      "Epoch 351/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 80.3424 - mse: 64.9129 - val_loss: 84.1687 - val_mse: 68.7680\n",
      "Epoch 352/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 79.9717 - mse: 64.5587 - val_loss: 84.1783 - val_mse: 68.7821\n",
      "Epoch 353/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 80.1133 - mse: 64.7882 - val_loss: 81.6573 - val_mse: 66.3437\n",
      "Epoch 354/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 80.1171 - mse: 64.7755 - val_loss: 83.4000 - val_mse: 68.0592\n",
      "Epoch 355/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 79.6099 - mse: 64.2250 - val_loss: 80.8269 - val_mse: 65.4717\n",
      "Epoch 356/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 80.4682 - mse: 65.1368 - val_loss: 79.4531 - val_mse: 64.1844\n",
      "Epoch 357/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 80.0614 - mse: 64.7765 - val_loss: 84.0322 - val_mse: 68.7769\n",
      "Epoch 358/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 79.7506 - mse: 64.4413 - val_loss: 80.4496 - val_mse: 65.1004\n",
      "Epoch 359/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 81.1694 - mse: 65.8351 - val_loss: 82.6835 - val_mse: 67.3796\n",
      "Epoch 360/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 80.3655 - mse: 65.0501 - val_loss: 82.4158 - val_mse: 67.1967\n",
      "Epoch 361/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 79.8039 - mse: 64.4478 - val_loss: 80.9795 - val_mse: 65.7192\n",
      "Epoch 362/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 79.6003 - mse: 64.2747 - val_loss: 84.1945 - val_mse: 68.9088\n",
      "Epoch 363/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 79.5413 - mse: 64.1969 - val_loss: 81.5938 - val_mse: 66.2989\n",
      "Epoch 364/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 79.8915 - mse: 64.5460 - val_loss: 80.0855 - val_mse: 64.6989\n",
      "Epoch 365/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 79.1912 - mse: 63.8309 - val_loss: 84.4705 - val_mse: 69.2033\n",
      "Epoch 366/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 79.5709 - mse: 64.2174 - val_loss: 81.0007 - val_mse: 65.7506\n",
      "Epoch 367/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 79.0334 - mse: 63.6519 - val_loss: 82.1850 - val_mse: 66.8516\n",
      "Epoch 368/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 79.6776 - mse: 64.3355 - val_loss: 81.2767 - val_mse: 65.9843\n",
      "Epoch 369/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 79.7049 - mse: 64.3622 - val_loss: 83.7760 - val_mse: 68.5008\n",
      "Epoch 370/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 78.7644 - mse: 63.4152 - val_loss: 80.2872 - val_mse: 65.0106\n",
      "Epoch 371/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 79.8898 - mse: 64.6066 - val_loss: 85.6501 - val_mse: 70.4443\n",
      "Epoch 372/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 79.7153 - mse: 64.3914 - val_loss: 81.7808 - val_mse: 66.4397\n",
      "Epoch 373/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 78.1883 - mse: 62.8783 - val_loss: 80.6498 - val_mse: 65.3506\n",
      "Epoch 374/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 79.2147 - mse: 63.9869 - val_loss: 84.1865 - val_mse: 68.8976\n",
      "Epoch 375/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 79.7246 - mse: 64.4863 - val_loss: 83.3612 - val_mse: 68.1135\n",
      "Epoch 376/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 78.5840 - mse: 63.3188 - val_loss: 86.0651 - val_mse: 71.0647\n",
      "Epoch 377/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 79.3203 - mse: 64.1301 - val_loss: 79.3669 - val_mse: 64.2269\n",
      "Epoch 378/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 79.7420 - mse: 64.5190 - val_loss: 84.5731 - val_mse: 69.4417\n",
      "Epoch 379/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 79.8808 - mse: 64.6878 - val_loss: 81.1648 - val_mse: 65.9983\n",
      "Epoch 380/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 78.7528 - mse: 63.5790 - val_loss: 81.9927 - val_mse: 66.8334\n",
      "Epoch 381/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 78.4041 - mse: 63.2097 - val_loss: 79.0526 - val_mse: 63.9168\n",
      "Epoch 382/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 78.4173 - mse: 63.2282 - val_loss: 80.5294 - val_mse: 65.2567\n",
      "Epoch 383/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 77.5449 - mse: 62.3117 - val_loss: 79.3186 - val_mse: 64.0557\n",
      "Epoch 384/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 78.6743 - mse: 63.4173 - val_loss: 81.3996 - val_mse: 66.2125\n",
      "Epoch 385/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 79.0022 - mse: 63.7756 - val_loss: 80.5489 - val_mse: 65.2796\n",
      "Epoch 386/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 78.9492 - mse: 63.7190 - val_loss: 80.1733 - val_mse: 65.0154\n",
      "Epoch 387/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 78.8581 - mse: 63.6948 - val_loss: 79.1931 - val_mse: 63.9715\n",
      "Epoch 388/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 78.7849 - mse: 63.6032 - val_loss: 78.5060 - val_mse: 63.2697\n",
      "Epoch 389/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 78.5441 - mse: 63.3276 - val_loss: 78.5898 - val_mse: 63.4102\n",
      "Epoch 390/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 79.2831 - mse: 64.1254 - val_loss: 80.4628 - val_mse: 65.2958\n",
      "Epoch 391/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 78.6904 - mse: 63.5392 - val_loss: 82.3956 - val_mse: 67.3248\n",
      "Epoch 392/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 78.1934 - mse: 63.0332 - val_loss: 83.7159 - val_mse: 68.4451\n",
      "Epoch 393/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 78.5627 - mse: 63.3671 - val_loss: 81.3597 - val_mse: 66.1922\n",
      "Epoch 394/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 78.1096 - mse: 62.9473 - val_loss: 79.8798 - val_mse: 64.7655\n",
      "Epoch 395/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 79.4470 - mse: 64.2845 - val_loss: 82.5418 - val_mse: 67.4768\n",
      "Epoch 396/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 78.0605 - mse: 62.9558 - val_loss: 82.6132 - val_mse: 67.5980\n",
      "Epoch 397/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 78.5415 - mse: 63.4599 - val_loss: 78.1305 - val_mse: 63.0347\n",
      "Epoch 398/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 79.2953 - mse: 64.2166 - val_loss: 78.1115 - val_mse: 62.9748\n",
      "Epoch 399/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 78.2478 - mse: 63.1931 - val_loss: 82.2808 - val_mse: 67.2200\n",
      "Epoch 400/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 78.5478 - mse: 63.4784 - val_loss: 80.1923 - val_mse: 65.0632\n",
      "Epoch 401/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 77.3709 - mse: 62.2611 - val_loss: 81.3274 - val_mse: 66.2565\n",
      "Epoch 402/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 79.0914 - mse: 64.0099 - val_loss: 80.3982 - val_mse: 65.3645\n",
      "Epoch 403/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 78.8102 - mse: 63.7430 - val_loss: 81.7501 - val_mse: 66.6927\n",
      "Epoch 404/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 78.1982 - mse: 63.1227 - val_loss: 77.4125 - val_mse: 62.2531\n",
      "Epoch 405/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 78.0108 - mse: 62.9229 - val_loss: 78.7923 - val_mse: 63.8022\n",
      "Epoch 406/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 78.5906 - mse: 63.5132 - val_loss: 84.9878 - val_mse: 69.8746\n",
      "Epoch 407/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 78.5699 - mse: 63.4238 - val_loss: 79.7860 - val_mse: 64.7024\n",
      "Epoch 408/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 78.3549 - mse: 63.2695 - val_loss: 78.1148 - val_mse: 62.9370\n",
      "Epoch 409/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 78.0403 - mse: 62.9349 - val_loss: 79.6666 - val_mse: 64.6342\n",
      "Epoch 410/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 77.7659 - mse: 62.7149 - val_loss: 82.3018 - val_mse: 67.2642\n",
      "Epoch 411/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 77.8359 - mse: 62.7589 - val_loss: 83.1043 - val_mse: 67.9743\n",
      "Epoch 412/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 78.9477 - mse: 63.8747 - val_loss: 82.4323 - val_mse: 67.4640\n",
      "Epoch 413/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 77.6114 - mse: 62.6315 - val_loss: 78.9594 - val_mse: 63.9066\n",
      "Epoch 414/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 77.8796 - mse: 62.9040 - val_loss: 80.8839 - val_mse: 65.8954\n",
      "Epoch 415/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 78.9688 - mse: 64.0080 - val_loss: 81.2335 - val_mse: 66.1686\n",
      "Epoch 416/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 78.2163 - mse: 63.2314 - val_loss: 84.8137 - val_mse: 69.8744\n",
      "Epoch 417/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 78.2179 - mse: 63.2096 - val_loss: 80.9974 - val_mse: 65.9756\n",
      "Epoch 418/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 78.0884 - mse: 63.0866 - val_loss: 80.0389 - val_mse: 65.0701\n",
      "Epoch 419/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 78.3649 - mse: 63.3981 - val_loss: 85.3648 - val_mse: 70.5009\n",
      "Epoch 420/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 77.5941 - mse: 62.6301 - val_loss: 83.0682 - val_mse: 68.1225\n",
      "Epoch 421/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 78.6194 - mse: 63.6048 - val_loss: 83.0668 - val_mse: 68.0039\n",
      "Epoch 422/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 78.2286 - mse: 63.2682 - val_loss: 83.9349 - val_mse: 68.9411\n",
      "Epoch 423/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 78.2710 - mse: 63.3163 - val_loss: 80.5686 - val_mse: 65.6127\n",
      "Epoch 424/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 77.4126 - mse: 62.4571 - val_loss: 78.6550 - val_mse: 63.6637\n",
      "Epoch 425/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 77.1983 - mse: 62.2444 - val_loss: 79.3304 - val_mse: 64.3703\n",
      "Epoch 426/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 77.0712 - mse: 62.1358 - val_loss: 80.3645 - val_mse: 65.3953\n",
      "Epoch 427/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 78.6784 - mse: 63.7426 - val_loss: 80.2943 - val_mse: 65.4287\n",
      "Epoch 428/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 77.9334 - mse: 63.0149 - val_loss: 81.9747 - val_mse: 67.0773\n",
      "Epoch 429/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 78.6795 - mse: 63.7912 - val_loss: 82.1323 - val_mse: 67.2565\n",
      "Epoch 430/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 77.7661 - mse: 62.8484 - val_loss: 82.5629 - val_mse: 67.5911\n",
      "Epoch 431/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 78.3025 - mse: 63.4198 - val_loss: 82.0179 - val_mse: 67.0846\n",
      "Epoch 432/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 77.9256 - mse: 62.9899 - val_loss: 78.2179 - val_mse: 63.2959\n",
      "Epoch 433/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 78.0907 - mse: 63.1171 - val_loss: 81.9116 - val_mse: 66.9665\n",
      "Epoch 434/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 77.2635 - mse: 62.2806 - val_loss: 80.7126 - val_mse: 65.8409\n",
      "Epoch 435/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 77.5483 - mse: 62.6134 - val_loss: 81.4166 - val_mse: 66.3911\n",
      "Epoch 436/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 78.4636 - mse: 63.4940 - val_loss: 80.0434 - val_mse: 65.1843\n",
      "Epoch 437/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 77.4464 - mse: 62.5453 - val_loss: 78.7346 - val_mse: 63.8979\n",
      "Epoch 438/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 78.2489 - mse: 63.3540 - val_loss: 81.9585 - val_mse: 67.1598\n",
      "Epoch 439/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 77.0248 - mse: 62.1444 - val_loss: 77.9145 - val_mse: 63.0351\n",
      "Epoch 440/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 76.9206 - mse: 62.0053 - val_loss: 78.0329 - val_mse: 63.2044\n",
      "Epoch 441/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 76.6318 - mse: 61.7972 - val_loss: 77.9314 - val_mse: 63.0323\n",
      "Epoch 442/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 77.8299 - mse: 62.9733 - val_loss: 79.3204 - val_mse: 64.6367\n",
      "Epoch 443/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 77.6685 - mse: 62.8509 - val_loss: 89.2046 - val_mse: 74.3128\n",
      "Epoch 444/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 77.3219 - mse: 62.4934 - val_loss: 81.8350 - val_mse: 67.1224\n",
      "Epoch 445/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 77.6829 - mse: 62.8182 - val_loss: 77.5713 - val_mse: 62.8480\n",
      "Epoch 446/500\n",
      "905/905 [==============================] - 2s 3ms/step - loss: 78.3018 - mse: 63.4424 - val_loss: 79.5131 - val_mse: 64.6303\n",
      "Epoch 447/500\n",
      "905/905 [==============================] - 3s 3ms/step - loss: 77.4997 - mse: 62.6673 - val_loss: 78.5729 - val_mse: 63.8194\n",
      "Epoch 448/500\n",
      "905/905 [==============================] - 2s 3ms/step - loss: 77.4628 - mse: 62.6316 - val_loss: 75.9745 - val_mse: 61.1149\n",
      "Epoch 449/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 76.8787 - mse: 62.0583 - val_loss: 78.9817 - val_mse: 64.2505\n",
      "Epoch 450/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 77.2093 - mse: 62.4402 - val_loss: 83.2623 - val_mse: 68.5056\n",
      "Epoch 451/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 76.9350 - mse: 62.1286 - val_loss: 82.8306 - val_mse: 67.9898\n",
      "Epoch 452/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 76.4883 - mse: 61.7640 - val_loss: 76.5648 - val_mse: 61.7845\n",
      "Epoch 453/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 77.0664 - mse: 62.3655 - val_loss: 82.5403 - val_mse: 67.8097\n",
      "Epoch 454/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 76.7877 - mse: 62.0221 - val_loss: 79.0011 - val_mse: 64.1820\n",
      "Epoch 455/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 77.6365 - mse: 62.8630 - val_loss: 85.8576 - val_mse: 71.1389\n",
      "Epoch 456/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 77.6272 - mse: 62.9386 - val_loss: 81.2629 - val_mse: 66.5663\n",
      "Epoch 457/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 76.9638 - mse: 62.2637 - val_loss: 93.9321 - val_mse: 79.0150\n",
      "Epoch 458/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 76.7963 - mse: 62.0366 - val_loss: 77.3577 - val_mse: 62.5883\n",
      "Epoch 459/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 76.7997 - mse: 62.0214 - val_loss: 78.8209 - val_mse: 64.1247\n",
      "Epoch 460/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 77.5831 - mse: 62.8155 - val_loss: 78.2964 - val_mse: 63.5636\n",
      "Epoch 461/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 75.8840 - mse: 61.1404 - val_loss: 77.9597 - val_mse: 63.1839\n",
      "Epoch 462/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 77.2579 - mse: 62.5532 - val_loss: 81.0058 - val_mse: 66.3884\n",
      "Epoch 463/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 76.8906 - mse: 62.1707 - val_loss: 79.0257 - val_mse: 64.2781\n",
      "Epoch 464/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 76.6310 - mse: 61.8764 - val_loss: 81.4077 - val_mse: 66.6241\n",
      "Epoch 465/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 76.7185 - mse: 61.9475 - val_loss: 77.7793 - val_mse: 63.0242\n",
      "Epoch 466/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 76.5508 - mse: 61.8136 - val_loss: 76.5483 - val_mse: 61.7312\n",
      "Epoch 467/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 77.0019 - mse: 62.2643 - val_loss: 77.2593 - val_mse: 62.6113\n",
      "Epoch 468/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 77.2125 - mse: 62.5451 - val_loss: 77.0945 - val_mse: 62.3532\n",
      "Epoch 469/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 77.1878 - mse: 62.4602 - val_loss: 77.9560 - val_mse: 63.2178\n",
      "Epoch 470/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 76.7183 - mse: 62.0176 - val_loss: 78.9055 - val_mse: 64.2784\n",
      "Epoch 471/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 76.5526 - mse: 61.8770 - val_loss: 77.3867 - val_mse: 62.8169\n",
      "Epoch 472/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 76.7496 - mse: 62.1223 - val_loss: 76.0736 - val_mse: 61.4838\n",
      "Epoch 473/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 76.3689 - mse: 61.6897 - val_loss: 80.2722 - val_mse: 65.7334\n",
      "Epoch 474/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 77.2231 - mse: 62.5793 - val_loss: 79.1923 - val_mse: 64.5067\n",
      "Epoch 475/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 76.3169 - mse: 61.5906 - val_loss: 79.4854 - val_mse: 64.7756\n",
      "Epoch 476/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 76.7491 - mse: 62.0581 - val_loss: 79.1099 - val_mse: 64.4852\n",
      "Epoch 477/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 76.8895 - mse: 62.2447 - val_loss: 78.4798 - val_mse: 63.7873\n",
      "Epoch 478/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 76.8422 - mse: 62.1834 - val_loss: 78.3908 - val_mse: 63.7975\n",
      "Epoch 479/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 76.3636 - mse: 61.6622 - val_loss: 82.4096 - val_mse: 67.7173\n",
      "Epoch 480/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 76.5640 - mse: 61.8975 - val_loss: 82.0054 - val_mse: 67.3372\n",
      "Epoch 481/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 76.4676 - mse: 61.8276 - val_loss: 80.6570 - val_mse: 66.0012\n",
      "Epoch 482/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 76.8579 - mse: 62.1952 - val_loss: 82.3276 - val_mse: 67.7823\n",
      "Epoch 483/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 76.7666 - mse: 62.1192 - val_loss: 78.2347 - val_mse: 63.5913\n",
      "Epoch 484/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 76.2445 - mse: 61.5570 - val_loss: 78.8276 - val_mse: 64.1133\n",
      "Epoch 485/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 76.5095 - mse: 61.8572 - val_loss: 77.8057 - val_mse: 63.2352\n",
      "Epoch 486/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 75.9532 - mse: 61.3534 - val_loss: 80.4205 - val_mse: 65.8561\n",
      "Epoch 487/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 76.6444 - mse: 62.0727 - val_loss: 78.0456 - val_mse: 63.4572\n",
      "Epoch 488/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 75.8988 - mse: 61.3339 - val_loss: 80.3839 - val_mse: 65.8387\n",
      "Epoch 489/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 76.8660 - mse: 62.2930 - val_loss: 80.6548 - val_mse: 66.1329\n",
      "Epoch 490/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 76.7902 - mse: 62.2110 - val_loss: 83.9214 - val_mse: 69.3859\n",
      "Epoch 491/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 76.4529 - mse: 61.8598 - val_loss: 81.7876 - val_mse: 67.2074\n",
      "Epoch 492/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 76.2026 - mse: 61.6285 - val_loss: 81.5463 - val_mse: 66.9259\n",
      "Epoch 493/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 77.1232 - mse: 62.5842 - val_loss: 80.0770 - val_mse: 65.5584\n",
      "Epoch 494/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 76.5328 - mse: 61.9823 - val_loss: 79.2694 - val_mse: 64.6968\n",
      "Epoch 495/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 76.1032 - mse: 61.5437 - val_loss: 82.2127 - val_mse: 67.7008\n",
      "Epoch 496/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 75.6823 - mse: 61.1665 - val_loss: 80.1150 - val_mse: 65.6340\n",
      "Epoch 497/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 76.5519 - mse: 62.0138 - val_loss: 81.9508 - val_mse: 67.3877\n",
      "Epoch 498/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 77.5923 - mse: 63.0811 - val_loss: 88.2618 - val_mse: 73.6437\n",
      "Epoch 499/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 76.3656 - mse: 61.8303 - val_loss: 81.5113 - val_mse: 66.8886\n",
      "Epoch 500/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 76.7971 - mse: 62.2153 - val_loss: 82.2454 - val_mse: 67.7135\n"
     ]
    }
   ],
   "source": [
    "# fit\n",
    "h_history_nn = h_model_nn.fit(h_X_nn_train_sc, h_y_nn_train, epochs = 500, verbose = 1, \n",
    "                          validation_data = (h_X_nn_test_sc, h_y_nn_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "034ebf42-97f6-4c45-8afe-b6aa38a878a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x360 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#breakfast hour\n",
    "# plot train and test loss (mse)\n",
    "fig, ax = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "ax[0].plot(h_history_nn.history['loss'], color = 'green')\n",
    "ax[1].plot(h_history_nn.history['val_loss'], color = 'blue')\n",
    "\n",
    "ax[0].set_title('Train')\n",
    "ax[0].set_xlabel('epochs')\n",
    "ax[0].set_ylabel('Loss = MSE')\n",
    "\n",
    "ax[1].set_title('Test')\n",
    "ax[1].set_xlabel('epochs')\n",
    "ax[1].set_ylabel('Loss = MSE');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12875aee-46ef-4b23-b129-06eeb40217af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "302/302 [==============================] - 0s 902us/step\n",
      "Test R2: 0.7415364631613148\n",
      "Mean Absolute Error: 16.63429069519043\n",
      "Huber Loss: 16.141130447387695\n"
     ]
    }
   ],
   "source": [
    "# https://medium.com/towards-data-science/loss-functions-and-their-use-in-neural-networks-a470e703f1e9\n",
    "h_nn_y_true = h_y_nn_test\n",
    "h_nn_y_pred = h_model_nn.predict(h_X_nn_test_sc)\n",
    "\n",
    "# R2\n",
    "print(f'Test R2: {r2_score(h_nn_y_true, h_nn_y_pred)}')\n",
    "\n",
    "# MAE\n",
    "mae = MeanAbsoluteError()\n",
    "print(f'Mean Absolute Error: {mae(h_nn_y_true, h_nn_y_pred)}')\n",
    "\n",
    "# Huber\n",
    "huber = Huber()\n",
    "print(f'Huber Loss: {huber(h_nn_y_true, h_nn_y_pred)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e93ab764-1d3e-421a-ac8d-ddd2befb1385",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
