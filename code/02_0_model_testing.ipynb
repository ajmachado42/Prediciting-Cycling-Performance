{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ba2e640d-2908-4fd0-bdf3-78f0c301dd65",
   "metadata": {},
   "source": [
    "# Model Testing\n",
    "---\n",
    "---\n",
    "### *Commented out models to prevent run times; results are in the commented out portions.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b4b83d0-7973-42c4-8713-7c6cc5dc39bb",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    "---\n",
    "## [Average Cycling Model Testing](#Average-Cycling-Performance-Model-Testing)\n",
    "> ### [Average Cycling Best Model](#Average-Cycling-Performance-Best-Model---XGBoost)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3237b7f-7fc2-4010-b60b-c7d880f28dd7",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Sources and Adaptations From\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9e3e2a61-2500-4f10-9091-992cc2485733",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Various modeling lessons and breakfast hours from DSI 523\n",
    "# https://medium.com/towards-data-science/loss-functions-and-their-use-in-neural-networks-a470e703f1e9\n",
    "# https://www.tensorflow.org/api_docs/python/tf/keras/losses/MeanAbsoluteError\n",
    "# https://www.tensorflow.org/api_docs/python/tf/keras/losses/Huber\n",
    "# https://towardsdatascience.com/what-is-batch-normalization-46058b4f583\n",
    "# https://www.tensorflow.org/tutorials/keras/save_and_load#save_the_entire_model\n",
    "# https://machinelearningmastery.com/xgboost-for-regression/\n",
    "# https://towardsdatascience.com/getting-started-with-xgboost-in-scikit-learn-f69f5f470a97"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9509e54f-20d0-4638-aebd-93d7a93be9f1",
   "metadata": {},
   "source": [
    "# Imports\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b828ad9a-87a1-4fcc-ab0b-1a27c6ba6159",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "pd.set_option('display.max_columns', None)\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import PolynomialFeatures, StandardScaler\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import Ridge, Lasso, ElasticNet\n",
    "from sklearn.ensemble import GradientBoostingRegressor, AdaBoostRegressor, VotingRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import AdaBoostRegressor, RandomForestRegressor\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.regularizers import l1, l2, l1_l2\n",
    "from tensorflow.keras.losses import MeanAbsoluteError, Huber\n",
    "\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ca0ce27d-3d8f-42de-9ea2-616dbf621a46",
   "metadata": {},
   "outputs": [],
   "source": [
    "a_df = pd.read_csv('../data/average/a_df.csv')\n",
    "h_df = pd.read_csv('../data/high/h_df.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16db770f-e93d-4bfb-9395-8176710ffbd9",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Average Cycling Performance Model Testing\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6ab7c15d-bf3f-4083-b508-3cd326ce0bd1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>elevation</th>\n",
       "      <th>dt</th>\n",
       "      <th>heart_rate</th>\n",
       "      <th>bearing</th>\n",
       "      <th>time_diff_s</th>\n",
       "      <th>total_time_s</th>\n",
       "      <th>ele_diff_m</th>\n",
       "      <th>total_ele_change_m</th>\n",
       "      <th>lat_lon</th>\n",
       "      <th>dist_diff_km</th>\n",
       "      <th>total_dist_km</th>\n",
       "      <th>temp</th>\n",
       "      <th>feels_like</th>\n",
       "      <th>pressure</th>\n",
       "      <th>humidity</th>\n",
       "      <th>dew_point</th>\n",
       "      <th>clouds</th>\n",
       "      <th>wind_speed</th>\n",
       "      <th>wind_deg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2022-07-20 16:07:45+00:00</td>\n",
       "      <td>38.773466</td>\n",
       "      <td>-121.363686</td>\n",
       "      <td>35.799999</td>\n",
       "      <td>1658333265</td>\n",
       "      <td>78</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>(38.77346634864807, -121.36368582956493)</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>297.65</td>\n",
       "      <td>297.17</td>\n",
       "      <td>1019</td>\n",
       "      <td>39</td>\n",
       "      <td>282.80</td>\n",
       "      <td>1</td>\n",
       "      <td>0.45</td>\n",
       "      <td>177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2022-07-20 16:07:46+00:00</td>\n",
       "      <td>38.773542</td>\n",
       "      <td>-121.363672</td>\n",
       "      <td>35.599998</td>\n",
       "      <td>1658333266</td>\n",
       "      <td>79</td>\n",
       "      <td>8.292053</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.200001</td>\n",
       "      <td>-0.2</td>\n",
       "      <td>(38.77354153431952, -121.36367183178663)</td>\n",
       "      <td>0.0084</td>\n",
       "      <td>0.0084</td>\n",
       "      <td>297.65</td>\n",
       "      <td>297.17</td>\n",
       "      <td>1019</td>\n",
       "      <td>39</td>\n",
       "      <td>282.80</td>\n",
       "      <td>1</td>\n",
       "      <td>0.45</td>\n",
       "      <td>177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2022-07-20 16:07:49+00:00</td>\n",
       "      <td>38.773630</td>\n",
       "      <td>-121.363682</td>\n",
       "      <td>35.200001</td>\n",
       "      <td>1658333269</td>\n",
       "      <td>82</td>\n",
       "      <td>-5.321180</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>-0.399998</td>\n",
       "      <td>-0.6</td>\n",
       "      <td>(38.77363029867411, -121.36368239298463)</td>\n",
       "      <td>0.0099</td>\n",
       "      <td>0.0183</td>\n",
       "      <td>297.65</td>\n",
       "      <td>297.17</td>\n",
       "      <td>1019</td>\n",
       "      <td>39</td>\n",
       "      <td>282.80</td>\n",
       "      <td>1</td>\n",
       "      <td>0.45</td>\n",
       "      <td>177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2022-07-20 16:07:51+00:00</td>\n",
       "      <td>38.773789</td>\n",
       "      <td>-121.363733</td>\n",
       "      <td>35.000000</td>\n",
       "      <td>1658333271</td>\n",
       "      <td>83</td>\n",
       "      <td>-13.956066</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>-0.200001</td>\n",
       "      <td>-0.8</td>\n",
       "      <td>(38.77378871664405, -121.36373268440366)</td>\n",
       "      <td>0.0181</td>\n",
       "      <td>0.0364</td>\n",
       "      <td>297.67</td>\n",
       "      <td>297.17</td>\n",
       "      <td>1019</td>\n",
       "      <td>38</td>\n",
       "      <td>282.43</td>\n",
       "      <td>1</td>\n",
       "      <td>0.45</td>\n",
       "      <td>177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2022-07-20 16:07:52+00:00</td>\n",
       "      <td>38.773786</td>\n",
       "      <td>-121.363766</td>\n",
       "      <td>35.000000</td>\n",
       "      <td>1658333272</td>\n",
       "      <td>83</td>\n",
       "      <td>-96.936537</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.8</td>\n",
       "      <td>(38.77378553152084, -121.36376612819731)</td>\n",
       "      <td>0.0029</td>\n",
       "      <td>0.0393</td>\n",
       "      <td>297.67</td>\n",
       "      <td>297.17</td>\n",
       "      <td>1019</td>\n",
       "      <td>38</td>\n",
       "      <td>282.43</td>\n",
       "      <td>1</td>\n",
       "      <td>0.45</td>\n",
       "      <td>177</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   timestamp   latitude   longitude  elevation          dt  \\\n",
       "0  2022-07-20 16:07:45+00:00  38.773466 -121.363686  35.799999  1658333265   \n",
       "1  2022-07-20 16:07:46+00:00  38.773542 -121.363672  35.599998  1658333266   \n",
       "2  2022-07-20 16:07:49+00:00  38.773630 -121.363682  35.200001  1658333269   \n",
       "3  2022-07-20 16:07:51+00:00  38.773789 -121.363733  35.000000  1658333271   \n",
       "4  2022-07-20 16:07:52+00:00  38.773786 -121.363766  35.000000  1658333272   \n",
       "\n",
       "   heart_rate    bearing  time_diff_s  total_time_s  ele_diff_m  \\\n",
       "0          78   0.000000            0             0    0.000000   \n",
       "1          79   8.292053            1             1   -0.200001   \n",
       "2          82  -5.321180            3             4   -0.399998   \n",
       "3          83 -13.956066            2             6   -0.200001   \n",
       "4          83 -96.936537            1             7    0.000000   \n",
       "\n",
       "   total_ele_change_m                                   lat_lon  dist_diff_km  \\\n",
       "0                 0.0  (38.77346634864807, -121.36368582956493)        0.0000   \n",
       "1                -0.2  (38.77354153431952, -121.36367183178663)        0.0084   \n",
       "2                -0.6  (38.77363029867411, -121.36368239298463)        0.0099   \n",
       "3                -0.8  (38.77378871664405, -121.36373268440366)        0.0181   \n",
       "4                -0.8  (38.77378553152084, -121.36376612819731)        0.0029   \n",
       "\n",
       "   total_dist_km    temp  feels_like  pressure  humidity  dew_point  clouds  \\\n",
       "0         0.0000  297.65      297.17      1019        39     282.80       1   \n",
       "1         0.0084  297.65      297.17      1019        39     282.80       1   \n",
       "2         0.0183  297.65      297.17      1019        39     282.80       1   \n",
       "3         0.0364  297.67      297.17      1019        38     282.43       1   \n",
       "4         0.0393  297.67      297.17      1019        38     282.43       1   \n",
       "\n",
       "   wind_speed  wind_deg  \n",
       "0        0.45       177  \n",
       "1        0.45       177  \n",
       "2        0.45       177  \n",
       "3        0.45       177  \n",
       "4        0.45       177  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4acc5f0b-46be-4d35-87ce-5e50e80274f6",
   "metadata": {
    "tags": []
   },
   "source": [
    "## X, y, train_test_split\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1422d14a-0130-44c4-afe3-74ee147cd7a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['timestamp', 'latitude', 'longitude', 'elevation', 'dt', 'heart_rate',\n",
       "       'bearing', 'time_diff_s', 'total_time_s', 'ele_diff_m',\n",
       "       'total_ele_change_m', 'lat_lon', 'dist_diff_km', 'total_dist_km',\n",
       "       'temp', 'feels_like', 'pressure', 'humidity', 'dew_point', 'clouds',\n",
       "       'wind_speed', 'wind_deg'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a7112d65-747f-47d3-a69d-5a905518bddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "a_features = ['elevation', 'bearing', 'time_diff_s', 'total_time_s', 'ele_diff_m',\n",
    "       'total_ele_change_m', 'dist_diff_km', 'total_dist_km',\n",
    "       'temp', 'feels_like', 'pressure', 'humidity', 'dew_point', 'clouds',\n",
    "       'wind_speed', 'wind_deg']\n",
    "a_X = a_df[a_features]\n",
    "a_y = a_df['heart_rate']\n",
    "\n",
    "a_X_train, a_X_test, a_y_train, a_y_test = train_test_split(a_X, a_y, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "58b0ddc5-1911-439c-89ec-342d06ab9dfe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "130.21652421652422"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a_y.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5eab97a-12cc-46e7-b1bb-b0222df5cd69",
   "metadata": {},
   "source": [
    "### StandardScaler X_train and X_test for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8902c045-7211-4157-9248-2fb9e0426418",
   "metadata": {},
   "outputs": [],
   "source": [
    "a_ss = StandardScaler()\n",
    "a_X_train_sc = a_ss.fit_transform(a_X_train)\n",
    "a_X_test_sc = a_ss.transform(a_X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04b67ac0-6d16-4efe-b05c-b03bcea45e88",
   "metadata": {},
   "source": [
    "### Polynomial X_train and X_test for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4f429dcf-6b97-424d-ac99-0f12184d3266",
   "metadata": {},
   "outputs": [],
   "source": [
    "a_poly = PolynomialFeatures()\n",
    "a_X_train_sc_p = a_poly.fit_transform(a_X_train_sc)\n",
    "a_X_test_sc_p = a_poly.fit_transform(a_X_test_sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e10c0a5a-9580-4cb1-8a6b-d50c38e4879a",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Linear Regression\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2308b699-e547-4eb1-914d-5d06f13712bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nLinear Regression Train R2 Score: 0.5260233155015783\\nLinear Regression Test R2 Score: 0.5322513167399172\\n'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "a_lr = LinearRegression()\n",
    "a_lr.fit(a_X_train_sc, a_y_train)\n",
    "print(f'Linear Regression Train R2 Score: {a_lr.score(a_X_train_sc, a_y_train)}')\n",
    "print(f'Linear Regression Test R2 Score: {a_lr.score(a_X_test_sc, a_y_test)}')\n",
    "'''\n",
    "'''\n",
    "Linear Regression Train R2 Score: 0.5260233155015783\n",
    "Linear Regression Test R2 Score: 0.5322513167399172\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a3eba4a-d843-40fe-894f-437fc51e4da6",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Linear Regression Pipeline (StandardScaler, Polynomial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7dbc010d-fd67-4801-ad74-b6108a94f87b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR Pipe Train R2 Score: 0.8082482755481244\n",
      "LR Pipe Test R2 Score: 0.7906546450250773\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "a_lr_pipe = Pipeline([\n",
    "    ('a_ss', StandardScaler()),\n",
    "    ('a_poly', PolynomialFeatures()),\n",
    "    ('a_lr', LinearRegression())\n",
    "])\n",
    "\n",
    "a_lr_pipe.fit(a_X_train, a_y_train)\n",
    "print(f'LR Pipe Train R2 Score: {a_lr_pipe.score(a_X_train, a_y_train)}')\n",
    "print(f'LR Pipe Test R2 Score: {a_lr_pipe.score(a_X_test, a_y_test)}')\n",
    "'''\n",
    "'''\n",
    "LR Pipe Train R2 Score: 0.8082482755481244\n",
    "LR Pipe Test R2 Score: 0.7906546450250773\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe5836ba-85f8-42db-af96-34667942de0f",
   "metadata": {},
   "source": [
    "## Regressor Boosting\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c3242f8-96db-41b8-9fca-e0b32a8aa4cc",
   "metadata": {
    "tags": []
   },
   "source": [
    "### AdaBoost with Linear Regression Base Estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "968bf2bf-2df7-48f1-9bfc-0f59053ef06c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ada LR Train R2 Score: 0.8046423913777858\n",
      "Ada LR Test R2 Score: 0.776694380832758\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "a_ada_lr = AdaBoostRegressor(base_estimator = LinearRegression(), random_state = 42)\n",
    "\n",
    "a_ada_lr.fit(a_X_train_sc_p, a_y_train)\n",
    "print(f'Ada LR Train R2 Score: {a_ada_lr.score(a_X_train_sc_p, a_y_train)}')\n",
    "print(f'Ada LR Test R2 Score: {a_ada_lr.score(a_X_test_sc_p, a_y_test)}')\n",
    "'''\n",
    "'''\n",
    "Ada LR Train R2 Score: 0.8046423913777858\n",
    "Ada LR Test R2 Score: 0.776694380832758\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4dcc69d-c9c3-47cb-a6e9-a1e24411836d",
   "metadata": {
    "tags": []
   },
   "source": [
    "### AdaBoost with Decision Tree Base Estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fc0afdc7-652b-4526-a213-bfca4d321bb9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n0.868164587298871\\n{'base_estimator__max_depth': 5, 'n_estimators': 200}\\nAda DT Train R2 Score: 0.8756108134812447\\nAda DT Test R2 Score: 0.8730399805443708\\n\""
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "a_ada_dt = AdaBoostRegressor(base_estimator = DecisionTreeRegressor(), random_state = 42)\n",
    "\n",
    "a_ada_dt_params = {\n",
    "    'n_estimators': [50, 100, 150, 200],\n",
    "    'base_estimator__max_depth': [1, 2, 5]\n",
    "}\n",
    "\n",
    "a_gs_ada_dt = GridSearchCV(a_ada_dt, param_grid = a_ada_dt_params, cv = 5)\n",
    "a_gs_ada_dt.fit(a_X_train_sc, a_y_train)\n",
    "print(a_gs_ada_dt.best_score_)\n",
    "print(a_gs_ada_dt.best_params_)\n",
    "print(f'Ada DT Train R2 Score: {a_gs_ada_dt.score(a_X_train_sc, a_y_train)}')\n",
    "print(f'Ada DT Test R2 Score: {a_gs_ada_dt.score(a_X_test_sc, a_y_test)}')\n",
    "'''\n",
    "\n",
    "'''\n",
    "0.868164587298871\n",
    "{'base_estimator__max_depth': 5, 'n_estimators': 200}\n",
    "Ada DT Train R2 Score: 0.8756108134812447\n",
    "Ada DT Test R2 Score: 0.8730399805443708\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6155407f-7539-4cbe-bca7-8b766fd70b94",
   "metadata": {
    "tags": []
   },
   "source": [
    "### AdaBoost with Random Forest Base Estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5de35077-25ad-44a6-bb79-dc018699bd4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n0.844253394135492\\n{'base_estimator__max_depth': 5, 'n_estimators': 50}\\nAda DT Train R2 Score: 0.8526320077631239\\nAda DT Test R2 Score: 0.8512220020729748\\n\""
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "a_ada_rf = AdaBoostRegressor(base_estimator = RandomForestRegressor(), random_state = 42)\n",
    "\n",
    "a_ada_rf_params = {\n",
    "    'n_estimators': [50, 100, 150, 200],\n",
    "    'base_estimator__max_depth': [1, 2, 5]\n",
    "}\n",
    "\n",
    "a_gs_ada_rf = GridSearchCV(a_ada_rf, param_grid = a_ada_rf_params, cv = 5)\n",
    "a_gs_ada_rf.fit(a_X_train_sc, a_y_train)\n",
    "print(a_gs_ada_rf.best_score_)\n",
    "print(a_gs_ada_rf.best_params_)\n",
    "print(f'Ada DT Train R2 Score: {a_gs_ada_rf.score(a_X_train_sc, a_y_train)}')\n",
    "print(f'Ada DT Test R2 Score: {a_gs_ada_rf.score(a_X_test_sc, a_y_test)}')\n",
    "'''\n",
    "'''\n",
    "0.844253394135492\n",
    "{'base_estimator__max_depth': 5, 'n_estimators': 50}\n",
    "Ada DT Train R2 Score: 0.8526320077631239\n",
    "Ada DT Test R2 Score: 0.8512220020729748\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c920337-6425-47be-9aaa-60769c0e1f00",
   "metadata": {},
   "source": [
    "### Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1f14bbf6-8fe7-4203-ba97-93d56b5a5234",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nGridSearch Best Score: 0.964003581281308\\n{'learning_rate': 0.8, 'max_depth': 4, 'n_estimators': 150}\\nGBoost Train R2 Score: 0.9888934084696182\\nGBoost Test R2 Score: 0.9709606783777225\\nMean Absolute Error: 2.7824393977965642\\nMean Squared Error: 14.658521217024555\\n\""
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "# Instantiate\n",
    "a_gboost = GradientBoostingRegressor()\n",
    "\n",
    "a_gboost_params = {\n",
    "    'max_depth': [2, 3, 4],\n",
    "    'n_estimators': [100, 125, 150],\n",
    "    'learning_rate': [0.8, .1, .12]\n",
    "}\n",
    "\n",
    "# Gridsearch\n",
    "a_gb_gs = GridSearchCV(a_gboost, param_grid = a_gboost_params, cv = 5)\n",
    "a_gb_gs.fit(a_X_train_sc, a_y_train)\n",
    "\n",
    "# Metrics\n",
    "print(f'GridSearch Best Score: {a_gb_gs.best_score_}')\n",
    "print(a_gb_gs.best_params_)\n",
    "print(f'GBoost Train R2 Score: {a_gb_gs.score(a_X_train_sc, a_y_train)}')\n",
    "print(f'GBoost Test R2 Score: {a_gb_gs.score(a_X_test_sc, a_y_test)}')\n",
    "\n",
    "a_gb_y_true = a_y_test\n",
    "a_gb_y_pred = a_gb_gs.predict(a_X_test_sc)\n",
    "\n",
    "# MAE\n",
    "print(f'Mean Absolute Error: {mean_absolute_error(a_gb_y_true, a_gb_y_pred)}')\n",
    "\n",
    "# MSE\n",
    "print(f'Mean Squared Error: {mean_squared_error(a_gb_y_true, a_gb_y_pred)}')\n",
    "'''\n",
    "'''\n",
    "GridSearch Best Score: 0.964003581281308\n",
    "{'learning_rate': 0.8, 'max_depth': 4, 'n_estimators': 150}\n",
    "GBoost Train R2 Score: 0.9888934084696182\n",
    "GBoost Test R2 Score: 0.9709606783777225\n",
    "Mean Absolute Error: 2.7824393977965642\n",
    "Mean Squared Error: 14.658521217024555\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4181f707-d03f-4ee2-b37b-1118d1248c1f",
   "metadata": {},
   "source": [
    "### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bbad08d0-11cc-4eec-8800-6c20b255c9d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GridSearch Best Score: 0.9659966627636546\n",
      "{'learning_rate': 0.8, 'max_depth': 4, 'n_estimators': 150}\n",
      "XGBoost Train R2 Score: 0.9888381686104007\n",
      "XGBoost Test R2 Score: 0.9719382323708556\n",
      "Mean Absolute Error: 2.7619688805285496\n",
      "Mean Squared Error: 14.165069746789941\n"
     ]
    }
   ],
   "source": [
    "# Instantiate\n",
    "a_xgboost = XGBRegressor()\n",
    "\n",
    "# Gridsearch\n",
    "a_xgboost_params = {\n",
    "    'max_depth': [2, 3, 4],\n",
    "    'n_estimators': [100, 125, 150],\n",
    "    'learning_rate': [0.8, .1, .12]\n",
    "}\n",
    "\n",
    "a_xgb_gs = GridSearchCV(a_xgboost, param_grid = a_xgboost_params, cv = 5)\n",
    "a_xgb_gs.fit(a_X_train_sc, a_y_train)\n",
    "\n",
    "# Metrics\n",
    "print(f'GridSearch Best Score: {a_xgb_gs.best_score_}')\n",
    "print(a_xgb_gs.best_params_)\n",
    "print(f'XGBoost Train R2 Score: {a_xgb_gs.score(a_X_train_sc, a_y_train)}')\n",
    "print(f'XGBoost Test R2 Score: {a_xgb_gs.score(a_X_test_sc, a_y_test)}')\n",
    "\n",
    "a_xgb_y_true = a_y_test\n",
    "a_xgb_y_pred = a_xgb_gs.predict(a_X_test_sc)\n",
    "\n",
    "# MAE\n",
    "print(f'Mean Absolute Error: {mean_absolute_error(a_xgb_y_true, a_xgb_y_pred)}')\n",
    "\n",
    "# MSE\n",
    "print(f'Mean Squared Error: {mean_squared_error(a_xgb_y_true, a_xgb_y_pred)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d8ca1c8-b384-47d3-996b-6f29e207438b",
   "metadata": {},
   "source": [
    "## Neural Net Regressor\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22cacd20-80c3-48ac-bea0-c84c0c8cd227",
   "metadata": {},
   "source": [
    "### X, y, train_test_split, StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "72562d7c-5ea8-442a-9d86-9e02f13c5004",
   "metadata": {},
   "outputs": [],
   "source": [
    "a_X_nn = a_df[a_features]\n",
    "a_y_nn = a_df['heart_rate']\n",
    "\n",
    "a_X_nn = np.array(a_X_nn)\n",
    "a_y_nn = np.array(a_y_nn)\n",
    "\n",
    "a_X_nn_train, a_X_nn_test, a_y_nn_train, a_y_nn_test = train_test_split(a_X_nn, a_y_nn, random_state = 42)\n",
    "\n",
    "a_ss_nn = StandardScaler()\n",
    "a_X_nn_train_sc = a_ss_nn.fit_transform(a_X_nn_train)\n",
    "a_X_nn_test_sc = a_ss.transform(a_X_nn_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2407b7a2-6e73-4b0e-a006-93ffbc16cbe4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16,)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a_X_nn[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ef54f2ba-d2bd-4c98-b754-af095d7e5d76",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-12 08:23:52.042300: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# Instantiate\n",
    "a_model_nn = Sequential()\n",
    "\n",
    "# Layers\n",
    "a_model_nn.add(Dense(128, input_dim = 16, activation = 'relu'))\n",
    "\n",
    "a_model_nn.add(BatchNormalization())\n",
    "a_model_nn.add(Dense(64, activation = 'relu', kernel_regularizer = l2(.1)))\n",
    "a_model_nn.add(Dense(128, activation = 'relu', kernel_regularizer = l2(.5)))\n",
    "a_model_nn.add(Dense(64, activation = 'relu', kernel_regularizer = l2(.1)))\n",
    "a_model_nn.add(Dense(128, activation = 'relu', kernel_regularizer = l2(.5))) \n",
    "a_model_nn.add(Dense(64, activation = 'relu', kernel_regularizer = l2(.1))) \n",
    "a_model_nn.add(Dense(1, kernel_regularizer = l2(.5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d60e4577-0412-443b-94e0-ea16b121a107",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile\n",
    "a_model_nn.compile(optimizer = 'adam', loss = 'mse', metrics = ['mse'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b676e67c-ea0c-4e85-88ea-8c8361d2fc68",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "354/354 [==============================] - 2s 3ms/step - loss: 1431.5569 - mse: 1336.9702 - val_loss: 719.7565 - val_mse: 636.5491\n",
      "Epoch 2/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 238.4683 - mse: 161.1318 - val_loss: 202.1411 - val_mse: 129.7049\n",
      "Epoch 3/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 204.2786 - mse: 135.8692 - val_loss: 201.0665 - val_mse: 136.2994\n",
      "Epoch 4/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 187.6803 - mse: 125.9558 - val_loss: 198.6508 - val_mse: 139.8193\n",
      "Epoch 5/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 167.9991 - mse: 111.7216 - val_loss: 140.2455 - val_mse: 86.3514\n",
      "Epoch 6/500\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 159.1123 - mse: 107.4396 - val_loss: 143.0365 - val_mse: 93.4364\n",
      "Epoch 7/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 147.8757 - mse: 100.1865 - val_loss: 156.3131 - val_mse: 110.5034\n",
      "Epoch 8/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 143.5465 - mse: 99.3519 - val_loss: 120.9134 - val_mse: 78.2970\n",
      "Epoch 9/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 138.9812 - mse: 97.8999 - val_loss: 114.7422 - val_mse: 74.9964\n",
      "Epoch 10/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 130.1016 - mse: 91.7054 - val_loss: 114.5353 - val_mse: 77.3990\n",
      "Epoch 11/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 128.0018 - mse: 92.0667 - val_loss: 111.2339 - val_mse: 76.4627\n",
      "Epoch 12/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 126.7040 - mse: 92.9527 - val_loss: 134.5781 - val_mse: 101.7697\n",
      "Epoch 13/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 117.8374 - mse: 86.0297 - val_loss: 118.7461 - val_mse: 87.8848\n",
      "Epoch 14/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 114.7156 - mse: 84.6499 - val_loss: 98.4742 - val_mse: 69.1507\n",
      "Epoch 15/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 115.0007 - mse: 86.4945 - val_loss: 116.1808 - val_mse: 88.4606\n",
      "Epoch 16/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 108.2421 - mse: 81.1687 - val_loss: 94.8732 - val_mse: 68.4781\n",
      "Epoch 17/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 103.8375 - mse: 78.0680 - val_loss: 98.7403 - val_mse: 73.6114\n",
      "Epoch 18/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 100.4458 - mse: 75.8748 - val_loss: 88.6015 - val_mse: 64.5786\n",
      "Epoch 19/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 101.1415 - mse: 77.6497 - val_loss: 116.0532 - val_mse: 93.1519\n",
      "Epoch 20/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 101.8054 - mse: 79.2813 - val_loss: 91.4183 - val_mse: 69.3788\n",
      "Epoch 21/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 95.5061 - mse: 73.8635 - val_loss: 87.1688 - val_mse: 65.9427\n",
      "Epoch 22/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 99.0504 - mse: 78.2069 - val_loss: 82.8861 - val_mse: 62.3788\n",
      "Epoch 23/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 93.3101 - mse: 73.2029 - val_loss: 80.3541 - val_mse: 60.5562\n",
      "Epoch 24/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 90.8315 - mse: 71.4070 - val_loss: 75.3884 - val_mse: 56.3110\n",
      "Epoch 25/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 93.1257 - mse: 74.3209 - val_loss: 74.7657 - val_mse: 56.2293\n",
      "Epoch 26/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 93.0080 - mse: 74.7532 - val_loss: 78.7540 - val_mse: 60.7461\n",
      "Epoch 27/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 88.1824 - mse: 70.4318 - val_loss: 70.7760 - val_mse: 53.2786\n",
      "Epoch 28/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 87.8439 - mse: 70.5901 - val_loss: 75.1577 - val_mse: 58.1277\n",
      "Epoch 29/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 85.1940 - mse: 68.3722 - val_loss: 75.4306 - val_mse: 58.8349\n",
      "Epoch 30/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 84.6982 - mse: 68.3092 - val_loss: 77.7995 - val_mse: 61.6746\n",
      "Epoch 31/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 85.0281 - mse: 69.0612 - val_loss: 83.3325 - val_mse: 67.6119\n",
      "Epoch 32/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 85.1066 - mse: 69.5077 - val_loss: 83.4195 - val_mse: 68.0329\n",
      "Epoch 33/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 82.1572 - mse: 66.8811 - val_loss: 76.4784 - val_mse: 61.3490\n",
      "Epoch 34/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 82.2768 - mse: 67.2958 - val_loss: 82.0055 - val_mse: 67.2278\n",
      "Epoch 35/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 81.1937 - mse: 66.5101 - val_loss: 70.4701 - val_mse: 55.9583\n",
      "Epoch 36/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 80.3561 - mse: 65.9463 - val_loss: 73.4803 - val_mse: 59.2390\n",
      "Epoch 37/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 78.9505 - mse: 64.7859 - val_loss: 68.3289 - val_mse: 54.2754\n",
      "Epoch 38/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 76.3511 - mse: 62.4317 - val_loss: 68.2505 - val_mse: 54.4166\n",
      "Epoch 39/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 81.6823 - mse: 67.9543 - val_loss: 72.3885 - val_mse: 58.8174\n",
      "Epoch 40/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 77.7255 - mse: 64.2514 - val_loss: 93.2643 - val_mse: 79.9363\n",
      "Epoch 41/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 83.9649 - mse: 70.6965 - val_loss: 80.9183 - val_mse: 67.7297\n",
      "Epoch 42/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 77.5585 - mse: 64.4856 - val_loss: 64.4256 - val_mse: 51.4210\n",
      "Epoch 43/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 75.9361 - mse: 63.0142 - val_loss: 69.0402 - val_mse: 56.1372\n",
      "Epoch 44/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 75.1828 - mse: 62.4159 - val_loss: 80.1607 - val_mse: 67.5555\n",
      "Epoch 45/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 72.6099 - mse: 60.0288 - val_loss: 64.4774 - val_mse: 51.9659\n",
      "Epoch 46/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 74.7422 - mse: 62.3233 - val_loss: 67.7795 - val_mse: 55.3822\n",
      "Epoch 47/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 76.3967 - mse: 64.1234 - val_loss: 70.2132 - val_mse: 58.0458\n",
      "Epoch 48/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 72.6914 - mse: 60.5507 - val_loss: 63.2987 - val_mse: 51.2254\n",
      "Epoch 49/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 74.2040 - mse: 62.2137 - val_loss: 78.6412 - val_mse: 66.7431\n",
      "Epoch 50/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 73.0412 - mse: 61.2038 - val_loss: 63.6392 - val_mse: 51.8847\n",
      "Epoch 51/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 72.8602 - mse: 61.1766 - val_loss: 63.5273 - val_mse: 51.8672\n",
      "Epoch 52/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 69.5415 - mse: 57.9605 - val_loss: 61.6225 - val_mse: 50.1021\n",
      "Epoch 53/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 70.5263 - mse: 59.0653 - val_loss: 76.5272 - val_mse: 65.1637\n",
      "Epoch 54/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 71.6709 - mse: 60.3268 - val_loss: 76.3579 - val_mse: 64.9929\n",
      "Epoch 55/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 72.1948 - mse: 60.9547 - val_loss: 58.2827 - val_mse: 47.1030\n",
      "Epoch 56/500\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 70.9187 - mse: 59.7806 - val_loss: 81.3841 - val_mse: 70.2379\n",
      "Epoch 57/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 69.0513 - mse: 58.0234 - val_loss: 80.4652 - val_mse: 69.4273\n",
      "Epoch 58/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 73.3102 - mse: 62.3592 - val_loss: 57.6881 - val_mse: 46.7271\n",
      "Epoch 59/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 67.3819 - mse: 56.5214 - val_loss: 71.7001 - val_mse: 60.8786\n",
      "Epoch 60/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 69.0918 - mse: 58.3157 - val_loss: 58.6651 - val_mse: 47.9291\n",
      "Epoch 61/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 67.0190 - mse: 56.3144 - val_loss: 69.9966 - val_mse: 59.3015\n",
      "Epoch 62/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 68.7442 - mse: 58.1267 - val_loss: 60.2669 - val_mse: 49.7026\n",
      "Epoch 63/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 67.2873 - mse: 56.7432 - val_loss: 68.8747 - val_mse: 58.3374\n",
      "Epoch 64/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 68.0640 - mse: 57.6029 - val_loss: 59.2967 - val_mse: 48.8651\n",
      "Epoch 65/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 67.1977 - mse: 56.7819 - val_loss: 73.2691 - val_mse: 62.9287\n",
      "Epoch 66/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 67.8253 - mse: 57.4660 - val_loss: 53.5922 - val_mse: 43.2775\n",
      "Epoch 67/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 64.6143 - mse: 54.3246 - val_loss: 58.3466 - val_mse: 48.0798\n",
      "Epoch 68/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 66.8970 - mse: 56.6686 - val_loss: 55.5059 - val_mse: 45.2913\n",
      "Epoch 69/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 67.7617 - mse: 57.5646 - val_loss: 56.9597 - val_mse: 46.8190\n",
      "Epoch 70/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 65.5819 - mse: 55.4350 - val_loss: 63.6061 - val_mse: 53.5312\n",
      "Epoch 71/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 66.2002 - mse: 56.1013 - val_loss: 81.3959 - val_mse: 71.4121\n",
      "Epoch 72/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 65.2794 - mse: 55.2469 - val_loss: 61.9831 - val_mse: 51.9782\n",
      "Epoch 73/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 65.5746 - mse: 55.6089 - val_loss: 56.0658 - val_mse: 46.1183\n",
      "Epoch 74/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 65.1684 - mse: 55.2461 - val_loss: 100.8868 - val_mse: 91.0981\n",
      "Epoch 75/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 66.5890 - mse: 56.6890 - val_loss: 58.2372 - val_mse: 48.3532\n",
      "Epoch 76/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 64.6483 - mse: 54.7776 - val_loss: 59.1978 - val_mse: 49.3261\n",
      "Epoch 77/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 65.8013 - mse: 55.9964 - val_loss: 54.9531 - val_mse: 45.1819\n",
      "Epoch 78/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 62.5716 - mse: 52.7977 - val_loss: 54.7041 - val_mse: 44.9349\n",
      "Epoch 79/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 63.3117 - mse: 53.5855 - val_loss: 67.0031 - val_mse: 57.2252\n",
      "Epoch 80/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 65.9479 - mse: 56.2422 - val_loss: 56.3022 - val_mse: 46.6008\n",
      "Epoch 81/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 63.2543 - mse: 53.5822 - val_loss: 56.3767 - val_mse: 46.6863\n",
      "Epoch 82/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 63.6894 - mse: 54.0352 - val_loss: 54.2596 - val_mse: 44.5868\n",
      "Epoch 83/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 65.5602 - mse: 55.9363 - val_loss: 73.9348 - val_mse: 64.2677\n",
      "Epoch 84/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 64.1870 - mse: 54.6217 - val_loss: 58.4956 - val_mse: 48.9236\n",
      "Epoch 85/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 63.5334 - mse: 54.0006 - val_loss: 58.4563 - val_mse: 48.9167\n",
      "Epoch 86/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 63.4550 - mse: 53.9240 - val_loss: 52.7255 - val_mse: 43.1862\n",
      "Epoch 87/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 61.3840 - mse: 51.8880 - val_loss: 52.5617 - val_mse: 43.0700\n",
      "Epoch 88/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 60.4355 - mse: 50.9769 - val_loss: 70.0793 - val_mse: 60.6761\n",
      "Epoch 89/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 63.3324 - mse: 53.8757 - val_loss: 55.6923 - val_mse: 46.2986\n",
      "Epoch 90/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 65.0211 - mse: 55.6107 - val_loss: 54.4046 - val_mse: 44.9724\n",
      "Epoch 91/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 61.6142 - mse: 52.2004 - val_loss: 54.6199 - val_mse: 45.2418\n",
      "Epoch 92/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 61.7558 - mse: 52.3894 - val_loss: 55.0740 - val_mse: 45.7251\n",
      "Epoch 93/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 62.2844 - mse: 52.9393 - val_loss: 56.4012 - val_mse: 47.0821\n",
      "Epoch 94/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 62.0043 - mse: 52.7037 - val_loss: 51.6415 - val_mse: 42.3648\n",
      "Epoch 95/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 63.4331 - mse: 54.1694 - val_loss: 54.6193 - val_mse: 45.3483\n",
      "Epoch 96/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 60.1577 - mse: 50.9247 - val_loss: 57.0396 - val_mse: 47.8677\n",
      "Epoch 97/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 61.2949 - mse: 52.0764 - val_loss: 53.3433 - val_mse: 44.1464\n",
      "Epoch 98/500\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 60.5835 - mse: 51.4038 - val_loss: 59.7293 - val_mse: 50.6019\n",
      "Epoch 99/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 59.7587 - mse: 50.5854 - val_loss: 53.0320 - val_mse: 43.8460\n",
      "Epoch 100/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 62.1948 - mse: 53.0427 - val_loss: 60.9585 - val_mse: 51.7844\n",
      "Epoch 101/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 59.3991 - mse: 50.2851 - val_loss: 52.4794 - val_mse: 43.3622\n",
      "Epoch 102/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 60.3952 - mse: 51.3041 - val_loss: 64.4832 - val_mse: 55.3508\n",
      "Epoch 103/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 62.0859 - mse: 52.9948 - val_loss: 56.8593 - val_mse: 47.8387\n",
      "Epoch 104/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 57.5850 - mse: 48.5114 - val_loss: 54.8282 - val_mse: 45.7716\n",
      "Epoch 105/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 59.1779 - mse: 50.1304 - val_loss: 50.9114 - val_mse: 41.8796\n",
      "Epoch 106/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 59.3964 - mse: 50.3593 - val_loss: 54.5262 - val_mse: 45.5235\n",
      "Epoch 107/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 60.2955 - mse: 51.2973 - val_loss: 81.7161 - val_mse: 72.6346\n",
      "Epoch 108/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 59.5459 - mse: 50.5594 - val_loss: 51.5658 - val_mse: 42.6351\n",
      "Epoch 109/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 60.2083 - mse: 51.2460 - val_loss: 50.4303 - val_mse: 41.4773\n",
      "Epoch 110/500\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 61.2212 - mse: 52.2535 - val_loss: 55.1380 - val_mse: 46.1456\n",
      "Epoch 111/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 60.4100 - mse: 51.4473 - val_loss: 51.4150 - val_mse: 42.4769\n",
      "Epoch 112/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 57.3737 - mse: 48.4192 - val_loss: 52.0469 - val_mse: 43.1213\n",
      "Epoch 113/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 58.1435 - mse: 49.2100 - val_loss: 56.8030 - val_mse: 47.9017\n",
      "Epoch 114/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 57.1395 - mse: 48.1917 - val_loss: 69.6359 - val_mse: 60.6211\n",
      "Epoch 115/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 57.2305 - mse: 48.3247 - val_loss: 54.5602 - val_mse: 45.6290\n",
      "Epoch 116/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 57.1834 - mse: 48.2986 - val_loss: 53.8290 - val_mse: 44.9003\n",
      "Epoch 117/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 59.1659 - mse: 50.2790 - val_loss: 51.0574 - val_mse: 42.1981\n",
      "Epoch 118/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 59.3011 - mse: 50.4196 - val_loss: 50.8186 - val_mse: 41.9695\n",
      "Epoch 119/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 58.7726 - mse: 49.8995 - val_loss: 51.1848 - val_mse: 42.2874\n",
      "Epoch 120/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 58.8051 - mse: 49.8980 - val_loss: 51.0736 - val_mse: 42.1357\n",
      "Epoch 121/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 59.9923 - mse: 51.0743 - val_loss: 53.5066 - val_mse: 44.6249\n",
      "Epoch 122/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 60.3337 - mse: 51.4141 - val_loss: 66.8855 - val_mse: 57.9112\n",
      "Epoch 123/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 58.0341 - mse: 49.1255 - val_loss: 49.7773 - val_mse: 40.8534\n",
      "Epoch 124/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 57.5835 - mse: 48.6688 - val_loss: 50.4291 - val_mse: 41.5236\n",
      "Epoch 125/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 58.8850 - mse: 49.9462 - val_loss: 56.5836 - val_mse: 47.6537\n",
      "Epoch 126/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 55.3750 - mse: 46.4314 - val_loss: 51.9722 - val_mse: 43.0264\n",
      "Epoch 127/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 55.7268 - mse: 46.7889 - val_loss: 47.3004 - val_mse: 38.3667\n",
      "Epoch 128/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 57.3784 - mse: 48.4454 - val_loss: 56.5214 - val_mse: 47.5728\n",
      "Epoch 129/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 57.5913 - mse: 48.6333 - val_loss: 53.4687 - val_mse: 44.4774\n",
      "Epoch 130/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 57.0401 - mse: 48.0760 - val_loss: 54.2692 - val_mse: 45.2779\n",
      "Epoch 131/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 55.0210 - mse: 46.0672 - val_loss: 50.1515 - val_mse: 41.2050\n",
      "Epoch 132/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 56.0998 - mse: 47.1590 - val_loss: 51.9587 - val_mse: 43.0385\n",
      "Epoch 133/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 56.2886 - mse: 47.3309 - val_loss: 48.5732 - val_mse: 39.6053\n",
      "Epoch 134/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 57.1303 - mse: 48.1491 - val_loss: 63.3450 - val_mse: 54.4327\n",
      "Epoch 135/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 56.8353 - mse: 47.8397 - val_loss: 51.7510 - val_mse: 42.7370\n",
      "Epoch 136/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 58.1539 - mse: 49.1550 - val_loss: 54.8182 - val_mse: 45.8265\n",
      "Epoch 137/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 56.0307 - mse: 47.0245 - val_loss: 59.7080 - val_mse: 50.6286\n",
      "Epoch 138/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 54.7872 - mse: 45.7624 - val_loss: 60.7461 - val_mse: 51.7759\n",
      "Epoch 139/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 56.4412 - mse: 47.3910 - val_loss: 50.0036 - val_mse: 40.9336\n",
      "Epoch 140/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 54.7917 - mse: 45.7376 - val_loss: 52.0097 - val_mse: 42.9536\n",
      "Epoch 141/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 56.8302 - mse: 47.7708 - val_loss: 56.0998 - val_mse: 46.9954\n",
      "Epoch 142/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 55.7495 - mse: 46.6655 - val_loss: 50.8130 - val_mse: 41.7394\n",
      "Epoch 143/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 54.6267 - mse: 45.5018 - val_loss: 54.6251 - val_mse: 45.4648\n",
      "Epoch 144/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 55.3549 - mse: 46.2407 - val_loss: 49.2613 - val_mse: 40.1403\n",
      "Epoch 145/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 54.8758 - mse: 45.7292 - val_loss: 52.2493 - val_mse: 43.1200\n",
      "Epoch 146/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 55.3495 - mse: 46.1995 - val_loss: 48.7441 - val_mse: 39.6059\n",
      "Epoch 147/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 55.5707 - mse: 46.4330 - val_loss: 50.0389 - val_mse: 40.8909\n",
      "Epoch 148/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 54.6847 - mse: 45.5243 - val_loss: 46.8680 - val_mse: 37.7417\n",
      "Epoch 149/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 54.5099 - mse: 45.3572 - val_loss: 49.4423 - val_mse: 40.3213\n",
      "Epoch 150/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 54.5677 - mse: 45.4025 - val_loss: 47.9393 - val_mse: 38.7654\n",
      "Epoch 151/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 55.7365 - mse: 46.5392 - val_loss: 46.7775 - val_mse: 37.5688\n",
      "Epoch 152/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 56.2101 - mse: 46.9938 - val_loss: 49.7681 - val_mse: 40.5709\n",
      "Epoch 153/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 54.5794 - mse: 45.3616 - val_loss: 58.7669 - val_mse: 49.4989\n",
      "Epoch 154/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 53.8095 - mse: 44.5905 - val_loss: 48.1108 - val_mse: 38.8862\n",
      "Epoch 155/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 54.3300 - mse: 45.1045 - val_loss: 50.0891 - val_mse: 40.8655\n",
      "Epoch 156/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 53.9940 - mse: 44.7592 - val_loss: 49.7224 - val_mse: 40.4625\n",
      "Epoch 157/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 53.7971 - mse: 44.5543 - val_loss: 47.8636 - val_mse: 38.6165\n",
      "Epoch 158/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 55.2244 - mse: 45.9641 - val_loss: 48.1139 - val_mse: 38.8577\n",
      "Epoch 159/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 53.5899 - mse: 44.2994 - val_loss: 50.9145 - val_mse: 41.6481\n",
      "Epoch 160/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 55.3943 - mse: 46.0770 - val_loss: 47.2846 - val_mse: 37.9740\n",
      "Epoch 161/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 52.4086 - mse: 43.0827 - val_loss: 53.1122 - val_mse: 43.7442\n",
      "Epoch 162/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 54.2057 - mse: 44.8652 - val_loss: 57.7586 - val_mse: 48.3738\n",
      "Epoch 163/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 52.3749 - mse: 43.0479 - val_loss: 47.8520 - val_mse: 38.5018\n",
      "Epoch 164/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 53.2988 - mse: 43.9344 - val_loss: 64.5395 - val_mse: 55.1243\n",
      "Epoch 165/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 54.9390 - mse: 45.5873 - val_loss: 46.3729 - val_mse: 37.0142\n",
      "Epoch 166/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 53.1355 - mse: 43.7810 - val_loss: 50.3361 - val_mse: 40.9883\n",
      "Epoch 167/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 51.6378 - mse: 42.2770 - val_loss: 50.1643 - val_mse: 40.8360\n",
      "Epoch 168/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 53.2714 - mse: 43.9003 - val_loss: 47.8040 - val_mse: 38.4195\n",
      "Epoch 169/500\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 52.7941 - mse: 43.4183 - val_loss: 50.3465 - val_mse: 40.9511\n",
      "Epoch 170/500\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 52.4636 - mse: 43.0944 - val_loss: 46.6142 - val_mse: 37.2302\n",
      "Epoch 171/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 51.8226 - mse: 42.4368 - val_loss: 54.0953 - val_mse: 44.7436\n",
      "Epoch 172/500\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 52.3381 - mse: 42.9170 - val_loss: 45.6516 - val_mse: 36.2192\n",
      "Epoch 173/500\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 53.7013 - mse: 44.2818 - val_loss: 52.1843 - val_mse: 42.7523\n",
      "Epoch 174/500\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 53.1844 - mse: 43.7717 - val_loss: 49.4463 - val_mse: 40.0213\n",
      "Epoch 175/500\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 52.2833 - mse: 42.8558 - val_loss: 50.3069 - val_mse: 40.9159\n",
      "Epoch 176/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 53.2444 - mse: 43.8031 - val_loss: 48.1668 - val_mse: 38.7277\n",
      "Epoch 177/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 51.9544 - mse: 42.5076 - val_loss: 47.3389 - val_mse: 37.9210\n",
      "Epoch 178/500\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 51.4756 - mse: 42.0166 - val_loss: 51.0200 - val_mse: 41.5134\n",
      "Epoch 179/500\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 51.9751 - mse: 42.4797 - val_loss: 46.0806 - val_mse: 36.5824\n",
      "Epoch 180/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 51.8103 - mse: 42.3336 - val_loss: 47.5989 - val_mse: 38.1552\n",
      "Epoch 181/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 51.9314 - mse: 42.4498 - val_loss: 50.0857 - val_mse: 40.5651\n",
      "Epoch 182/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 52.0238 - mse: 42.5092 - val_loss: 46.8022 - val_mse: 37.2689\n",
      "Epoch 183/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 51.5490 - mse: 42.0232 - val_loss: 45.9598 - val_mse: 36.4504\n",
      "Epoch 184/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 52.2293 - mse: 42.7144 - val_loss: 46.7077 - val_mse: 37.2174\n",
      "Epoch 185/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 51.8843 - mse: 42.3673 - val_loss: 53.0412 - val_mse: 43.4763\n",
      "Epoch 186/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 52.0147 - mse: 42.4853 - val_loss: 58.4804 - val_mse: 48.8972\n",
      "Epoch 187/500\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 52.3895 - mse: 42.8569 - val_loss: 51.2833 - val_mse: 41.8112\n",
      "Epoch 188/500\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 51.9708 - mse: 42.4340 - val_loss: 58.3976 - val_mse: 48.9366\n",
      "Epoch 189/500\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 51.7801 - mse: 42.2321 - val_loss: 55.3138 - val_mse: 45.7097\n",
      "Epoch 190/500\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 51.4416 - mse: 41.8813 - val_loss: 49.2722 - val_mse: 39.7175\n",
      "Epoch 191/500\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 49.7926 - mse: 40.2388 - val_loss: 46.7315 - val_mse: 37.1522\n",
      "Epoch 192/500\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 52.2651 - mse: 42.7084 - val_loss: 50.1583 - val_mse: 40.5541\n",
      "Epoch 193/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 50.9974 - mse: 41.4196 - val_loss: 46.6806 - val_mse: 37.0897\n",
      "Epoch 194/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 51.4904 - mse: 41.9080 - val_loss: 49.6057 - val_mse: 40.0380\n",
      "Epoch 195/500\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 51.5811 - mse: 42.0184 - val_loss: 44.1993 - val_mse: 34.6499\n",
      "Epoch 196/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 50.9730 - mse: 41.4278 - val_loss: 51.0143 - val_mse: 41.4915\n",
      "Epoch 197/500\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 50.3647 - mse: 40.7896 - val_loss: 49.1334 - val_mse: 39.5119\n",
      "Epoch 198/500\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 51.6889 - mse: 42.0951 - val_loss: 50.7691 - val_mse: 41.1906\n",
      "Epoch 199/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 49.7773 - mse: 40.1801 - val_loss: 50.5862 - val_mse: 41.0402\n",
      "Epoch 200/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 49.2753 - mse: 39.6522 - val_loss: 46.8487 - val_mse: 37.2173\n",
      "Epoch 201/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 51.8550 - mse: 42.2098 - val_loss: 46.7127 - val_mse: 37.0713\n",
      "Epoch 202/500\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 50.0181 - mse: 40.3884 - val_loss: 47.1462 - val_mse: 37.5118\n",
      "Epoch 203/500\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 49.3197 - mse: 39.6984 - val_loss: 45.9561 - val_mse: 36.3450\n",
      "Epoch 204/500\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 49.7717 - mse: 40.1479 - val_loss: 46.4754 - val_mse: 36.8943\n",
      "Epoch 205/500\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 50.0399 - mse: 40.4216 - val_loss: 45.1925 - val_mse: 35.5654\n",
      "Epoch 206/500\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 50.7492 - mse: 41.1113 - val_loss: 48.3333 - val_mse: 38.7047\n",
      "Epoch 207/500\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 50.3054 - mse: 40.6526 - val_loss: 49.6474 - val_mse: 40.0279\n",
      "Epoch 208/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 49.9804 - mse: 40.3572 - val_loss: 50.0552 - val_mse: 40.3861\n",
      "Epoch 209/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 50.2325 - mse: 40.5844 - val_loss: 46.1261 - val_mse: 36.4799\n",
      "Epoch 210/500\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 51.0367 - mse: 41.3989 - val_loss: 46.4654 - val_mse: 36.8392\n",
      "Epoch 211/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 51.3353 - mse: 41.6720 - val_loss: 47.5140 - val_mse: 37.8451\n",
      "Epoch 212/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 49.0713 - mse: 39.4158 - val_loss: 56.2525 - val_mse: 46.5526\n",
      "Epoch 213/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 49.1302 - mse: 39.4604 - val_loss: 44.0322 - val_mse: 34.3563\n",
      "Epoch 214/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 49.8515 - mse: 40.1838 - val_loss: 51.2510 - val_mse: 41.5068\n",
      "Epoch 215/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 49.3540 - mse: 39.6617 - val_loss: 46.4665 - val_mse: 36.7627\n",
      "Epoch 216/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 49.4849 - mse: 39.7768 - val_loss: 45.3037 - val_mse: 35.5607\n",
      "Epoch 217/500\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 49.7491 - mse: 40.0227 - val_loss: 47.6675 - val_mse: 37.9500\n",
      "Epoch 218/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 48.1661 - mse: 38.4418 - val_loss: 46.0715 - val_mse: 36.3388\n",
      "Epoch 219/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 50.2305 - mse: 40.4988 - val_loss: 46.9621 - val_mse: 37.2702\n",
      "Epoch 220/500\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 50.2129 - mse: 40.5067 - val_loss: 49.0074 - val_mse: 39.2710\n",
      "Epoch 221/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 48.3945 - mse: 38.6764 - val_loss: 44.2535 - val_mse: 34.5560\n",
      "Epoch 222/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 48.8354 - mse: 39.1087 - val_loss: 45.9354 - val_mse: 36.1845\n",
      "Epoch 223/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 48.3689 - mse: 38.6421 - val_loss: 47.3362 - val_mse: 37.6692\n",
      "Epoch 224/500\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 50.4383 - mse: 40.7369 - val_loss: 44.2601 - val_mse: 34.5465\n",
      "Epoch 225/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 51.0865 - mse: 41.3531 - val_loss: 46.1415 - val_mse: 36.4051\n",
      "Epoch 226/500\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 48.7983 - mse: 39.0696 - val_loss: 46.4108 - val_mse: 36.6733\n",
      "Epoch 227/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 48.1868 - mse: 38.4594 - val_loss: 57.1141 - val_mse: 47.4211\n",
      "Epoch 228/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 49.8394 - mse: 40.1039 - val_loss: 47.2270 - val_mse: 37.4782\n",
      "Epoch 229/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 48.4190 - mse: 38.6671 - val_loss: 51.3864 - val_mse: 41.6752\n",
      "Epoch 230/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 49.8382 - mse: 40.0775 - val_loss: 50.0519 - val_mse: 40.2289\n",
      "Epoch 231/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 48.5683 - mse: 38.8169 - val_loss: 45.8984 - val_mse: 36.1610\n",
      "Epoch 232/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 49.7914 - mse: 40.0178 - val_loss: 49.0540 - val_mse: 39.2546\n",
      "Epoch 233/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 47.7003 - mse: 37.9424 - val_loss: 49.3254 - val_mse: 39.6205\n",
      "Epoch 234/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 49.1485 - mse: 39.3869 - val_loss: 44.4316 - val_mse: 34.6242\n",
      "Epoch 235/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 49.5016 - mse: 39.7074 - val_loss: 49.2625 - val_mse: 39.5163\n",
      "Epoch 236/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 49.0365 - mse: 39.2345 - val_loss: 53.4859 - val_mse: 43.6412\n",
      "Epoch 237/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 48.3446 - mse: 38.5439 - val_loss: 45.6471 - val_mse: 35.8653\n",
      "Epoch 238/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 49.3179 - mse: 39.5225 - val_loss: 46.5991 - val_mse: 36.8446\n",
      "Epoch 239/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 47.3053 - mse: 37.5250 - val_loss: 45.9429 - val_mse: 36.1309\n",
      "Epoch 240/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 48.3741 - mse: 38.5624 - val_loss: 44.8911 - val_mse: 35.0815\n",
      "Epoch 241/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 49.0519 - mse: 39.2356 - val_loss: 50.8428 - val_mse: 40.9992\n",
      "Epoch 242/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 48.1326 - mse: 38.3415 - val_loss: 45.6824 - val_mse: 35.9133\n",
      "Epoch 243/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 48.0227 - mse: 38.2109 - val_loss: 46.2664 - val_mse: 36.4236\n",
      "Epoch 244/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 47.7832 - mse: 37.9657 - val_loss: 46.2023 - val_mse: 36.4179\n",
      "Epoch 245/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 47.9550 - mse: 38.1159 - val_loss: 44.0053 - val_mse: 34.1841\n",
      "Epoch 246/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 49.1596 - mse: 39.3305 - val_loss: 44.6274 - val_mse: 34.7864\n",
      "Epoch 247/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 47.5017 - mse: 37.6753 - val_loss: 49.2333 - val_mse: 39.4232\n",
      "Epoch 248/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 48.5964 - mse: 38.7710 - val_loss: 47.8899 - val_mse: 38.0949\n",
      "Epoch 249/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 49.3603 - mse: 39.5399 - val_loss: 51.0022 - val_mse: 41.1763\n",
      "Epoch 250/500\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 48.0038 - mse: 38.1752 - val_loss: 45.5652 - val_mse: 35.7110\n",
      "Epoch 251/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 46.9069 - mse: 37.0806 - val_loss: 47.4478 - val_mse: 37.6442\n",
      "Epoch 252/500\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 47.9551 - mse: 38.1311 - val_loss: 43.1759 - val_mse: 33.3779\n",
      "Epoch 253/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 47.2446 - mse: 37.4416 - val_loss: 49.7146 - val_mse: 39.9828\n",
      "Epoch 254/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 48.2495 - mse: 38.4558 - val_loss: 47.3075 - val_mse: 37.5381\n",
      "Epoch 255/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 47.5508 - mse: 37.7568 - val_loss: 45.2294 - val_mse: 35.4723\n",
      "Epoch 256/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 47.6496 - mse: 37.8635 - val_loss: 49.6068 - val_mse: 39.8372\n",
      "Epoch 257/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 48.2168 - mse: 38.4287 - val_loss: 50.5767 - val_mse: 40.7483\n",
      "Epoch 258/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 47.2572 - mse: 37.4630 - val_loss: 44.2648 - val_mse: 34.4591\n",
      "Epoch 259/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 45.9442 - mse: 36.1422 - val_loss: 45.9033 - val_mse: 36.1266\n",
      "Epoch 260/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 48.2060 - mse: 38.4035 - val_loss: 44.9697 - val_mse: 35.2147\n",
      "Epoch 261/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 48.1330 - mse: 38.3256 - val_loss: 46.8875 - val_mse: 37.0208\n",
      "Epoch 262/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 46.8198 - mse: 36.9864 - val_loss: 46.1101 - val_mse: 36.2600\n",
      "Epoch 263/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 47.5517 - mse: 37.7495 - val_loss: 45.3504 - val_mse: 35.5404\n",
      "Epoch 264/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 47.6659 - mse: 37.8776 - val_loss: 45.2269 - val_mse: 35.4373\n",
      "Epoch 265/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 47.8511 - mse: 38.0573 - val_loss: 45.8424 - val_mse: 36.1021\n",
      "Epoch 266/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 47.4599 - mse: 37.6778 - val_loss: 55.4277 - val_mse: 45.7044\n",
      "Epoch 267/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 48.7151 - mse: 38.9230 - val_loss: 45.6108 - val_mse: 35.8076\n",
      "Epoch 268/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 48.1108 - mse: 38.2963 - val_loss: 48.0727 - val_mse: 38.2939\n",
      "Epoch 269/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 48.7052 - mse: 38.8826 - val_loss: 45.6049 - val_mse: 35.7921\n",
      "Epoch 270/500\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 47.1080 - mse: 37.2707 - val_loss: 44.2245 - val_mse: 34.3698\n",
      "Epoch 271/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 45.9743 - mse: 36.1304 - val_loss: 43.6871 - val_mse: 33.8600\n",
      "Epoch 272/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 46.4637 - mse: 36.6261 - val_loss: 44.1882 - val_mse: 34.3306\n",
      "Epoch 273/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 48.6061 - mse: 38.7575 - val_loss: 45.2570 - val_mse: 35.3787\n",
      "Epoch 274/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 46.3438 - mse: 36.5093 - val_loss: 47.3882 - val_mse: 37.5711\n",
      "Epoch 275/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 46.4359 - mse: 36.6043 - val_loss: 59.0179 - val_mse: 49.2378\n",
      "Epoch 276/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 47.3684 - mse: 37.5387 - val_loss: 46.6368 - val_mse: 36.7757\n",
      "Epoch 277/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 48.3027 - mse: 38.4851 - val_loss: 46.0046 - val_mse: 36.2169\n",
      "Epoch 278/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 47.4376 - mse: 37.6221 - val_loss: 45.2421 - val_mse: 35.4079\n",
      "Epoch 279/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 47.0765 - mse: 37.2495 - val_loss: 45.3628 - val_mse: 35.5529\n",
      "Epoch 280/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 46.6642 - mse: 36.8552 - val_loss: 47.2745 - val_mse: 37.4556\n",
      "Epoch 281/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 46.5172 - mse: 36.7163 - val_loss: 44.5991 - val_mse: 34.8002\n",
      "Epoch 282/500\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 46.3717 - mse: 36.5807 - val_loss: 52.0263 - val_mse: 42.2794\n",
      "Epoch 283/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 48.1340 - mse: 38.3279 - val_loss: 52.7219 - val_mse: 43.0078\n",
      "Epoch 284/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 47.2647 - mse: 37.4804 - val_loss: 45.0220 - val_mse: 35.2290\n",
      "Epoch 285/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 46.6455 - mse: 36.8800 - val_loss: 43.6520 - val_mse: 33.8679\n",
      "Epoch 286/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 46.9778 - mse: 37.2095 - val_loss: 48.4446 - val_mse: 38.7225\n",
      "Epoch 287/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 46.1348 - mse: 36.3644 - val_loss: 44.9179 - val_mse: 35.1511\n",
      "Epoch 288/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 47.0863 - mse: 37.2894 - val_loss: 41.9372 - val_mse: 32.1880\n",
      "Epoch 289/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 47.6102 - mse: 37.8368 - val_loss: 45.9537 - val_mse: 36.1687\n",
      "Epoch 290/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 47.1313 - mse: 37.3457 - val_loss: 44.2020 - val_mse: 34.4363\n",
      "Epoch 291/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 47.5171 - mse: 37.7201 - val_loss: 54.2618 - val_mse: 44.3975\n",
      "Epoch 292/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 47.1029 - mse: 37.3089 - val_loss: 51.2056 - val_mse: 41.3688\n",
      "Epoch 293/500\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 47.0961 - mse: 37.3058 - val_loss: 44.5637 - val_mse: 34.7445\n",
      "Epoch 294/500\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 46.7225 - mse: 36.9335 - val_loss: 46.4470 - val_mse: 36.6810\n",
      "Epoch 295/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 44.9387 - mse: 35.1552 - val_loss: 43.7991 - val_mse: 34.0340\n",
      "Epoch 296/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 46.2873 - mse: 36.5036 - val_loss: 47.5121 - val_mse: 37.7072\n",
      "Epoch 297/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 47.3007 - mse: 37.4987 - val_loss: 43.6550 - val_mse: 33.8928\n",
      "Epoch 298/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 47.4395 - mse: 37.6523 - val_loss: 47.0356 - val_mse: 37.2811\n",
      "Epoch 299/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 47.2946 - mse: 37.4835 - val_loss: 43.7226 - val_mse: 33.9012\n",
      "Epoch 300/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 46.7637 - mse: 36.9197 - val_loss: 47.7901 - val_mse: 37.9894\n",
      "Epoch 301/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 46.2095 - mse: 36.3839 - val_loss: 47.6202 - val_mse: 37.7869\n",
      "Epoch 302/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 47.0960 - mse: 37.2664 - val_loss: 50.7553 - val_mse: 40.8938\n",
      "Epoch 303/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 45.6150 - mse: 35.8070 - val_loss: 42.7401 - val_mse: 32.9160\n",
      "Epoch 304/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 46.7489 - mse: 36.9561 - val_loss: 46.3144 - val_mse: 36.5351\n",
      "Epoch 305/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 45.8018 - mse: 36.0114 - val_loss: 44.7104 - val_mse: 34.8724\n",
      "Epoch 306/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 46.7221 - mse: 36.9214 - val_loss: 44.2938 - val_mse: 34.4557\n",
      "Epoch 307/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 45.5153 - mse: 35.7094 - val_loss: 47.0104 - val_mse: 37.2228\n",
      "Epoch 308/500\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 46.4378 - mse: 36.6545 - val_loss: 54.0263 - val_mse: 44.1784\n",
      "Epoch 309/500\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 45.5682 - mse: 35.7800 - val_loss: 46.2992 - val_mse: 36.5407\n",
      "Epoch 310/500\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 47.2860 - mse: 37.5189 - val_loss: 44.0308 - val_mse: 34.2446\n",
      "Epoch 311/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 47.8137 - mse: 38.0322 - val_loss: 43.9635 - val_mse: 34.1921\n",
      "Epoch 312/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 45.3236 - mse: 35.5239 - val_loss: 44.0552 - val_mse: 34.2470\n",
      "Epoch 313/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 45.7620 - mse: 35.9806 - val_loss: 43.4662 - val_mse: 33.6816\n",
      "Epoch 314/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 46.0646 - mse: 36.2777 - val_loss: 43.6232 - val_mse: 33.8367\n",
      "Epoch 315/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 45.1614 - mse: 35.3622 - val_loss: 50.4123 - val_mse: 40.6285\n",
      "Epoch 316/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 46.5604 - mse: 36.7643 - val_loss: 55.3379 - val_mse: 45.6231\n",
      "Epoch 317/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 46.3096 - mse: 36.5150 - val_loss: 45.0226 - val_mse: 35.2119\n",
      "Epoch 318/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 45.8598 - mse: 36.0741 - val_loss: 46.9420 - val_mse: 37.1718\n",
      "Epoch 319/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 46.5040 - mse: 36.6886 - val_loss: 45.7939 - val_mse: 35.9428\n",
      "Epoch 320/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 45.4090 - mse: 35.5997 - val_loss: 44.4187 - val_mse: 34.6233\n",
      "Epoch 321/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 45.7016 - mse: 35.8921 - val_loss: 46.0029 - val_mse: 36.1716\n",
      "Epoch 322/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 44.7005 - mse: 34.8669 - val_loss: 45.8571 - val_mse: 36.0535\n",
      "Epoch 323/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 45.9075 - mse: 36.0891 - val_loss: 44.3192 - val_mse: 34.5198\n",
      "Epoch 324/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 46.0894 - mse: 36.2715 - val_loss: 47.4483 - val_mse: 37.5973\n",
      "Epoch 325/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 44.8750 - mse: 35.0584 - val_loss: 43.5348 - val_mse: 33.7523\n",
      "Epoch 326/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 45.5804 - mse: 35.7918 - val_loss: 52.1533 - val_mse: 42.4229\n",
      "Epoch 327/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 46.7996 - mse: 37.0166 - val_loss: 43.5208 - val_mse: 33.7183\n",
      "Epoch 328/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 45.3850 - mse: 35.6022 - val_loss: 44.7095 - val_mse: 34.9538\n",
      "Epoch 329/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 45.5993 - mse: 35.8260 - val_loss: 49.0545 - val_mse: 39.2244\n",
      "Epoch 330/500\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 44.1901 - mse: 34.4069 - val_loss: 46.4263 - val_mse: 36.6207\n",
      "Epoch 331/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 45.1348 - mse: 35.3727 - val_loss: 44.9009 - val_mse: 35.1309\n",
      "Epoch 332/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 45.4806 - mse: 35.7129 - val_loss: 47.4625 - val_mse: 37.6720\n",
      "Epoch 333/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 45.6432 - mse: 35.8795 - val_loss: 46.4359 - val_mse: 36.6636\n",
      "Epoch 334/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 45.6005 - mse: 35.8350 - val_loss: 52.3674 - val_mse: 42.6493\n",
      "Epoch 335/500\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 46.1222 - mse: 36.3496 - val_loss: 43.0952 - val_mse: 33.3031\n",
      "Epoch 336/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 44.7825 - mse: 34.9992 - val_loss: 45.3099 - val_mse: 35.5189\n",
      "Epoch 337/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 45.8213 - mse: 36.0322 - val_loss: 45.0837 - val_mse: 35.2585\n",
      "Epoch 338/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 44.7997 - mse: 35.0071 - val_loss: 43.1285 - val_mse: 33.3150\n",
      "Epoch 339/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 45.4483 - mse: 35.6638 - val_loss: 48.2639 - val_mse: 38.4516\n",
      "Epoch 340/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 45.3130 - mse: 35.5637 - val_loss: 45.4449 - val_mse: 35.6855\n",
      "Epoch 341/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 45.2601 - mse: 35.5157 - val_loss: 44.4811 - val_mse: 34.7502\n",
      "Epoch 342/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 44.6264 - mse: 34.8717 - val_loss: 45.7876 - val_mse: 35.9693\n",
      "Epoch 343/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 44.8803 - mse: 35.1109 - val_loss: 44.0764 - val_mse: 34.3212\n",
      "Epoch 344/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 44.5095 - mse: 34.7448 - val_loss: 45.1954 - val_mse: 35.3849\n",
      "Epoch 345/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 44.9948 - mse: 35.2238 - val_loss: 44.2693 - val_mse: 34.4974\n",
      "Epoch 346/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 45.5583 - mse: 35.7675 - val_loss: 44.2583 - val_mse: 34.4434\n",
      "Epoch 347/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 45.1486 - mse: 35.3555 - val_loss: 47.5311 - val_mse: 37.7719\n",
      "Epoch 348/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 45.1565 - mse: 35.3536 - val_loss: 44.0417 - val_mse: 34.2113\n",
      "Epoch 349/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 45.2796 - mse: 35.4704 - val_loss: 44.8167 - val_mse: 34.9853\n",
      "Epoch 350/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 45.1115 - mse: 35.3076 - val_loss: 42.2664 - val_mse: 32.4099\n",
      "Epoch 351/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 44.1073 - mse: 34.2759 - val_loss: 44.1259 - val_mse: 34.3055\n",
      "Epoch 352/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 44.6869 - mse: 34.9071 - val_loss: 48.0613 - val_mse: 38.2300\n",
      "Epoch 353/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 45.9191 - mse: 36.1402 - val_loss: 42.8687 - val_mse: 33.0693\n",
      "Epoch 354/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 45.4137 - mse: 35.6382 - val_loss: 42.4018 - val_mse: 32.6265\n",
      "Epoch 355/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 44.8534 - mse: 35.0751 - val_loss: 48.5552 - val_mse: 38.8058\n",
      "Epoch 356/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 45.3390 - mse: 35.5500 - val_loss: 47.4012 - val_mse: 37.6353\n",
      "Epoch 357/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 45.0610 - mse: 35.2861 - val_loss: 41.9009 - val_mse: 32.1213\n",
      "Epoch 358/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 45.3689 - mse: 35.6150 - val_loss: 46.0911 - val_mse: 36.3259\n",
      "Epoch 359/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 45.7481 - mse: 35.9976 - val_loss: 41.7847 - val_mse: 32.0182\n",
      "Epoch 360/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 44.3748 - mse: 34.6156 - val_loss: 43.3206 - val_mse: 33.5495\n",
      "Epoch 361/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 44.9496 - mse: 35.1712 - val_loss: 47.1404 - val_mse: 37.3961\n",
      "Epoch 362/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 44.6018 - mse: 34.8260 - val_loss: 44.4295 - val_mse: 34.6295\n",
      "Epoch 363/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 44.8091 - mse: 35.0397 - val_loss: 43.0848 - val_mse: 33.3160\n",
      "Epoch 364/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 44.6441 - mse: 34.8621 - val_loss: 43.8543 - val_mse: 34.1198\n",
      "Epoch 365/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 44.7999 - mse: 35.0374 - val_loss: 45.6439 - val_mse: 35.8750\n",
      "Epoch 366/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 45.0429 - mse: 35.2905 - val_loss: 42.7785 - val_mse: 33.0393\n",
      "Epoch 367/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 43.7698 - mse: 34.0360 - val_loss: 62.0094 - val_mse: 52.1774\n",
      "Epoch 368/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 44.6464 - mse: 34.8924 - val_loss: 48.3189 - val_mse: 38.6080\n",
      "Epoch 369/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 45.8763 - mse: 36.1291 - val_loss: 44.0743 - val_mse: 34.2971\n",
      "Epoch 370/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 44.7474 - mse: 35.0126 - val_loss: 48.9799 - val_mse: 39.2132\n",
      "Epoch 371/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 43.5860 - mse: 33.8565 - val_loss: 43.0162 - val_mse: 33.3068\n",
      "Epoch 372/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 43.9310 - mse: 34.2158 - val_loss: 46.4219 - val_mse: 36.7522\n",
      "Epoch 373/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 44.0096 - mse: 34.2937 - val_loss: 44.0284 - val_mse: 34.3030\n",
      "Epoch 374/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 44.6024 - mse: 34.8759 - val_loss: 42.6224 - val_mse: 32.8841\n",
      "Epoch 375/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 44.5619 - mse: 34.8164 - val_loss: 43.9076 - val_mse: 34.1513\n",
      "Epoch 376/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 45.3694 - mse: 35.6284 - val_loss: 43.7396 - val_mse: 33.9870\n",
      "Epoch 377/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 45.4504 - mse: 35.7085 - val_loss: 41.7792 - val_mse: 32.0402\n",
      "Epoch 378/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 44.8539 - mse: 35.1051 - val_loss: 47.1900 - val_mse: 37.4222\n",
      "Epoch 379/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 43.3806 - mse: 33.6570 - val_loss: 50.5241 - val_mse: 40.7384\n",
      "Epoch 380/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 45.0439 - mse: 35.3094 - val_loss: 45.5386 - val_mse: 35.8025\n",
      "Epoch 381/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 44.6701 - mse: 34.9149 - val_loss: 43.6056 - val_mse: 33.8295\n",
      "Epoch 382/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 44.3959 - mse: 34.6319 - val_loss: 43.7463 - val_mse: 33.9953\n",
      "Epoch 383/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 44.5504 - mse: 34.8040 - val_loss: 44.1637 - val_mse: 34.4156\n",
      "Epoch 384/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 45.5117 - mse: 35.7621 - val_loss: 44.3380 - val_mse: 34.6087\n",
      "Epoch 385/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 44.0948 - mse: 34.3510 - val_loss: 49.3000 - val_mse: 39.6077\n",
      "Epoch 386/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 44.3276 - mse: 34.6009 - val_loss: 43.6360 - val_mse: 33.9519\n",
      "Epoch 387/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 44.1251 - mse: 34.4155 - val_loss: 45.5821 - val_mse: 35.8364\n",
      "Epoch 388/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 44.2523 - mse: 34.5187 - val_loss: 45.1622 - val_mse: 35.4200\n",
      "Epoch 389/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 42.9856 - mse: 33.2519 - val_loss: 41.0825 - val_mse: 31.3476\n",
      "Epoch 390/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 44.6376 - mse: 34.9211 - val_loss: 43.2149 - val_mse: 33.4828\n",
      "Epoch 391/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 44.8600 - mse: 35.1426 - val_loss: 45.8733 - val_mse: 36.0919\n",
      "Epoch 392/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 43.4548 - mse: 33.7232 - val_loss: 44.0215 - val_mse: 34.2768\n",
      "Epoch 393/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 43.9229 - mse: 34.2064 - val_loss: 41.6977 - val_mse: 31.9762\n",
      "Epoch 394/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 43.9242 - mse: 34.2003 - val_loss: 42.6542 - val_mse: 32.9239\n",
      "Epoch 395/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 44.3590 - mse: 34.6172 - val_loss: 44.6998 - val_mse: 34.9537\n",
      "Epoch 396/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 44.2876 - mse: 34.5252 - val_loss: 45.2052 - val_mse: 35.4364\n",
      "Epoch 397/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 43.8899 - mse: 34.1386 - val_loss: 44.3805 - val_mse: 34.6252\n",
      "Epoch 398/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 43.6276 - mse: 33.8951 - val_loss: 48.7272 - val_mse: 39.0366\n",
      "Epoch 399/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 43.5286 - mse: 33.7747 - val_loss: 44.0632 - val_mse: 34.3166\n",
      "Epoch 400/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 44.4407 - mse: 34.7016 - val_loss: 44.8737 - val_mse: 35.1667\n",
      "Epoch 401/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 45.7074 - mse: 35.9816 - val_loss: 42.6979 - val_mse: 32.9711\n",
      "Epoch 402/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 43.0843 - mse: 33.3467 - val_loss: 42.7583 - val_mse: 33.0549\n",
      "Epoch 403/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 43.1153 - mse: 33.3731 - val_loss: 42.0891 - val_mse: 32.3697\n",
      "Epoch 404/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 43.4948 - mse: 33.7751 - val_loss: 41.8306 - val_mse: 32.0925\n",
      "Epoch 405/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 42.9681 - mse: 33.2319 - val_loss: 44.6626 - val_mse: 34.9164\n",
      "Epoch 406/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 44.3149 - mse: 34.5641 - val_loss: 46.1013 - val_mse: 36.3702\n",
      "Epoch 407/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 44.3785 - mse: 34.6528 - val_loss: 41.7518 - val_mse: 32.0370\n",
      "Epoch 408/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 44.2725 - mse: 34.5301 - val_loss: 41.1826 - val_mse: 31.4529\n",
      "Epoch 409/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 44.0054 - mse: 34.2698 - val_loss: 46.9763 - val_mse: 37.2396\n",
      "Epoch 410/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 44.4980 - mse: 34.7748 - val_loss: 45.0172 - val_mse: 35.2844\n",
      "Epoch 411/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 43.4560 - mse: 33.7446 - val_loss: 45.5602 - val_mse: 35.7897\n",
      "Epoch 412/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 42.7766 - mse: 33.0090 - val_loss: 48.0736 - val_mse: 38.3411\n",
      "Epoch 413/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 44.1611 - mse: 34.3977 - val_loss: 44.0476 - val_mse: 34.3103\n",
      "Epoch 414/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 43.5232 - mse: 33.7762 - val_loss: 41.7499 - val_mse: 31.9987\n",
      "Epoch 415/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 44.6195 - mse: 34.8717 - val_loss: 45.7695 - val_mse: 36.0127\n",
      "Epoch 416/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 43.6232 - mse: 33.8955 - val_loss: 53.0086 - val_mse: 43.2911\n",
      "Epoch 417/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 43.5884 - mse: 33.8305 - val_loss: 45.3200 - val_mse: 35.5735\n",
      "Epoch 418/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 44.6437 - mse: 34.9049 - val_loss: 43.1903 - val_mse: 33.4638\n",
      "Epoch 419/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 44.2245 - mse: 34.4871 - val_loss: 45.5519 - val_mse: 35.7835\n",
      "Epoch 420/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 43.2642 - mse: 33.5174 - val_loss: 41.5922 - val_mse: 31.8496\n",
      "Epoch 421/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 42.8505 - mse: 33.1043 - val_loss: 41.7236 - val_mse: 32.0008\n",
      "Epoch 422/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 44.9449 - mse: 35.2047 - val_loss: 43.0522 - val_mse: 33.2953\n",
      "Epoch 423/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 42.6481 - mse: 32.9205 - val_loss: 44.0701 - val_mse: 34.3500\n",
      "Epoch 424/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 44.6766 - mse: 34.9593 - val_loss: 43.0893 - val_mse: 33.3756\n",
      "Epoch 425/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 42.6126 - mse: 32.8823 - val_loss: 43.9263 - val_mse: 34.1535\n",
      "Epoch 426/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 43.7778 - mse: 34.0594 - val_loss: 46.4238 - val_mse: 36.7394\n",
      "Epoch 427/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 43.9104 - mse: 34.1817 - val_loss: 42.3184 - val_mse: 32.5931\n",
      "Epoch 428/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 43.6109 - mse: 33.8755 - val_loss: 43.2765 - val_mse: 33.5455\n",
      "Epoch 429/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 43.4393 - mse: 33.6951 - val_loss: 44.0232 - val_mse: 34.2759\n",
      "Epoch 430/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 42.9799 - mse: 33.2311 - val_loss: 43.4662 - val_mse: 33.7217\n",
      "Epoch 431/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 42.8570 - mse: 33.1163 - val_loss: 42.0112 - val_mse: 32.2783\n",
      "Epoch 432/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 41.7681 - mse: 32.0434 - val_loss: 44.6596 - val_mse: 34.9659\n",
      "Epoch 433/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 43.0740 - mse: 33.3584 - val_loss: 43.7764 - val_mse: 34.0882\n",
      "Epoch 434/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 44.0630 - mse: 34.3430 - val_loss: 47.2626 - val_mse: 37.5782\n",
      "Epoch 435/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 44.2628 - mse: 34.5225 - val_loss: 44.6104 - val_mse: 34.8694\n",
      "Epoch 436/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 43.2117 - mse: 33.4841 - val_loss: 43.7302 - val_mse: 33.9976\n",
      "Epoch 437/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 44.1875 - mse: 34.4739 - val_loss: 42.0143 - val_mse: 32.2555\n",
      "Epoch 438/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 43.4376 - mse: 33.7125 - val_loss: 43.2177 - val_mse: 33.4937\n",
      "Epoch 439/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 43.0699 - mse: 33.3554 - val_loss: 45.1258 - val_mse: 35.4443\n",
      "Epoch 440/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 44.0191 - mse: 34.3074 - val_loss: 45.0394 - val_mse: 35.2910\n",
      "Epoch 441/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 43.2681 - mse: 33.5401 - val_loss: 44.1832 - val_mse: 34.4412\n",
      "Epoch 442/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 43.2047 - mse: 33.4681 - val_loss: 44.7722 - val_mse: 35.0381\n",
      "Epoch 443/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 43.6998 - mse: 33.9884 - val_loss: 43.4612 - val_mse: 33.7881\n",
      "Epoch 444/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 43.7279 - mse: 34.0031 - val_loss: 43.5988 - val_mse: 33.8731\n",
      "Epoch 445/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 43.5029 - mse: 33.7928 - val_loss: 44.2112 - val_mse: 34.4569\n",
      "Epoch 446/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 43.7607 - mse: 34.0512 - val_loss: 41.7619 - val_mse: 32.0732\n",
      "Epoch 447/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 43.5559 - mse: 33.8678 - val_loss: 43.4138 - val_mse: 33.7344\n",
      "Epoch 448/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 42.4548 - mse: 32.7573 - val_loss: 51.8968 - val_mse: 42.2590\n",
      "Epoch 449/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 43.5689 - mse: 33.8764 - val_loss: 46.5341 - val_mse: 36.8707\n",
      "Epoch 450/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 43.3111 - mse: 33.6027 - val_loss: 44.7048 - val_mse: 34.9791\n",
      "Epoch 451/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 43.5346 - mse: 33.8190 - val_loss: 41.0311 - val_mse: 31.3170\n",
      "Epoch 452/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 43.6137 - mse: 33.8990 - val_loss: 42.4406 - val_mse: 32.7408\n",
      "Epoch 453/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 43.5198 - mse: 33.8100 - val_loss: 44.2134 - val_mse: 34.5439\n",
      "Epoch 454/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 42.8595 - mse: 33.1466 - val_loss: 42.4014 - val_mse: 32.6864\n",
      "Epoch 455/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 42.8232 - mse: 33.1049 - val_loss: 45.3287 - val_mse: 35.6633\n",
      "Epoch 456/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 43.1371 - mse: 33.4369 - val_loss: 43.3171 - val_mse: 33.6142\n",
      "Epoch 457/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 42.4481 - mse: 32.7511 - val_loss: 42.8270 - val_mse: 33.1240\n",
      "Epoch 458/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 44.2301 - mse: 34.5336 - val_loss: 42.7606 - val_mse: 33.1124\n",
      "Epoch 459/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 43.4724 - mse: 33.7953 - val_loss: 45.3201 - val_mse: 35.6350\n",
      "Epoch 460/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 42.3767 - mse: 32.6706 - val_loss: 41.5075 - val_mse: 31.8120\n",
      "Epoch 461/500\n",
      "354/354 [==============================] - 2s 6ms/step - loss: 42.8116 - mse: 33.1262 - val_loss: 49.9297 - val_mse: 40.3053\n",
      "Epoch 462/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 42.0806 - mse: 32.3999 - val_loss: 41.5584 - val_mse: 31.8862\n",
      "Epoch 463/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 42.6127 - mse: 32.9125 - val_loss: 43.3651 - val_mse: 33.6615\n",
      "Epoch 464/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 43.6394 - mse: 33.9289 - val_loss: 45.3284 - val_mse: 35.6182\n",
      "Epoch 465/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 43.0473 - mse: 33.3608 - val_loss: 41.0448 - val_mse: 31.3653\n",
      "Epoch 466/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 43.0952 - mse: 33.4131 - val_loss: 43.7304 - val_mse: 34.0583\n",
      "Epoch 467/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 43.4612 - mse: 33.7970 - val_loss: 42.9380 - val_mse: 33.2649\n",
      "Epoch 468/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 43.1162 - mse: 33.4202 - val_loss: 43.7050 - val_mse: 33.9852\n",
      "Epoch 469/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 42.4541 - mse: 32.7737 - val_loss: 46.4003 - val_mse: 36.7597\n",
      "Epoch 470/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 43.6286 - mse: 33.9416 - val_loss: 41.3984 - val_mse: 31.7400\n",
      "Epoch 471/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 42.2181 - mse: 32.5430 - val_loss: 43.5884 - val_mse: 33.9237\n",
      "Epoch 472/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 43.2834 - mse: 33.6165 - val_loss: 48.6528 - val_mse: 39.0285\n",
      "Epoch 473/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 43.6042 - mse: 33.9338 - val_loss: 42.3525 - val_mse: 32.6735\n",
      "Epoch 474/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 42.3535 - mse: 32.6592 - val_loss: 42.2274 - val_mse: 32.5376\n",
      "Epoch 475/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 42.6862 - mse: 32.9850 - val_loss: 48.3819 - val_mse: 38.5974\n",
      "Epoch 476/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 44.0651 - mse: 34.3395 - val_loss: 41.3180 - val_mse: 31.6008\n",
      "Epoch 477/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 42.8178 - mse: 33.0874 - val_loss: 42.2902 - val_mse: 32.5700\n",
      "Epoch 478/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 43.4466 - mse: 33.7108 - val_loss: 46.4386 - val_mse: 36.6802\n",
      "Epoch 479/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 42.5828 - mse: 32.8939 - val_loss: 44.7017 - val_mse: 34.9974\n",
      "Epoch 480/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 42.2035 - mse: 32.5241 - val_loss: 41.6367 - val_mse: 31.9601\n",
      "Epoch 481/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 42.9831 - mse: 33.3093 - val_loss: 45.8559 - val_mse: 36.1199\n",
      "Epoch 482/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 42.7499 - mse: 33.0732 - val_loss: 41.4799 - val_mse: 31.7826\n",
      "Epoch 483/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 42.5977 - mse: 32.9196 - val_loss: 42.9498 - val_mse: 33.3006\n",
      "Epoch 484/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 41.8802 - mse: 32.1861 - val_loss: 41.8501 - val_mse: 32.1526\n",
      "Epoch 485/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 41.6613 - mse: 31.9653 - val_loss: 42.1329 - val_mse: 32.4743\n",
      "Epoch 486/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 41.9361 - mse: 32.2490 - val_loss: 42.5214 - val_mse: 32.8377\n",
      "Epoch 487/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 42.2744 - mse: 32.5660 - val_loss: 44.2949 - val_mse: 34.5615\n",
      "Epoch 488/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 43.3665 - mse: 33.6478 - val_loss: 45.6511 - val_mse: 35.9648\n",
      "Epoch 489/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 42.0871 - mse: 32.3685 - val_loss: 42.7947 - val_mse: 33.0674\n",
      "Epoch 490/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 43.5876 - mse: 33.8623 - val_loss: 41.0128 - val_mse: 31.2826\n",
      "Epoch 491/500\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 43.7681 - mse: 34.0442 - val_loss: 59.2456 - val_mse: 49.6026\n",
      "Epoch 492/500\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 42.9410 - mse: 33.2366 - val_loss: 42.0687 - val_mse: 32.3226\n",
      "Epoch 493/500\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 42.2849 - mse: 32.5750 - val_loss: 44.0606 - val_mse: 34.3656\n",
      "Epoch 494/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 43.2927 - mse: 33.6159 - val_loss: 42.6579 - val_mse: 32.9476\n",
      "Epoch 495/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 42.2139 - mse: 32.5149 - val_loss: 40.9576 - val_mse: 31.2293\n",
      "Epoch 496/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 42.5544 - mse: 32.8592 - val_loss: 42.5133 - val_mse: 32.8435\n",
      "Epoch 497/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 42.3081 - mse: 32.6297 - val_loss: 41.4673 - val_mse: 31.8086\n",
      "Epoch 498/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 42.4780 - mse: 32.8072 - val_loss: 43.8992 - val_mse: 34.1834\n",
      "Epoch 499/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 42.2575 - mse: 32.5581 - val_loss: 42.4568 - val_mse: 32.7909\n",
      "Epoch 500/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 42.2978 - mse: 32.6080 - val_loss: 43.4911 - val_mse: 33.7912\n"
     ]
    }
   ],
   "source": [
    "# fit\n",
    "a_history_nn = a_model_nn.fit(a_X_nn_train_sc, a_y_nn_train, epochs = 500, verbose = 1, \n",
    "                          validation_data = (a_X_nn_test_sc, a_y_nn_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b6adc2a6-f40b-4657-a166-7c4c4ddec36d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA4EAAAFNCAYAAAC+H2oqAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAABJAklEQVR4nO3deZxcVZn/8e/TS3rLnjTZVwhhkzUCijIIIiAOIIoGBTLKTNRBhZ/ODKDjuAZRQYVRVGQXJAaVISIgGFlkJ4EAJiEhkIQ02ZqsnU6n08vz++PcSlV3V/WS7urq2/V5v179qqpTt+qeumk4/a3n3HPN3QUAAAAAyA8Fue4AAAAAAKD3EAIBAAAAII8QAgEAAAAgjxACAQAAACCPEAIBAAAAII8QAgEAAAAgjxACgX7MzB40s1m57gcAAAD6DkIg0MeY2c6Un2Yzq0t5/OmuvJe7n+Hut2errwAA5EpPjpfR+z1mZv+ajb4CfU1RrjsAoCV3H5i4b2arJf2ru/+19XZmVuTujb3ZNwAA+orOjpcA2qISCMSEmZ1kZlVmdrmZbZB0q5kNM7P7zazazLZG98envGbvt5pm9i9m9qSZXRNtu8rMzsjZBwIAIAvMrMDMrjCzN8xss5nNM7Ph0XOlZnZn1L7NzF4ws1FmNkfS+yX9LKok/iy3nwLILkIgEC+jJQ2XNEnSbIX/hm+NHk+UVCepvYHrOEnLJY2U9ENJN5uZZbPDAAD0si9LOkfSP0kaK2mrpJ9Hz82SNETSBEkjJH1eUp27f13S3yV90d0HuvsXe7vTQG8iBALx0izpm+5e7+517r7Z3f/g7rvcvUbSHIVBL5M17v5rd2+SdLukMZJG9UK/AQDoLZ+T9HV3r3L3eknfkvRxMyuS1KAQ/g5w9yZ3X+TuO3LYVyAnOCcQiJdqd9+deGBm5ZJ+Iul0ScOi5kFmVhgFvdY2JO64+66oCDgwzXYAAMTVJEn3mllzSluTwpeev1GoAs41s6GS7lQIjA293ksgh6gEAvHirR5/VdJ0Sce5+2BJJ0btTPEEAOSrtZLOcPehKT+l7v62uze4+7fd/RBJ75X0EUkXRa9rPcYC/RYhEIi3QQrnAW6LTnr/Zo77AwBArv1S0hwzmyRJZlZpZmdH9z9gZu8ys0JJOxSmhyZmzmyUNDUXHQZ6GyEQiLefSiqT9I6kZyU9lNPeAACQe9dJmi/pYTOrURgfj4ueGy3p9woBcJmkxxWmhCZe9/FoBe3re7fLQO8ydyrfAAAAAJAvqAQCAAAAQB4hBAIAAABAHiEEAgAAAEAeIQQCAAAAQB4hBAIAAABAHinKdQeyZeTIkT558uRcdwMAkGWLFi16x90rc92PuGB8BID8kWmM7LchcPLkyVq4cGGuuwEAyDIzW5PrPsQJ4yMA5I9MYyTTQQEAAAAgjxACAQAAACCPEAIBAAAAII8QAgEAAAAgjxACAQAAACCPEAIBAAAAII8QAgEAAAAgjxACAQAAACCPEAIBAAAAII8QAjP40/I/6f4V9+e6GwAA9CmbNkk33ii99VauewIA2FeEwAx+9PSPdO0z1+a6GwAA9CmrVkmf+5z0j3/kuicAgH1FCMzAzOTuue4GAAB9ilm4ZYgEgPjKWgg0s1vMbJOZtfmu0Mz+w8zczEamtF1pZivNbLmZnZbSfoyZvRo9d71ZYvjJLlOv7AYAgFghBAJA/GWzEnibpNNbN5rZBEmnSnorpe0QSTMlHRq95gYzK4ye/oWk2ZKmRT9t3jNbXIxwAACkIgQCQPxlLQS6+xOStqR56ieS/ktqkbDOljTX3evdfZWklZKONbMxkga7+zMe5mbeIemcbPU5FdNBAQC9zcymm9nilJ8dZnaZmQ03s0fM7PXodljKa9LOpMleH8MtQyQAxFevnhNoZmdJetvdX2711DhJa1MeV0Vt46L7rduzzmRUAgEAvcrdl7v7ke5+pKRjJO2SdK+kKyQtcPdpkhZEjzuaSZMVhEAAiL9eC4FmVi7p65L+J93Tadq8nfZM+5htZgvNbGF1dfW+dTT5XlQCAQC5dIqkN9x9jcKMmduj9tuVnBWTdiZNNjvVO2fmAwCyqTcrgftLmiLpZTNbLWm8pBfNbLRChW9CyrbjJa2L2senaU/L3W909xnuPqOysrJbnaUSCADIsZmS7o7uj3L39ZIU3e4XtWeaSZN1fE8KAPHVayHQ3V919/3cfbK7T1YYqI529w2S5kuaaWYlZjZFYQGY56OBrsbMjo9WBb1I0n290d9eWoQUAIA2zGyApLMk3dPRpmna2sSznp0pE+2EEAgAsZXNS0TcLekZSdPNrMrMLs60rbsvkTRP0lJJD0m6xN2boqe/IOkmhSkub0h6MFt9TtOv3toVAACpzpD0ortvjB5vjBZLU3S7KWrPNJOmhR6dKUMIBIDYK8rWG7v7+R08P7nV4zmS5qTZbqGkw3q0c53AdFAAQA6dr+RUUCnMmJkl6ero9r6U9t+a2Y8ljVU0kyabHSMEAkD8ZS0Exh0LwwAAciFaSO1USZ9Lab5a0rxoVs1bks6TwkwaM0vMpGlUy5k0WepfuGWIBID4IgRmQCUQAJAL7r5L0ohWbZsVVgtNt33amTTZQggEgPjr1esExgmVQAAA2iIEAkD8EQIzsLQLrgEAkN8IgQAQf4TAdjAdFACAlgiBABB/hMAMmA4KAEBbhEAAiD9CYAYsDAMAQFuEQACIP0JgBlQCAQBoixAIAPFHCMyASiAAAG0RAgEg/giBGZixOigAAK0RAgEg/giB7WA6KAAALRECASD+CIEZMB0UAIC2CIEAEH+EwAxYGAYAgLY4WwIA4o8QmAGVQAAAMuN7UgCIL0JgBlQCAQBoi+mgABB/hMAMTMx3AQCgNUIgAMQfIbAdTAcFAKAlQiAAxB8hMAOmgwIA0BYhEADijxCYAQvDAADQFiEQAOKPEJgBlUAAANoiBAJA/BECM6ASCABAW4RAAIg/QmAGxtVwAQBogxAIAPFHCGwH00EBAGiJEAgA8UcIzIDpoAAAtEUIBID4IwRmwMIwAAC0RQgEgPgjBGZAJRAAgLYIgQAQf4TADKgEAgDQFiEQAOIvayHQzG4xs01m9o+Uth+Z2Wtm9oqZ3WtmQ1Oeu9LMVprZcjM7LaX9GDN7NXrueuulZTtNrA4KAEBrhEAAiL9sVgJvk3R6q7ZHJB3m7odLWiHpSkkys0MkzZR0aPSaG8ysMHrNLyTNljQt+mn9nlnDdFAAAFoiBAJA/GUtBLr7E5K2tGp72N0bo4fPShof3T9b0lx3r3f3VZJWSjrWzMZIGuzuz3iYm3mHpHOy1edUTAcFAOSCmQ01s99HM2eWmdl7zGy4mT1iZq9Ht8NStk87kyZ7/cv2HgAA2ZbLcwI/K+nB6P44SWtTnquK2sZF91u3Zx0LwwAAcuQ6SQ+5+0GSjpC0TNIVkha4+zRJC6LHHc2kySq+JwWA+MpJCDSzr0tqlHRXoinNZt5Oe6b3nW1mC81sYXV1dXf7SCUQANCrzGywpBMl3SxJ7r7H3bcpzJi5PdrsdiVnxaSdSZPdPoZbhkgAiK9eD4FmNkvSRyR92pMpq0rShJTNxktaF7WPT9Oelrvf6O4z3H1GZWVl9/pJJRAA0PumSqqWdKuZvWRmN5lZhaRR7r5ekqLb/aLtM82kyRpCIADEX6+GQDM7XdLlks5y910pT82XNNPMSsxsisICMM9HA12NmR0frQp6kaT7eqWvrA4KAOh9RZKOlvQLdz9KUq2iqZ8ZdGrGTM/OlIl2QggEgNjK5iUi7pb0jKTpZlZlZhdL+pmkQZIeMbPFZvZLSXL3JZLmSVoq6SFJl7h7U/RWX5B0k8IUlzeUPI8w65gOCgDoZVWSqtz9uejx7xVC4cZosTRFt5tStk83k6aFHp0pQwgEgNgrytYbu/v5aZpvbmf7OZLmpGlfKOmwHuxap5gxHRQA0LvcfYOZrTWz6e6+XNIpCl+QLpU0S9LV0W1iVsx8Sb81sx9LGqtoJk02+0gIBID4y1oIjDsTC8MAAHLiS5LuMrMBkt6U9BmFmTvzolk1b0k6TwozacwsMZOmUS1n0mQFIRAA4o8QmAGVQABALrj7Ykkz0jx1Sobt086kyRZCIADEXy6vE9inUQkEAKAtQiAAxB8hMAMzVgcFAKA1QiAAxB8hsB1MBwUAoCVCIADEHyEwA6aDAgDQFiEQAOKPEJgBC8MAANAWIRAA4o8QmAGVQAAA2iIEAkD8EQIzoBIIAEBbhEAAiD9CYAZUAgEAaIsQCADxRwgEAACdxhWUACD+CIEZMB0UAIDMqAQCQHwRAjNgOigAAG0xHRQA4o8QmAGVQAAAMiMEAkB8EQIzoBIIAEB6ZoRAAIgzQmAGVAIBAEiPEAgA8UYIzMDE8mcAAKRDCASAeCMEtoPpoAAAtEUIBIB4IwRmwHRQAADSIwQCQLwRAjNgYRgAANIjBAJAvBECM6ASCABAeoRAAIg3QmAGVAIBAEiPEAgA8UYIzMCM1UEBAEiHEAgA8UYIbAfTQQEAaIsQCADxRgjMgOmgAACkRwgEgHgjBGbAwjAAAKRHCASAeMtaCDSzW8xsk5n9I6VtuJk9YmavR7fDUp670sxWmtlyMzstpf0YM3s1eu5666WT9agEAgCQHiEQAOItm5XA2ySd3qrtCkkL3H2apAXRY5nZIZJmSjo0es0NZlYYveYXkmZLmhb9tH7PrKASCABAeqydBgDxlrUQ6O5PSNrSqvlsSbdH92+XdE5K+1x3r3f3VZJWSjrWzMZIGuzuz3goy92R8pqsMjHCAQCQCZVAAIiv3j4ncJS7r5ek6Ha/qH2cpLUp21VFbeOi+63bewXTQQEAaIvpoAAQb31lYZh0ZTdvpz39m5jNNrOFZrawurq6ex1iOigAIAfMbHV0LvxiM1sYtXX5nPrs9pEQCABx1tshcGM0xVPR7aaovUrShJTtxktaF7WPT9Oelrvf6O4z3H1GZWVltzrKdFAAQA59wN2PdPcZ0eN9Oac+awiBABBvvR0C50uaFd2fJem+lPaZZlZiZlMUFoB5PpoyWmNmx0ergl6U8pqsSixCypRQAEAf0KVz6rPdGUIgAMRbNi8RcbekZyRNN7MqM7tY0tWSTjWz1yWdGj2Wuy+RNE/SUkkPSbrE3Zuit/qCpJsUBrY3JD2YrT636H9UCWRKKACgl7mkh81skZnNjtq6ek59Cz15ukR4P0IgAMRZUbbe2N3Pz/DUKRm2nyNpTpr2hZIO68GudUovXY4QAIDWTnD3dWa2n6RHzOy1drbt1Lnz7n6jpBslacaMGd2Ob4RAAIi3vrIwTJ/FdFAAQG9y93XR7SZJ9ypM7+zqOfVZRQgEgHgjBGbAdFAAQG8zswozG5S4L+lDkv6hLp5Tn/1+EgIBIM6yNh007lgYBgCQA6Mk3RuNQUWSfuvuD5nZC5LmRefXvyXpPCmcU29miXPqG9XynPqsIQQCQLwRAjOgEggA6G3u/qakI9K0b1YXz6nPJkIgAMQb00EzoBIIAEB6hEAAiDdCYAZcLB4AgPQIgQAQb4TADjAdFACAlgiBABBvhMAMmA4KAEB6hEAAiDdCYAYsDAMAQHqEQACIN0JgBlQCAQAAAPRHhMAMqAQCAJAelUAAiDdCYAaJSiAAAGiJEAgA8UYI7ADTQQEAaIkQCADxRgjMgOmgAACkRwgEgHjbpxBoZkU93ZG+hoVhAAA9qT+NnYRAAIi3jCHQzJ5Muf+bVk8/n7Ue9RFUAgEAXZUvYychEADirb1KYEXK/UNbPdfvV02hEggA2Ad5MXYSAgEg3toLge39773f/6/f+s9YDQDoPXkxdhICASDe2js/YaiZfVQhKA41s3OjdpM0JOs96yOYDgoA6IK8GDsJgQAQb+2FwMclnZVy/59Tnnsiaz3qI5gOCgDYB3kxdhICASDeMoZAd/9Mb3akr2FhGABAV+XL2EkIBIB4a2910H82s0kpj//HzF42s/lmNqV3upc7VAIBAF2VL2MnIRAA4q29hWHmSKqWJDP7iKQLJH1W0nxJv8x+13KLSiAAYB/kxdhJCASAeGt3dVB33xXdP1fSze6+yN1vklSZ/a7lVqISCABAF+TF2EkIBIB4ay8EmpkNNLMCSadIWpDyXGl2u9V3MB0UANAFeTF2EgIBIN7aWx30p5IWS9ohaZm7L5QkMztK0vqs9yzHmA4KANgHP1UejJ1MlgGAeGtvddBbzOwvkvaT9HLKUxsk9fvVz1gYBgDQVfk0djI8AkB8ZQyBZnZ0ysMj05wj99a+7tTM/p+kf5Xkkl5VGBjLJf1O0mRJqyV9wt23RttfKeliSU2Svuzuf9nXfXe6j1QCAQBdlM2xsy9hOigAxFt700EXSlqiaJUzSakjmUs6eV92aGbjJH1Z0iHuXmdm8yTNlHSIpAXufrWZXSHpCkmXm9kh0fOHShor6a9mdqC7N+3L/rvQT0lUAgEAXZKVsbOvIQQCQLy1FwK/KuljkuokzZV0r7vv7MH9lplZg0IFcJ2kKyWdFD1/u6THJF0u6WxJc929XtIqM1sp6VhJz/RQX9KiEggA2AfZHDv7DEIgAMRbxtVB3f0n7v4+SV+UNEHSAjObZ2ZHdmeH7v62pGsUpsSsl7Td3R+WNMrd10fbrFc4n0KSxklam/IWVVEbAAB9SrbGzr6GEAgA8dbeJSIkSe6+StJ9kh5WqMAd2J0dmtkwhereFIXpnRVmdkF7L0nXrQzvPdvMFprZwurq6nSbdKWfYUeMcgCALuru2GlmhWb2kpndHz0ebmaPmNnr0e2wlG2vNLOVZrbczE7ryc+RuX+EQACIs4wh0MymmtnXzOw5Sd9WWOXsIHef1819flDSKnevdvcGSX+U9F5JG81sTLTvMZI2RdtXKXybmjBeYfpoG+5+o7vPcPcZlZXduyYv00EBAF3Vg2PnpZKWpTy+QuG8+WkK1x68Itpf6nnzp0u6wcwKu/kxOkQIBIB4a68SuFLSJyQ9pHD+3URJ/25mXzGzr3Rjn29JOt7Myi2U205RGOjmS5oVbTNL4RtURe0zzazEzKZImibp+W7sv1OoBAIA9kG3x04zGy/pTEk3pTSfrXC+vKLbc1La57p7fVR9TJw3n1WEQACIt/YWhvmOktMuB/bUDt39OTP7vaQXJTVKeknSjdE+5pnZxQpB8bxo+yXRCqJLo+0vyfbKoBKVQADAPumJsfOnkv5L0qCUthbnzZtZ6nnzz6Zs1yvnzRMCASDe2rtY/LeytVN3/6akb7ZqrleoCqbbfo6kOdnqTzpUAgEAXdXdsdPMPiJpk7svMrOTOvOSdN1I876zJc2WpIkTJ3ani9H7EQIBIM46XBgmX1nacRUAgKw6QdJZZrZa4RITJ5vZnermefM9ec586AMhEADijBDYAaaDAgB6i7tf6e7j3X2ywoIvf3P3C9TnzpsnBAJAnLV3TmBeYzooAKAPuVp96bx5QiAAxFqnKoFmdnLqbT5gYRgAQHd0d+x098fc/SPR/c3ufoq7T4tut6RsN8fd93f36e7+YM/0vn2EQACIt85OB72m1W2/RyUQANBN/XbsJAQCQLx19ZzAvFkthUogAKCH9LuxkxAIAPHGwjAZJCqBAACgJYZIAIg3QmAHmA4KAEBbDI8AEF+EwAyYDgoAQHpMBwWAeOtsCNwZ3dZkqyN9DQvDAAC6qd+OnYRAAIi3ToVAdz8x9TYfUAkEAHRHfx47CYEAEG9MB82ASiAAAOkRAgEg3giBGVj/W9EbAIAeQQgEgHgjBHaA6aAAALRECASAeOswBJpZhZkVRPcPNLOzzKw4+13LLaaDAgD2VX8fOwmBABBvnakEPiGp1MzGSVog6TOSbstmp/oCFoYBAHRDvx47CYEAEG+dCYHm7rsknSvpf939o5IOyW63co9KIACgG/r12EkIBIB461QINLP3SPq0pD9HbUXZ61LfQCUQANAN/XrsJAQCQLx1JgReJulKSfe6+xIzmyrp0az2qg9IVAIBANgHl6kfj52EQACItw6/lXT3xyU9LknRSe7vuPuXs92xvoLpoACArurvYychEADirTOrg/7WzAabWYWkpZKWm9l/Zr9rucV0UADAvurvYychEADirTPTQQ9x9x2SzpH0gKSJki7MZqf6AhaGAQB0Q78eOwmBABBvnQmBxdG1jc6RdJ+7N0j9vzxGJRAA0A39euwkBAJAvHUmBP5K0mpJFZKeMLNJknZks1N9AZVAAEA39Ouxk7XTACDeOrMwzPWSrk9pWmNmH8hel/qGRCUQAICuyoexk+9IASC+OrMwzBAz+7GZLYx+rlX4ZjMvMB0UANBV/X3sZDooAMRbZ6aD3iKpRtInop8dkm7tzk7NbKiZ/d7MXjOzZWb2HjMbbmaPmNnr0e2wlO2vNLOVZrbczE7rzr670EdJTAcFAOyTHh87+xJCIADEW4fTQSXt7+4fS3n8bTNb3M39XifpIXf/uJkNkFQu6WuSFrj71WZ2haQrJF1uZodIminpUEljJf3VzA5096Zu9qFdLAwDAOiGbIydfQYhEADirTOVwDoze1/igZmdIKluX3doZoMlnSjpZkly9z3uvk3S2ZJujza7XWFFNUXtc9293t1XSVop6dh93X8X+qmof9neFQCg/+nRsbOvIQQCQLx1phL4eUl3mNmQ6PFWSbO6sc+pkqol3WpmR0haJOlSSaPcfb0kuft6M9sv2n6cpGdTXl8VtWUVlUAAQDf09NjZpxACASDeOqwEuvvL7n6EpMMlHe7uR0k6uRv7LJJ0tKRfRO9VqzD1M5N0y3SmHXrMbHbiJPzq6upudDFZCQQAoKuyMHb2KYRAAIi3zkwHlSS5+w53T1zj6Cvd2GeVpCp3fy56/HuFULjRzMZIUnS7KWX7CSmvHy9pXYY+3ujuM9x9RmVlZTe62OI9e+R9AAD5p6tjp5mVmtnzZvaymS0xs29H7X1s8TRCIADEWadDYCv7XCZz9w2S1prZ9KjpFElLJc1XcqrMLEn3RffnS5ppZiVmNkXSNEnP7+v+O4vpoACAHtaZsbNe0slRFfFISaeb2fEKM2YWuPs0SQuix2q1eNrpkm4ws8Is9L0FQiAAxFtnzglMp7v/6/+SpLuilUHflPQZhUA6z8wulvSWpPMkyd2XmNk8haDYKOmSbK8MKrEwDACgx3U4oHgYdHZGD4ujH1dYJO2kqP12SY9Julwpi6dJWmVmicXTnunJjrdGCASAeMsYAs2sRukHLJNU1p2duvtiSTPSPHVKhu3nSJrTnX12FZVAAEBX9cTYGVXyFkk6QNLP3f05M+tbi6cRAgEg1jKGQHcf1Jsd6WuoBAIAuqonxs5otsuRZjZU0r1mdlg7m3dq8TQzmy1ptiRNnDixu10kBAJAzO3rOYH9nu37aY8AAHRbdA3dxxTO9evW4mk9vXAaIRAA4o0Q2AGmgwIAeouZVUYVQJlZmaQPSnpNfW3xNEIgAMTavi4M0+8xHRQAkANjJN0enRdYIGmeu99vZs+oTy2eRggEgDgjBGbAwjAAgN7m7q9IOipN+2b1pcXTCIEAEGtMB82ASiAAAOkZp80DQKwRAjOgEggAQGZ8RwoA8UUIzMD4mhMAgLSYDgoA8UYI7ADTQQEAaIkQCADxRgjMgOmgAACkRwgEgHgjBGbAwjAAAKRHCASAeCMEZkAlEACA9AiBABBvhMAMqAQCAJAeIRAA4o0QmAGVQAAA0iMEAkC8EQIBAECXEAIBIN4IgRkwHRQAgPQIgQAQb4TADJgOCgBAeoRAAIg3QmAGVAIBAEiPEAgA8UYIzIBKIAAA6RECASDeCIEZUAkEACA9QiAAxBshMINEJRAAALRECASAeCMEdoDpoAAAtEQIBIB4IwRmwHRQAADSMybLAECsEQIzYGEYAAAy4ztSAIgvQmAGVAIBAEiP6aAAEG+EwAyoBAIAkB4hEADijRCYgXHCAwAAaRECASDechYCzazQzF4ys/ujx8PN7BEzez26HZay7ZVmttLMlpvZab3ZT6aDAgDQEiEQAOItl5XASyUtS3l8haQF7j5N0oLosczsEEkzJR0q6XRJN5hZYbY7x3RQAADSIwQCQLzlJASa2XhJZ0q6KaX5bEm3R/dvl3ROSvtcd69391WSVko6thf6KIlKIAAArRECASDeclUJ/Kmk/5LUnNI2yt3XS1J0u1/UPk7S2pTtqqK2rKISCABAeoRAAIi3Xg+BZvYRSZvcfVFnX5KmLe3QY2azzWyhmS2srq7e5z5G7xV2xCgHAEALhEAAiLdcVAJPkHSWma2WNFfSyWZ2p6SNZjZGkqLbTdH2VZImpLx+vKR16d7Y3W909xnuPqOysrJbnbS02RMAABACASDeej0EuvuV7j7e3ScrLPjyN3e/QNJ8SbOizWZJui+6P1/STDMrMbMpkqZJer7X+st0UABALzGzCWb2qJktM7MlZnZp1N6nVtAmBAJAvPWl6wReLelUM3td0qnRY7n7EknzJC2V9JCkS9y9KdudYTooACAHGiV91d0PlnS8pEuiVbL71grahEAAiLWchkB3f8zdPxLd3+zup7j7tOh2S8p2c9x9f3ef7u4P9kbfWBgGANDb3H29u78Y3a9RuJTSOPWxFbSLi6WGBunPf872ngAA2dCXKoF9CpVAAEAumdlkSUdJek59bAXtiy+WDj9cOu88acWKbO8NANDTCIEZUAkEAOSKmQ2U9AdJl7n7jvY2TdPWZuDqydWzJWnSJGnuXKmuTnryyW6/HQCglxECM0hUAgEA6E1mVqwQAO9y9z9Gzd1aQbsnV89OGD483O7e3SNvBwDoRYTADjAdFADQWyx8A3mzpGXu/uOUp/rcCtplZeGWEAgA8VOU6w70VUwHBQDkwAmSLpT0qpktjtq+prBi9jwzu1jSW5LOk8IK2maWWEG7Ub20grYklZaG27q63tgbAKAnEQIzYGEYAEBvc/cnlf48P0k6JcNr5kiak7VOZVBcHC4VQSUQAOKH6aAZUAkEACAzszAllEogAMQPITADKoEAALSvtJRKIADEESEwA8s4GwcAAEihEkgIBID4IQR2gOmgAACkV1rKdFAAiCNCYAZMBwUAoH1MBwWAeCIEZsDCMAAAtI/poAAQT4TADKgEAgDQPqaDAkA8EQIzoBIIAED7mA4KAPFECMwgUQkEAADpcZ1AAIgnQmAHmA4KAEB6iUrg8uVSfX2uewMA6CxCYAZMBwUAoH1lZdLatdJBB0mXXZbr3gAAOosQmAELwwAA0L7SUqm2Ntx/9tnc9gUA0HmEwAyoBAIA0L7S0uT9UaNy1w8AQNcQAjOgEggAQPvKypL3R47MXT8AAF1DCMwgUQkEAADppVYC9+zJXT8AAF1DCOwA00EBAEivpiZ5f8eO3PUDANA1hMAMmA4KAED73ve+cDtxYstACADo2wiBGRQXFEuS9jQxvwUAgHQ++lGpqUmaMSNZCdy4UXr55dz2CwDQvqJcd6CvGjhgoCqKK/R2zdu57goAAH1WQYE0aFAyBL773eHagUykAYC+i0pgBmamCUMmaO2OtbnuCgAAfdrgwckQuDYaNjlHEAD6rl4PgWY2wcweNbNlZrbEzC6N2oeb2SNm9np0OyzlNVea2UozW25mp/VWXycMnqC12wmBAAC0Z/DgcE5gavXvrbdy1x8AQPtyUQlslPRVdz9Y0vGSLjGzQyRdIWmBu0+TtCB6rOi5mZIOlXS6pBvMrLA3OjphMJVAAAA6MnhwODewri7ZtmZN7voDAGhfr4dAd1/v7i9G92skLZM0TtLZkm6PNrtd0jnR/bMlzXX3endfJWmlpGN7o68ThkzQhp0bVN9Y3xu7AwAglgYNCrepU0AJgQDQd+X0nEAzmyzpKEnPSRrl7uulEBQl7RdtNk5SajmuKmrLuklDJkmSVm9b3Ru7AwAglkaPDrfLlyfbCIEA0HflLASa2UBJf5B0mbu3d/q4pWlLu+aYmc02s4VmtrC6urrbfTx2XCg4PlP1TLffCwCA/uoDH5CKiqRrr0223X+/dNVV4f4DD0jz5+embwCAtnISAs2sWCEA3uXuf4yaN5rZmOj5MZI2Re1VkiakvHy8pHXp3tfdb3T3Ge4+o7Kystv9PLjyYA0rHaYn33qy2+8FAEB/NXSodPLJ0p/+lGxbulT6+tfDeYJnnimdfXbOugcAaCUXq4OapJslLXP3H6c8NV/SrOj+LEn3pbTPNLMSM5siaZqk53ujrwVWoBMmnqCn1j7VG7sDACC2br5ZGj483E+cIyhJmzal3x4AkDu5qASeIOlCSSeb2eLo58OSrpZ0qpm9LunU6LHcfYmkeZKWSnpI0iXu3tRbnX332Hdr+TvLtXPPzt7aJQAgT5nZLWa2ycz+kdLW5y6hlM748dKKFdI3viGdd16yfePG3PUJAJBeLlYHfdLdzd0Pd/cjo58H3H2zu5/i7tOi2y0pr5nj7vu7+3R3f7A3+3v0mKPlci3esLg3dwsAyE+3KVwOKVWfu4RSJiNGSN/5jjR5crItNQTu3t3rXQIApJHT1UHj4Jgxx0iSXnj7hRz3BADQ37n7E5K2tGruc5dQ6kjqaflvvpm8v3lz7/elPc3NUmNjrnsBAL2PENiBMYPG6LD9DtMdr9wh97SLkgIAkE197hJKHSlI+evittuS9/taCDzzTKm4ONe9AIDeRwjshEuPu1SLNyzmUhEAgL4kZ5dQ6sjBByfvL16cvP/OO1nfdZc89FCuewAAuUEI7IRPHvpJlRaV6u5X7851VwAA+afPXUKpI+9/v/T6623b9yUEPvNMmLYJAOg5hMBOGFQySGdNP0t3vHKHllYvzXV3AAD5pc9dQqkzDjggXDA+VVeLkE89Jb33vdIPftBz/UqH8wIB5BtCYCf98IM/VHlxuc646wxV12Z/Kg0AIP+Y2d2SnpE03cyqzOxi9dFLKHXGmWe2vGZg61DYkcQ1Bh9/vOf6lE5tbXbfHwD6GkJgJ00aOkl/Ov9PWrt9ra5/7vpcdwcA0A+5+/nuPsbdi919vLvf3FcvodRZDz4ofeQj0re+Fc7BW7Kk86/dvj3cbtuWjZ4l7eRSwADyDCGwC2aMnaGzpp+lHz79Q33z0W9qV8OuXHcJAIA+7YQTpD/9SfriF6WyMunaa9tu8/LL0oc+FJ57441k+4YN4Xbr1uz2MRuVwIaG8AMAfREhsIuuO/06nTLlFH3nie/oqr9flevuAAAQCyNGSJ/7nHTrrdJf/9pysZef/ER65BHpP/4jnEu4NDr9PnGh+aqqEAj/93+lTFdrqq2Vpk6Vzjmn44VkPvYx6eijk4+zUQmcNEkaO7bn3xcAegIhsIsmDZ2kBz79gM6afpZ+sfAXuuPlO7RzD/NIAADoyFVXSZMnS9/9rjR+vHT55WHK5733SuXlye1efTXcJiqBu3aFSuGXvywtXx7ahg6VvvGN5PbPPCOtWiXdd1/LamI6f/yj9NJLycfZqASuX9/3LokBAAmEwH005+Q5Kioo0qz/m6UvPfilXHcHAIA+r6xMuvhi6YknQkj64Q+lww8PIe8Pf0hu9/zzUlNTshIoJYPhunVSTU0Ij9/7nrRiRXiPU09NbtvVVUhbh8DGRumVV7r2HgAQJ4TAfXTYfodpxRdX6JOHflK3Lb5N//fa/+W6SwAA9Hn//u/SWWclH7/1lvTb30qnny6tXCmVlEg//rH09a+HoDh5csvXr1kTfhKWLWu7j1//Orw2Yfhw6ROfCPfTXQ6i9XTQb39bOuKI5LRUAOhvCIHdMKR0iG45+xYdO+5YnXfPeRr343Ga/rPpuv6561VTX5Pr7gEA0OcMHx6mbD7xRHj8jW9I550X7u+/fzL0/eAH0muvSR//eMvX/+xn0rvelXy8enXbfdx2Wzgfb/Zs6c47w8Iy99wTnkutLia0rgQ+/XTL9z77bOmGGzr3+aSWC8I0pVy04/vflx54oPPvAwDZQgjspvLicj18wcP67JGf1XsnvFcrNq/QpQ9dqgvvvVC/XPhL/X7p7+WZzmIHACBPvf/90pNPhktHpJo/X7riinC/tFT62tdaPv/iiy0f/+Mfmffx619LF17Ysu3tt9tut3NnmJI6c2Y4n7CkJLSvXx9C3Pz50iWXdPiR9kpdzTRxmYvm5vBZzjyz5bZr14Ypsrt3d/79AaC7CIE9YEjpEP3qn3+le867R7848xd6/8T3677l9+kLf/6CzrvnPJ3/h/O1ZNMSubuamvvUdXwBAMiZE06QClr9JXLggWEBmYULQ8AbNqz997jppq6twpkuBNbWSn/7m/S734XFZwoLQ/uaNckL1nfknXekf/qnMKV1y5Zke+Iah1VV6V/3pS9Jt9wiLViQ+b0fekjavLlz/eiMt9+WDj00LKQDID8RAnvY52d8Xk985gk99OmH9OisR3Xh4RfqnqX36LBfHKYB3xuggd8fqH+b/2+6bfFtWl+zXtt3b9c9S+7RJ3//ST3yxiO57j4AADlnJh1zTJgeKkl33x2mkP75z9JddyWnjyYC4siRnXvf+npp8eK27bW1YcEZKSw6k1jVc/XqluEt3cSe2lpp0SLpk58MU1wvvzx9CFyxIn2fEs/X1WV+/owzwnmUVVXSr36V/rzGrrjttnC+469+1b33Qf644YYwVZvJbf1HUa470F+ddsBpkqSTJp+kaz50jW596VZtr9+uZe8s0y2Lb9FNL93U5jXzlszTXefepdP2P01/eeMvOnjkwTq48mCVFpX2dvcBAOgzZs5s+fj886V588IUyx/8IFzz7/TTO36fL3whXJj+ve9NnvcnSd/5jvTBD4b7W7cmF4r5zW+kceOS223e3DJwtn4shctadBQCt2+XhgwJ9xPTQDNVChPVuqefliZMCPfHjpX++Z/b/aj61rekgw5qe+zWrAmX05DaVmE769vflgYODGH9K1/Zt/foij17wnTdoUOzv69MVq6ULroofBlRWZm7fuRKYjp0ut957Lurr5bmzk3/5VS2EQJ7wX4V++ny912+97G765WNr+j/Xvs/XffcdRpSOkTvHvtuPbX2KX36j59u8doCK9C04dM064hZ2lK3Rau2rdIxY47R75b8Ttd+6Fq9Z8J75O6qGFDR2x8LAICcMAu3BQXSlVeG+1/+cggJDz8sPfts+IP9Bz+QduyQfvSjMG301lvDtt/7nlRcLD3+uDRqVFgs5q9/Dc8lzjGsqAhVvquvTu73rrtCiHruuXDdwtbnM0rhnMVHH00+fuqpML305ZeTbStWSO9+d7ifCIwPPihdc02o0s2eHVZIPeOM9FM2//Y36Re/kP7rv8K0ztahZM+eENSktiEwdbXV+vqWzzU1JafCZrJ0acvPffDBoZ9d5R76dv750jnntL/tpz4VLiHS3Jz8t8+W5cvDv+9FF7VsnzMnhOd58zp3fmhq0N9X990nTZkSLoHSV6xcSQjsSYn/fzU2SkW9nMqsvy5aMmPGDF+4cGGuu9Ehd5dF/0drbG7U39f8XfcsvUcnTT5Jf33zryovLtfjax7X4g2LVVJYohHlI7SuZl2L9zCZSopKNLhksCqKK1RaVKoBhQP0rlHv0uiK0drTtEfv1L2jiYMn6tT9T9Vjqx/TIZWHaFPtJg0uGazpI6br8FGHq7G5UX9c9kedfsDpGjNojHY17JIUFr8BgL7KzBa5+4xc9yMu4jI+7iv3EBZSw8zzz0vHHRfun39+mAa5a1e4aP0114SQ+D//ExarKS+X/vKX0Pb229JPfxpeN2ZMy8tOZDJuXPrzDiXpqKPCN/7Dh4eVT5ubw/ap0ztHjw7VREk6+WRp0qRkeB0/PgTft95Kbj98eFj59AMfSAakRYukGdF/EanBadky6ZBDkq+dOTNMtZXCKqoXXhiqEh/8oDRiRPrP8N3vhmOVcNpp4ZzFrnrrrfDZWvcxncRzy5eHc0YTXnklnFN5773hOPSEESNCMN+xQxo0KNl+0UWhMnz11WHKrxRC9A03hGN62mnJbRPH+YYbQvW5I7fcEhYH2rIlOcU5UXEbNCj0pbVFi8LxSw1kO3aELzPOPbfrn7sj5eVhyvKdd0qf/nTH22fL3/8evlhJLB7VUxobw+/XoYf27Pt2JPG7vXZt+O87O/tIP0YSAmOgsblRr29+XVOHTVVxYbGerXpWW+u26uE3Htauhl0qLSqVy7WxdqP+vOLPOnDEgWpobtCWui3aUrdFzd68930yGV42XHua9mjnnp0aNGCQBpUM0uZdm9XkTTpmzDHaf/j+enrt0zpi1BFaUr1EJ048UQeNPEhrtq9RszfrtXde01Gjj9LHDvmYpg2fptqGWj208iE1NTep2Zu1tHqpjht/nCYPnaypw6Zq9MDRWl+zXpUVlSovLpe7q66xTmu2rdGBIw5UYUEHX0UCQIQQ2DX9aXzsrKamUBWcPTt9FcM9VNzGjAkXtH/qKenII8NKoR/6UAhvd98dQsf06eHC9Fu2hIrYqlXSkiWh+vj5z4fKzb/8i/Se9ySnXSY89VT44/5jH0teRmLAgLDPxCqiCQMGhIpeqjvvDJfIePbZtp/huONCfwYPDkEzUa36xCfCtRn/9Cfp2mvbvu5Tnwqfu/VzV14Z2n/zG+nmm8P5mP/5n+FczURVc+LEEIyvuSYcV3fpuuvC8ZgyJQTp1CrhmjVhxdYjjgjbfvKToX3UqDAdtqgoBMING5KL/bi3nLb6uc+Fz7Z5cwjzc+eGf5uZM6VZs8J7J6aobtgQLslx1VWhOltQECq1xx4bgtzRR4cKcqrEH+V//7v0vvcl2085JVRgP/OZENqk8L5f/3qYppsazG+8MfRTCv+GxcUt9/HLX4aVce+8MzweOTJ8nvnzk9N8r79euvTS5DFIVV8fVs494ohQ+R48ODw+55xQPVy6NFRou8K9/SA+bFiY2vyNb0j/9m/JqckJ69aF37PrrkuG+xdeCKGqvJ1aQk1NcmpxZyS2e+edzF9U7Is5c6T//u/w+3HUUem3eeed8N/DpZemn0qd+DKnKxW9xOd5+unw/4xsIATmido9tSovLt9bXZRCtbGxuVHrd67Xn1f8WUePOVobazdq6rCpcne9ufVN/eaV32jr7q2aeehMPVv1rMxMAwoH6JWNr6iusU7ratZpR/0ONXuzdjcm17EuKypTgRXowBEHavGGxXKl/30qLSrd+7pCK9TgksHaunurBhQO0OShk7Vx50Ztrw8j4JShU1RUULS3Enn0mKPlcu1p2qMCK9DYgWO1oXaDVmxeoRMmnKDRA0eruKBYb9e8rfqmejU2N+qwysM0ZdgUPVv1rI4YdYQqKypVUliiSUMnacPODXpz65uaNGSS9jTt0TFjj1FdQ52KC4tVUliimj01qmuoU7M3y8xUWV6pkeUjtWHnBo0eOFq7G3erYkCF9jTt0YDCAWpoalBRQVGLY5567NO1A+g5hMCuydfxsSdt2hT+CEwXKBsbw1TOiy4K0wrvvTdUH8eMSU7re+gh6f77wx+xp5wSAsEvfxkCyX/+Zzhn8YEHpI9+NDm1dNmycI7fHXdIt98eKnJz50r/+79t+zB0aPJcxNYGDkye89hVn/qU9NvfhkA9dWoIAscfn3n7QYOkr341bDtoUAjHibA7Y0ZYATbh3/89BKHXXw9/iP/pT6GCe+qp0okntt+vd70rBM7EeaGPPx7+kD/55OQ+SktDlXX1amnatLCfkhKpujpMxZ0wIbymrCz5vg8/HPbf2Bgqtps2hX4//XQIcJ/9bHLbs88OIeHRR8NnuSla+uH668PU4m3bwr/3Aw+EKaVS+FLg3e8OIW737nCsrrkmPHfyycl/++uuC++5cWMIgLNnJ1eTLS0NIforX0mGxjvuCNW6E0+UPvzhEHgLCkLgWL489O273w1fNJx7blh06NZbw/mwf/hDOBavvBK+6Pjwh8PnHzCgZRh94onwxcOAAeHxJz4RKtKXXBK+qFixInxhMmtWmOIshS8+ioqSwWfDhvDfxVVXJadFSuFLm5roctv33x/+3b73vfC5EgHzj38M/32kqqkJv1+pFbXGxvBvcO65LadCt/ahD0mPPBIqjN//fvptzj8//Df3+OMtfyc3bgy/NyefHG6POCJMm73nnvA7M3Vq6EfrLwNSv+CYNy+54FVPIwSiR7i7djfu1qOrH9Vx447T8LIw/8LMtHDdQr2++XVtrN2oAivQKVNOUWVFpeob6zVu8Dg9tvoxrdyyUn954y8qKyrTiZNO1MJ1C7Vm+xodMOwAjR0UvvZbsGqB6pvqZTKNKB+h1955TaVFpSotKlV9Y702121WRXGFpo2Ypr+v+btqG2rV1Nyk4sJi7Wna0173e8yQkiHaXr9dR4w6Qq9uelXv2u9dGlk+Umu2r9Huxt0aVjpMG2s3amvdVp12wGna3bhbBVYgk6nAClS1o0pHjTlKFcUV4fM21uvMaWeqqqZK7q6SohINKBygQivUnqY9Gj1wtJq9WWMHjdW6mnWaOGSiigqKtHb7Wg0tHaq3a97WiLIROrjyYDU2N2rb7m0aVTFKJUUlqiiu0PLNyzVwwEBVlldqU+0m7ajfoZo9Ndq5Z6eOGXOMDhp5kB5b/ZjeM+E9qtpRpdXbVmvqsKkaVTFKRQVFenH9i5oybIoGDRik8uJyvb7ldRUVFMndNaxsmA4aeZBq6mu0tHqpCgsKNW7QONU31auyPJyo0tjcqMElg7WrYZeef/t5zRg7Q6MHjg6fYUf4DHUNdZowZIJMpi11WzS0dKjW7lirgQMGakTZCJmZ3F21DbVqaGrQkNIhMlmnQnZTc5NcrqKC8PWcu8vlKrC2X+U1e7PcnWp0jBACu4bxsW+bNStcOmP27PB46dLwB2Sm88Kqq8PU1rlzpZ/8JASh3/1O+v3vQ5g699xQMSstDaHqssukN98MIeall0LYuummUEFK2LkzVOzuuCNsIyXDY0VF6NPEiaH9sstCSEn12mvhD/zTT297/cP77gsVq67++XnggWG66je+0bI9EegyKStrufJqYWEIGSNHJleBTSguTlZoE6/93vekxx4Lx+mgg8JnKylJnk953HEh7Cckthk7NrnibHvMksdi9OgQQioqQrXtXe+SXn214/foSElJCF/HHx+qme7hs118cQhsraUe05/8JFSYW1eqpRCQv/rV8Ds4a1ao4o4YIf3wh2Eq7KJFYbtE0Pvud6UDDghfYlx+efj9SrjyylCN27EjhKk33gjX1Pztb5PbHH54CKcJxxwT/m2WLQu/01/4QgiyTz8dplfut18Ibhs2hH/vxx8PofJ3vwufcezY8N9abW2yql9WFsL/+vWh4v7yy6G/3/++9POfh/0mAv83vhFC8Mknp/99OuGEEPSPPjq895o14cuQ9eulCy4I1e/EglHHHx/2O2hQOPZ1deH3pzMLXnWEEIh+KfHHfKo3t76phqYGbdu9TUeOPlIvrHtBO/fs1JCSIVqzfY0GlwzWgSMO1Jpta+Ryrdi8QhXFFWpsblR9U73Kiso0qGSQTKZmb9bG2o167u3nNGbgGJUXl6u8uFyrtq5SY3OjXt30qoaVDdPuxt3a07RHk4ZMUklRyd4Q1tjcqMdWP6aR5SNVYAWqbajV6m2rNWPsDL3w9gtq9mYdNeYoNTQ1aNH6RRpWOkzFhcV7+z+8bLgGlQxS1Y6qvQG3pLBE9U0tz+YfXDJYtXtq1eRduw5lcUGxGpobOt6wF5UXl2tXwy4VFRTtncI8cMBAjR44Wqu3rd7bVlRQpLKiMtU31auooEjFBcUqLixWU3OTavbUaOCAgSq0Qo0dNFZrd6xVTX2NhpcN1849O9XQ3KCK4goNLR0qSSosKFRjc6OGlQ5T1Y4qNTQ36H0T36fdjbv1zq539M6ud7Rt9zZNGDxBzd6sAitQcWGxqnZU6bD9DpPJVFZcps27NquxuVH7D99fW+u2aueenXK53F3b67errqFOHz/k4yq0Qi2pXqItdVvU5E0qtEIdXHmwttRtUe2eWhUXFu8NvPsP219rtq9RXUOdhpUN07DSYWr2ZlUMqNDQkqEaP3i83tj6hpZWL9WwsmGqb6zXqIGjtKN+hwYNGKRdDbtkZiq0Qg0vG743RJuZiguKNaJshFyu1dtWa1jpME0eOjn8PjXU7v08qcdod+NulRSGLykSAX7MwDF7K+eDSwZrcMngvdPEm5qbtH7nek0dNlXN3qwxA8doQOGAvf/eB1cerJOnnNyt3xlCYNcwPvZfiSl9nVlkor4+/ME8enR4vHNn+MN7587wx2vCm2+GP4xHjAgB4uCDk9WYhIaGsO8LLgh/FCemOdbWhv089VSoyB11VAiAixaFKYI7doQ/fn/3uzCF9YMfDFWcH/84/GF/wAFhaul114UpnGYhFA0YEKZkbt0aKmDPPhumY27bFoJTbW0IGQcdFKppVVWhyrpgQagKVVeHvn7sYyEgnXFGqH49+2x4/aGHhuNw1VXJvs6ZI33xi6GS9+c/h1C9cGHo21e+EqaPHnxw6LcUKnrr1oXPkmrkyBAGKqL1/CorQ7Xtq1+VTjqp5cqyDz4YKkRHHhlC1T/9Uwgyy5cntznhhFB9++tfw9TUU04J1c6Ec88NoWjDhhAwGhszX46kI1dfLX3zm20XFEqYOzdULBOfoXWoTlVYGCqF27Ylw3JJSfhda13Fnj07BOsnngiPL700+cXDkCHpA2priYWepPB7tXZt28/xta+F0Fldndy2p514Yvgs6a4/WlERpt2mrhT83HPhd787CIFAH5cIFwm7G3eruKB4b0Vqa91W7Wnao8qKStXuqVVjc6PKi8u1pW6LRg0cpa11W7Vq2yqVF5draOlQbd+9XfVN9Xp98+uaPnK6TKZdDbs0euBoDSkdooEDBspk+vtbf9ebW9/UoZWH6o2tb2jK0CmaNmKaVm1dpc11m1VTX6PpI6dr867Nqm2o3fvYPVTSttRt0eNrHldleaXeM+E9ampu0lvb31Jjc6Oqd1WruKB47+eRpGPGHqMn1jyh4oJi7ajfoSnDpqiuIYxIG2s3alPtJk0fMV1rtq/R/sP2V82eGq3aukrb6rfpwOEHanjZcNU31WvDzg17K4zN3qyGpoa9gSURYhqbG/Xm1jc1fvB4jaoYpepd1RpSMkTFhcVaV7NOTd6kAivYe+xXb1u9dzryc28/p8ElgzWyfKRGlo3UkNIhWrxhsSRpZPlI1TfVa9CAQXpr+1sqKijS2zVva2jpUJUVlWlj7UaNLB+59xibmYoKirRm2xq9sfUNNXuzJg6ZqMlDJ6vQCrW7cbde2vCShpYO1eCSwXunEQ8pGaLlm5drd+NumUwNzQ17Q1btntoQGhtqVVFcoSNGH6F1Nes0pGSI1u9cr8Elg1XfWK+KARVydzV7s9buWLv3nF9Jqm+s1zu73tkbKiVp2+5te38HSwpLVFJUoqbmpr3Ts4eUDtHuxt1qaGrQ0NKhavImbd8dRuB008ELrVBTh03V6m2r037hcPFRF+ums9peMqcrCIFdw/iIvqSjc9H21e7dIVQk3ru5OSz4c9ppnb80hnsIxmPHtn9eW1NTmBK4dWtYqOaOO1pOSdy5MwSboUNDCBs6NExnnDw5VIlS+1xbG/ZZVNTyuVQ7doT3rKkJFa3Uz/Paa+E9KitDf444ItnHxGJJzc1huuKPfpQMPu4hkH/3u6GPn/hEOP/xkUdCCLn33lCdLC1NXs5g1aqwCFJVVQhyv/lNCICJKZADBoTw+eKL0kc+EqY8794dqsxXXZU8/7G5ObQ9+2w4j/Cii0LQveWW5Pme27eHPl1wQZjq+re/hanQTz8dphnPmBE+6913h6r1QQeFFXQvvzxMud2xI1QKjz8+TLkuKAjHed26sF1RUaiY33dfqB7OmhX+nc48MwQx99DHkSPDsR0+PITeb387BLuLLgr7WrIkVASffDJ8sXDNNaFavnFj+DfYb79Q1b7iirC/X/86VPsGDgyfacuW8Lt26qmh8n3uud3/74MQCAD9UNWOKlWWV6qkqKTDbRubG1VohW2m0DY1N+1tq2+s1/b67Ro0YFCLS88kQnZJUcneSmKiorerYVeojhYUa+eenarZU7O36lffVK+BAwburWRv371dLpcp7K+0qHRvKN1XhMCuYXwE0Ntqa5PVz668prw8+5cFaS0R2NvT1BQqiYkvB/b1y4zq6uQlXrZuDcG5q8epI5nGSK4TCAAxNn5w59eUTpwT2Vrq+Y9lxWUqKy5rs01xYZhuK6lN4Ey9jMyQ0iEaUjqkxesk7Q2MlRV5eJVlAMhz+xJsejoMdVZHAVAKVdXU6vC+BtXUa3wmLg/SWzpZEAcAAAAA9AexCYFmdrqZLTezlWbWw5eIBAAAAID8EIsQaGaFkn4u6QxJh0g638wOyW2vAAAAACB+YhECJR0raaW7v+nueyTNlXR2jvsEAAAAALETlxA4TtLalMdVURsAAAAAoAviEgLTrbnT5toWZjbbzBaa2cLq6upe6BYAAAAAxEtcQmCVpAkpj8dLWtd6I3e/0d1nuPuMykqWIQcAAACA1uISAl+QNM3MppjZAEkzJc3PcZ8AAAAAIHZicbF4d280sy9K+oukQkm3uPuSHHcLAAAAAGInFiFQktz9AUkP5LofAAAAABBn5t5mfZV+wcyqJa3p5tuMlPROD3SnP+GYpMdxaYtj0hbHJL3uHpdJ7s6J4J3UQ+OjxO9zOhyTtjgmbXFM0uO4tNUTxyTtGNlvQ2BPMLOF7j4j1/3oSzgm6XFc2uKYtMUxSY/jEk/8u7XFMWmLY9IWxyQ9jktb2TwmcVkYBgAAAADQAwiBAAAAAJBHCIHtuzHXHeiDOCbpcVza4pi0xTFJj+MST/y7tcUxaYtj0hbHJD2OS1tZOyacEwgAAAAAeYRKIAAAAADkEUJgGmZ2upktN7OVZnZFrvvTm8zsFjPbZGb/SGkbbmaPmNnr0e2wlOeujI7TcjM7LTe9zi4zm2Bmj5rZMjNbYmaXRu15e1zMrNTMnjezl6Nj8u2oPW+PSYKZFZrZS2Z2f/SYY2K22sxeNbPFZrYwasv74xJX+TpGMj62xfiYHmNkZoyRLeV0fHR3flJ+JBVKekPSVEkDJL0s6ZBc96sXP/+Jko6W9I+Uth9KuiK6f4WkH0T3D4mOT4mkKdFxK8z1Z8jCMRkj6ejo/iBJK6LPnrfHRZJJGhjdL5b0nKTj8/mYpBybr0j6raT7o8ccE2m1pJGt2vL+uMTxJ5/HSMbHtMeE8TH9cWGMzHxsGCNbHo+cjY9UAts6VtJKd3/T3fdImivp7Bz3qde4+xOStrRqPlvS7dH92yWdk9I+193r3X2VpJUKx69fcff17v5idL9G0jJJ45THx8WDndHD4ujHlcfHRJLMbLykMyXdlNKc18ekHRyXeMrbMZLxsS3Gx/QYI9NjjOy0XjkmhMC2xklam/K4KmrLZ6Pcfb0U/ocvab+oPe+OlZlNlnSUwrd6eX1coikdiyVtkvSIu+f9MZH0U0n/Jak5pS3fj4kU/vh52MwWmdnsqI3jEk/8+7TE73GE8bElxsi0firGyNZyNj4W7esL+zFL08YSqunl1bEys4GS/iDpMnffYZbu44dN07T1u+Pi7k2SjjSzoZLuNbPD2tm83x8TM/uIpE3uvsjMTurMS9K09atjkuIEd19nZvtJesTMXmtn23w6LnHEv0/n5NVxYnxsizGyJcbIjHI2PlIJbKtK0oSUx+MlrctRX/qKjWY2RpKi201Re94cKzMrVhjg7nL3P0bNeX9cJMndt0l6TNLpyu9jcoKks8xstcIUuZPN7E7l9zGRJLn7uuh2k6R7Faav5P1xiSn+fVrK+99jxsf2MUbuxRiZRi7HR0JgWy9ImmZmU8xsgKSZkubnuE+5Nl/SrOj+LEn3pbTPNLMSM5siaZqk53PQv6yy8JXmzZKWufuPU57K2+NiZpXRt5syszJJH5T0mvL4mLj7le4+3t0nK/x/42/ufoHy+JhIkplVmNmgxH1JH5L0D+X5cYkxxsiW8vr3mPExPcbIthgj28r5+NiTK9z0lx9JH1ZY4eoNSV/PdX96+bPfLWm9pAaFbxwuljRC0gJJr0e3w1O2/3p0nJZLOiPX/c/SMXmfQrn9FUmLo58P5/NxkXS4pJeiY/IPSf8TteftMWl1fE5ScuWzvD4mCqtIvhz9LEn8PzXfj0ucf/J1jGR8THtMGB/THxfGyPaPD2Ok5358tOgNAQAAAAB5gOmgAAAAAJBHCIEAAAAAkEcIgQAAAACQRwiBAAAAAJBHCIEAAAAAkEcIgUAeMLOTzOz+XPcDAIC+hPER+YoQCAAAAAB5hBAI9CFmdoGZPW9mi83sV2ZWaGY7zexaM3vRzBaYWWW07ZFm9qyZvWJm95rZsKj9ADP7q5m9HL1m/+jtB5rZ783sNTO7y8ws2v5qM1savc81OfroAABkxPgI9CxCINBHmNnBkj4p6QR3P1JSk6RPS6qQ9KK7Hy3pcUnfjF5yh6TL3f1wSa+mtN8l6efufoSk90paH7UfJekySYdImirpBDMbLumjkg6N3ud72fyMAAB0FeMj0PMIgUDfcYqkYyS9YGaLo8dTJTVL+l20zZ2S3mdmQyQNdffHo/bbJZ1oZoMkjXP3eyXJ3Xe7+65om+fdvcrdmyUtljRZ0g5JuyXdZGbnSkpsCwBAX8H4CPQwQiDQd5ik2939yOhnurt/K8123sF7ZFKfcr9JUpG7N0o6VtIfJJ0j6aGudRkAgKxjfAR6GCEQ6DsWSPq4me0nSWY23MwmKfx3+vFom09JetLdt0vaambvj9ovlPS4u++QVGVm50TvUWJm5Zl2aGYDJQ1x9wcUpsIc2eOfCgCA7mF8BHpYUa47ACBw96Vm9t+SHjazAkkNki6RVCvpUDNbJGm7wnkRkjRL0i+jQexNSZ+J2i+U9Csz+070Hue1s9tBku4zs1KFb0n/Xw9/LAAAuoXxEeh55t5e5RxArpnZTncfmOt+AADQlzA+AvuO6aAAAAAAkEeoBAIAAABAHqESCAAAAAB5hBAIAAAAAHmEEAgAAAAAeYQQCAAAAAB5hBAIAAAAAHmEEAgAAAAAeeT/A7gH4bes542PAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 1080x360 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#breakfast hour\n",
    "# plot train and test loss (mse)\n",
    "fig, ax = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "ax[0].plot(a_history_nn.history['loss'], color = 'green')\n",
    "ax[1].plot(a_history_nn.history['val_loss'], color = 'blue')\n",
    "\n",
    "ax[0].set_title('Train')\n",
    "ax[0].set_xlabel('epochs')\n",
    "ax[0].set_ylabel('Loss = MSE')\n",
    "\n",
    "ax[1].set_title('Test')\n",
    "ax[1].set_xlabel('epochs')\n",
    "ax[1].set_ylabel('Loss = MSE');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f7a59700-0e54-4489-b2d9-6ac968862dab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 0s 1ms/step\n",
      "Test R2: 0.9330577294184806\n",
      "Mean Absolute Error: 24.59176254272461\n",
      "Huber Loss: 24.096338272094727\n"
     ]
    }
   ],
   "source": [
    "# https://medium.com/towards-data-science/loss-functions-and-their-use-in-neural-networks-a470e703f1e9\n",
    "a_nn_y_true = a_y_nn_test\n",
    "a_nn_y_pred = a_model_nn.predict(a_X_nn_test_sc)\n",
    "\n",
    "# R2\n",
    "print(f'Test R2: {r2_score(a_nn_y_true, a_nn_y_pred)}')\n",
    "\n",
    "# MAE\n",
    "mae = MeanAbsoluteError()\n",
    "print(f'Mean Absolute Error: {mae(a_nn_y_true, a_nn_y_pred)}')\n",
    "\n",
    "# Huber\n",
    "huber = Huber()\n",
    "print(f'Huber Loss: {huber(a_nn_y_true, a_nn_y_pred)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d0f289d-1569-471e-a074-5c8321fcb722",
   "metadata": {},
   "source": [
    "# Average Cycling Performance Best Model - XGBoost\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8b1eb6d-d573-44ac-a1d7-565d143fe1d1",
   "metadata": {},
   "source": [
    "## Pickle That"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a227fa4d-a3fb-4bbe-8fdb-4839b2e441de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ram://7f42dd7e-108d-445f-87db-92324e8039d5/assets\n"
     ]
    }
   ],
   "source": [
    "# Pickle a_NN\n",
    "with open('../models/a_model_nn.pkl', 'wb') as f:\n",
    "    pickle.dump(a_model_nn, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c8211e8a-a0f1-4c9e-a52e-e92a534c9b69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pickle a_XGB\n",
    "with open('../models/a_model_xgb.pkl', 'wb') as f:\n",
    "    pickle.dump(a_xgb_gs, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a9cd977-53bf-42e7-965a-e201deef56bc",
   "metadata": {},
   "source": [
    "# High Cycling Performance Modeling\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a24e074-bd9d-45a1-92fe-2535d9ebfb43",
   "metadata": {
    "tags": []
   },
   "source": [
    "## X, y, train_test_split\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f7921df7-2cf4-42ba-b4b1-6a0f230578be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['timestamp', 'latitude', 'longitude', 'elevation', 'dt', 'heart_rate',\n",
       "       'bearing', 'time_diff_s', 'total_time_s', 'ele_diff_m',\n",
       "       'total_ele_change_m', 'lat_lon', 'dist_diff_km', 'total_dist_km',\n",
       "       'temp', 'feels_like', 'pressure', 'humidity', 'dew_point', 'clouds',\n",
       "       'wind_speed', 'wind_deg'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "969387f3-9c8e-4c11-8be3-5222b2df2a2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "h_features = ['elevation', 'bearing', 'time_diff_s', 'total_time_s', 'ele_diff_m',\n",
    "       'total_ele_change_m', 'dist_diff_km', 'total_dist_km',\n",
    "       'temp', 'feels_like', 'pressure', 'humidity', 'dew_point', 'clouds',\n",
    "       'wind_speed', 'wind_deg']\n",
    "h_X = h_df[h_features]\n",
    "h_y = h_df['heart_rate']\n",
    "\n",
    "h_X_train, h_X_test, h_y_train, h_y_test = train_test_split(h_X, h_y, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "cc8982b8-f0bf-41e6-9a9f-dc8948dfff99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "130.53650259067356"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h_y.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e872296-5e47-4fb1-8f37-5672dea404e4",
   "metadata": {},
   "source": [
    "### StandardScaler X_train and X_test for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "253e8153-5c95-4066-b6be-eabe265fd2a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "h_ss = StandardScaler()\n",
    "h_X_train_sc = h_ss.fit_transform(h_X_train)\n",
    "h_X_test_sc = h_ss.transform(h_X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d829a19f-5c0c-4853-b565-34f35c44ae00",
   "metadata": {},
   "source": [
    "### Polynomial X_train and X_test for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e3047e1a-7a7e-47eb-a725-6c7d27c4227d",
   "metadata": {},
   "outputs": [],
   "source": [
    "h_poly = PolynomialFeatures()\n",
    "h_X_train_sc_p = h_poly.fit_transform(h_X_train_sc)\n",
    "h_X_test_sc_p = h_poly.fit_transform(h_X_test_sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "351263ad-bdf0-40fd-9333-5f2f8af3dd58",
   "metadata": {},
   "source": [
    "## Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "0e3968af-6d12-49ba-80ac-778c655ccf20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GridSearch Best Score: 0.8839302929148015\n",
      "{'learning_rate': 0.8, 'max_depth': 4, 'n_estimators': 150}\n",
      "GBoost Train R2 Score: 0.9313990799272799\n",
      "GBoost Test R2 Score: 0.8849043647905527\n",
      "Mean Absolute Error: 4.056626449058407\n",
      "Mean Squared Error: 30.153280288091132\n"
     ]
    }
   ],
   "source": [
    "# Instantiate\n",
    "h_gboost = GradientBoostingRegressor()\n",
    "\n",
    "h_gboost_params = {\n",
    "    'max_depth': [2, 3, 4],\n",
    "    'n_estimators': [100, 125, 150],\n",
    "    'learning_rate': [0.8, .1, .12]\n",
    "}\n",
    "\n",
    "# Gridsearch\n",
    "h_gb_gs = GridSearchCV(h_gboost, param_grid = h_gboost_params, cv = 5)\n",
    "h_gb_gs.fit(h_X_train_sc, h_y_train)\n",
    "\n",
    "# Metrics\n",
    "print(f'GridSearch Best Score: {h_gb_gs.best_score_}')\n",
    "print(h_gb_gs.best_params_)\n",
    "print(f'GBoost Train R2 Score: {h_gb_gs.score(h_X_train_sc, h_y_train)}')\n",
    "print(f'GBoost Test R2 Score: {h_gb_gs.score(h_X_test_sc, h_y_test)}')\n",
    "\n",
    "h_gb_y_true = h_y_test\n",
    "h_gb_y_pred = h_gb_gs.predict(h_X_test_sc)\n",
    "\n",
    "# MAE\n",
    "print(f'Mean Absolute Error: {mean_absolute_error(h_gb_y_true, h_gb_y_pred)}')\n",
    "\n",
    "# MSE\n",
    "print(f'Mean Squared Error: {mean_squared_error(h_gb_y_true, h_gb_y_pred)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38228abd-c78d-492d-bf62-e810d1449682",
   "metadata": {},
   "source": [
    "## XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "361f95ab-bfda-4cb9-8ba3-2a28aff106c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GridSearch Best Score: 0.8861220006738151\n",
      "{'learning_rate': 0.8, 'max_depth': 4, 'n_estimators': 150}\n",
      "XGBoost Train R2 Score: 0.9303844407794039\n",
      "XGBoost Test R2 Score: 0.8895029087565683\n",
      "Mean Absolute Error: 3.9739194626882286\n",
      "Mean Squared Error: 28.948532732964043\n"
     ]
    }
   ],
   "source": [
    "# Instantiate\n",
    "h_xgboost = XGBRegressor()\n",
    "\n",
    "# Gridsearch\n",
    "h_xgboost_params = {\n",
    "    'max_depth': [2, 3, 4],\n",
    "    'n_estimators': [100, 125, 150],\n",
    "    'learning_rate': [0.8, .1, .12]\n",
    "}\n",
    "\n",
    "h_xgb_gs = GridSearchCV(h_xgboost, param_grid = h_xgboost_params, cv = 5)\n",
    "h_xgb_gs.fit(h_X_train_sc, h_y_train)\n",
    "\n",
    "# Metrics\n",
    "print(f'GridSearch Best Score: {h_xgb_gs.best_score_}')\n",
    "print(h_xgb_gs.best_params_)\n",
    "print(f'XGBoost Train R2 Score: {h_xgb_gs.score(h_X_train_sc, h_y_train)}')\n",
    "print(f'XGBoost Test R2 Score: {h_xgb_gs.score(h_X_test_sc, h_y_test)}')\n",
    "\n",
    "h_xgb_y_true = h_y_test\n",
    "h_xgb_y_pred = h_xgb_gs.predict(h_X_test_sc)\n",
    "\n",
    "# MAE\n",
    "print(f'Mean Absolute Error: {mean_absolute_error(h_xgb_y_true, h_xgb_y_pred)}')\n",
    "\n",
    "# MSE\n",
    "print(f'Mean Squared Error: {mean_squared_error(h_xgb_y_true, h_xgb_y_pred)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42951d10-b39e-443f-a36a-e766a664d496",
   "metadata": {},
   "source": [
    "## Neural Net Regressor\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c11b9530-ac08-4da0-a6cf-5c75fc7eab57",
   "metadata": {},
   "source": [
    "### X, y, train_test_split, StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "546830b3-b4ef-40e7-9a72-ed98f8a51ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "h_X_nn = h_df[h_features]\n",
    "h_y_nn = h_df['heart_rate']\n",
    "\n",
    "h_X_nn = np.array(h_X_nn)\n",
    "h_y_nn = np.array(h_y_nn)\n",
    "\n",
    "h_X_nn_train, h_X_nn_test, h_y_nn_train, h_y_nn_test = train_test_split(h_X_nn, h_y_nn, random_state = 42)\n",
    "\n",
    "h_ss_nn = StandardScaler()\n",
    "h_X_nn_train_sc = h_ss_nn.fit_transform(h_X_nn_train)\n",
    "h_X_nn_test_sc = h_ss.transform(h_X_nn_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "c5fe911f-9448-4b16-bc94-270ee4531a05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16,)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h_X_nn[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "ecf6188a-a85f-403f-8c24-523eed885157",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate\n",
    "h_model_nn = Sequential()\n",
    "\n",
    "# Layers\n",
    "h_model_nn.add(Dense(128, input_dim = 16, activation = 'relu'))\n",
    "\n",
    "h_model_nn.add(BatchNormalization())\n",
    "h_model_nn.add(Dense(64, activation = 'relu', kernel_regularizer = l2(.1)))\n",
    "h_model_nn.add(Dense(128, activation = 'relu', kernel_regularizer = l2(.5)))\n",
    "h_model_nn.add(Dense(64, activation = 'relu', kernel_regularizer = l2(.1)))\n",
    "h_model_nn.add(Dense(128, activation = 'relu', kernel_regularizer = l2(.5))) \n",
    "h_model_nn.add(Dense(64, activation = 'relu', kernel_regularizer = l2(.1))) \n",
    "h_model_nn.add(Dense(1, kernel_regularizer = l2(.5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "c56569da-5f33-4e0a-9262-29899d45bbea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile\n",
    "h_model_nn.compile(optimizer = 'adam', loss = 'mse', metrics = ['mse'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d926489-6913-4c2b-9715-6d6c1f8f69f5",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "905/905 [==============================] - 3s 2ms/step - loss: 725.4988 - mse: 642.7099 - val_loss: 286.9963 - val_mse: 218.5618\n",
      "Epoch 2/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 230.9302 - mse: 170.0964 - val_loss: 252.4674 - val_mse: 198.1227\n",
      "Epoch 3/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 198.2631 - mse: 148.9034 - val_loss: 229.1277 - val_mse: 184.1654\n",
      "Epoch 4/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 183.0060 - mse: 141.6569 - val_loss: 221.9557 - val_mse: 183.9065\n",
      "Epoch 5/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 169.8376 - mse: 134.4731 - val_loss: 226.6934 - val_mse: 193.7600\n",
      "Epoch 6/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 159.5181 - mse: 128.7633 - val_loss: 199.6174 - val_mse: 170.9244\n",
      "Epoch 7/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 151.3692 - mse: 124.3090 - val_loss: 198.2903 - val_mse: 172.7566\n",
      "Epoch 8/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 146.4748 - mse: 122.3049 - val_loss: 167.8888 - val_mse: 144.9099\n",
      "Epoch 9/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 143.7682 - mse: 121.8946 - val_loss: 193.3752 - val_mse: 172.3720\n",
      "Epoch 10/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 137.4588 - mse: 117.4107 - val_loss: 182.1469 - val_mse: 162.9440\n",
      "Epoch 11/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 133.9291 - mse: 115.3802 - val_loss: 188.0414 - val_mse: 170.1105\n",
      "Epoch 12/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 128.4276 - mse: 111.1112 - val_loss: 170.3754 - val_mse: 153.6397\n",
      "Epoch 13/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 130.0434 - mse: 113.7158 - val_loss: 158.8999 - val_mse: 143.0452\n",
      "Epoch 14/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 124.8950 - mse: 109.3984 - val_loss: 191.1404 - val_mse: 175.9201\n",
      "Epoch 15/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 121.7714 - mse: 106.9473 - val_loss: 151.8890 - val_mse: 137.3425\n",
      "Epoch 16/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 120.2179 - mse: 105.9796 - val_loss: 145.9696 - val_mse: 131.9743\n",
      "Epoch 17/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 119.0592 - mse: 105.3309 - val_loss: 134.0208 - val_mse: 120.5460\n",
      "Epoch 18/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 117.0951 - mse: 103.8136 - val_loss: 137.5863 - val_mse: 124.4621\n",
      "Epoch 19/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 114.8818 - mse: 101.9711 - val_loss: 159.2494 - val_mse: 146.4545\n",
      "Epoch 20/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 113.1860 - mse: 100.6187 - val_loss: 137.1050 - val_mse: 124.7455\n",
      "Epoch 21/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 111.6157 - mse: 99.3490 - val_loss: 163.4136 - val_mse: 151.1612\n",
      "Epoch 22/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 113.0453 - mse: 101.0407 - val_loss: 128.9428 - val_mse: 117.0823\n",
      "Epoch 23/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 109.2471 - mse: 97.5121 - val_loss: 124.5502 - val_mse: 112.9725\n",
      "Epoch 24/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 106.8744 - mse: 95.3443 - val_loss: 127.1153 - val_mse: 115.6511\n",
      "Epoch 25/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 107.7356 - mse: 96.4170 - val_loss: 140.1787 - val_mse: 128.8907\n",
      "Epoch 26/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 104.5741 - mse: 93.4116 - val_loss: 125.9983 - val_mse: 114.8513\n",
      "Epoch 27/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 103.5471 - mse: 92.5197 - val_loss: 143.5803 - val_mse: 132.6736\n",
      "Epoch 28/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 105.0443 - mse: 94.1852 - val_loss: 116.4085 - val_mse: 105.5822\n",
      "Epoch 29/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 104.7829 - mse: 94.0294 - val_loss: 127.8634 - val_mse: 117.1653\n",
      "Epoch 30/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 102.2174 - mse: 91.5301 - val_loss: 136.4140 - val_mse: 125.7794\n",
      "Epoch 31/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 103.8739 - mse: 93.2628 - val_loss: 128.0851 - val_mse: 117.6021\n",
      "Epoch 32/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 101.5994 - mse: 91.0551 - val_loss: 125.9048 - val_mse: 115.3506\n",
      "Epoch 33/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 101.5350 - mse: 91.0656 - val_loss: 115.4651 - val_mse: 105.0742\n",
      "Epoch 34/500\n",
      "905/905 [==============================] - 2s 3ms/step - loss: 99.1456 - mse: 88.7306 - val_loss: 111.8129 - val_mse: 101.4491\n",
      "Epoch 35/500\n",
      "905/905 [==============================] - 2s 3ms/step - loss: 99.1744 - mse: 88.8413 - val_loss: 140.7067 - val_mse: 130.3156\n",
      "Epoch 36/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 99.0976 - mse: 88.8067 - val_loss: 107.0129 - val_mse: 96.7507\n",
      "Epoch 37/500\n",
      "905/905 [==============================] - 2s 3ms/step - loss: 98.9679 - mse: 88.7225 - val_loss: 124.7311 - val_mse: 114.4342\n",
      "Epoch 38/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 97.8145 - mse: 87.5962 - val_loss: 103.2603 - val_mse: 93.0486\n",
      "Epoch 39/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 98.9089 - mse: 88.6814 - val_loss: 111.1718 - val_mse: 100.9089\n",
      "Epoch 40/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 96.6879 - mse: 86.4239 - val_loss: 112.8369 - val_mse: 102.6291\n",
      "Epoch 41/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 96.5265 - mse: 86.3089 - val_loss: 113.2350 - val_mse: 103.0473\n",
      "Epoch 42/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 95.9674 - mse: 85.7776 - val_loss: 101.8682 - val_mse: 91.6405\n",
      "Epoch 43/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 95.7522 - mse: 85.5703 - val_loss: 104.1665 - val_mse: 93.9423\n",
      "Epoch 44/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 96.9763 - mse: 86.7939 - val_loss: 115.2833 - val_mse: 105.1496\n",
      "Epoch 45/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 95.3476 - mse: 85.1608 - val_loss: 97.3860 - val_mse: 87.1735\n",
      "Epoch 46/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 94.5023 - mse: 84.3370 - val_loss: 104.5478 - val_mse: 94.3855\n",
      "Epoch 47/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 94.0990 - mse: 83.9173 - val_loss: 96.9936 - val_mse: 86.7871\n",
      "Epoch 48/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 95.1137 - mse: 84.9294 - val_loss: 90.6632 - val_mse: 80.4659\n",
      "Epoch 49/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 94.5079 - mse: 84.3145 - val_loss: 95.7421 - val_mse: 85.5594\n",
      "Epoch 50/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 92.9421 - mse: 82.7671 - val_loss: 98.5194 - val_mse: 88.2858\n",
      "Epoch 51/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 94.6062 - mse: 84.4098 - val_loss: 99.9398 - val_mse: 89.7318\n",
      "Epoch 52/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 93.0517 - mse: 82.7975 - val_loss: 91.8464 - val_mse: 81.5981\n",
      "Epoch 53/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 92.9346 - mse: 82.6725 - val_loss: 100.0730 - val_mse: 89.7930\n",
      "Epoch 54/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 91.6834 - mse: 81.3857 - val_loss: 97.0819 - val_mse: 86.8060\n",
      "Epoch 55/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 91.3100 - mse: 81.0046 - val_loss: 110.1505 - val_mse: 99.8906\n",
      "Epoch 56/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 91.1412 - mse: 80.8019 - val_loss: 96.3120 - val_mse: 85.9045\n",
      "Epoch 57/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 91.2090 - mse: 80.8124 - val_loss: 98.0707 - val_mse: 87.6064\n",
      "Epoch 58/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 90.0014 - mse: 79.6021 - val_loss: 92.5228 - val_mse: 82.1045\n",
      "Epoch 59/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 89.5527 - mse: 79.1135 - val_loss: 93.8137 - val_mse: 83.3393\n",
      "Epoch 60/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 89.9695 - mse: 79.5098 - val_loss: 88.5470 - val_mse: 78.0441\n",
      "Epoch 61/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 89.5543 - mse: 79.0627 - val_loss: 91.1644 - val_mse: 80.6570\n",
      "Epoch 62/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 88.7087 - mse: 78.1857 - val_loss: 94.7440 - val_mse: 84.1316\n",
      "Epoch 63/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 89.1493 - mse: 78.5859 - val_loss: 89.5271 - val_mse: 78.9694\n",
      "Epoch 64/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 88.2884 - mse: 77.7032 - val_loss: 87.3165 - val_mse: 76.7438\n",
      "Epoch 65/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 88.4056 - mse: 77.8020 - val_loss: 91.3361 - val_mse: 80.6587\n",
      "Epoch 66/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 87.7594 - mse: 77.0797 - val_loss: 87.5379 - val_mse: 76.8180\n",
      "Epoch 67/500\n",
      "905/905 [==============================] - 2s 3ms/step - loss: 87.5580 - mse: 76.8235 - val_loss: 92.4706 - val_mse: 81.7235\n",
      "Epoch 68/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 87.5582 - mse: 76.7792 - val_loss: 88.7693 - val_mse: 77.8918\n",
      "Epoch 69/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 86.5915 - mse: 75.7085 - val_loss: 87.2380 - val_mse: 76.3507\n",
      "Epoch 70/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 86.4744 - mse: 75.5678 - val_loss: 89.0756 - val_mse: 78.1720\n",
      "Epoch 71/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 87.1603 - mse: 76.2320 - val_loss: 91.5779 - val_mse: 80.6603\n",
      "Epoch 72/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 85.5519 - mse: 74.5819 - val_loss: 86.2920 - val_mse: 75.2748\n",
      "Epoch 73/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 85.4261 - mse: 74.4306 - val_loss: 91.5194 - val_mse: 80.4822\n",
      "Epoch 74/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 85.8286 - mse: 74.8332 - val_loss: 89.8525 - val_mse: 78.8849\n",
      "Epoch 75/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 84.8275 - mse: 73.7964 - val_loss: 88.4992 - val_mse: 77.4622\n",
      "Epoch 76/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 86.1192 - mse: 75.0830 - val_loss: 90.3657 - val_mse: 79.3349\n",
      "Epoch 77/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 85.4452 - mse: 74.3654 - val_loss: 92.4552 - val_mse: 81.3201\n",
      "Epoch 78/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 85.5207 - mse: 74.4515 - val_loss: 89.3817 - val_mse: 78.3159\n",
      "Epoch 79/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 85.5642 - mse: 74.4809 - val_loss: 86.6270 - val_mse: 75.5711\n",
      "Epoch 80/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 85.1075 - mse: 74.0395 - val_loss: 84.1149 - val_mse: 73.0364\n",
      "Epoch 81/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 83.8094 - mse: 72.7192 - val_loss: 85.8214 - val_mse: 74.7189\n",
      "Epoch 82/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 83.9387 - mse: 72.8287 - val_loss: 101.8921 - val_mse: 90.8178\n",
      "Epoch 83/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 84.9458 - mse: 73.7714 - val_loss: 85.6190 - val_mse: 74.4643\n",
      "Epoch 84/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 83.5154 - mse: 72.3212 - val_loss: 91.7614 - val_mse: 80.4626\n",
      "Epoch 85/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 83.2974 - mse: 72.0783 - val_loss: 87.5684 - val_mse: 76.3741\n",
      "Epoch 86/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 83.8505 - mse: 72.6085 - val_loss: 90.1833 - val_mse: 78.9472\n",
      "Epoch 87/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 84.5481 - mse: 73.2211 - val_loss: 84.5183 - val_mse: 73.1729\n",
      "Epoch 88/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 82.9187 - mse: 71.5328 - val_loss: 89.5095 - val_mse: 78.1864\n",
      "Epoch 89/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 84.2862 - mse: 72.8612 - val_loss: 86.5751 - val_mse: 75.1969\n",
      "Epoch 90/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 83.3539 - mse: 71.9298 - val_loss: 84.6503 - val_mse: 73.2653\n",
      "Epoch 91/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 82.7920 - mse: 71.3587 - val_loss: 85.4456 - val_mse: 73.9667\n",
      "Epoch 92/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 83.1788 - mse: 71.6846 - val_loss: 85.2290 - val_mse: 73.6828\n",
      "Epoch 93/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 82.4209 - mse: 70.9107 - val_loss: 85.2879 - val_mse: 73.8218\n",
      "Epoch 94/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 83.3273 - mse: 71.7983 - val_loss: 84.7568 - val_mse: 73.2240\n",
      "Epoch 95/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 83.2123 - mse: 71.6691 - val_loss: 82.2878 - val_mse: 70.7695\n",
      "Epoch 96/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 82.2326 - mse: 70.6724 - val_loss: 88.1215 - val_mse: 76.4771\n",
      "Epoch 97/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 81.7105 - mse: 70.1578 - val_loss: 84.4892 - val_mse: 72.9661\n",
      "Epoch 98/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 81.8116 - mse: 70.2446 - val_loss: 83.9413 - val_mse: 72.3315\n",
      "Epoch 99/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 81.8410 - mse: 70.2460 - val_loss: 81.3916 - val_mse: 69.7780\n",
      "Epoch 100/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 81.3994 - mse: 69.7622 - val_loss: 86.0106 - val_mse: 74.3916\n",
      "Epoch 101/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 82.3174 - mse: 70.6662 - val_loss: 87.3616 - val_mse: 75.7311\n",
      "Epoch 102/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 81.4934 - mse: 69.8114 - val_loss: 84.6716 - val_mse: 72.8932\n",
      "Epoch 103/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 81.5115 - mse: 69.8290 - val_loss: 87.3735 - val_mse: 75.6848\n",
      "Epoch 104/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 81.7483 - mse: 70.0510 - val_loss: 81.6470 - val_mse: 69.9240\n",
      "Epoch 105/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 82.3423 - mse: 70.6160 - val_loss: 79.8976 - val_mse: 68.1184\n",
      "Epoch 106/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 80.3186 - mse: 68.5562 - val_loss: 80.4664 - val_mse: 68.7818\n",
      "Epoch 107/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 81.4914 - mse: 69.7579 - val_loss: 83.5890 - val_mse: 71.8412\n",
      "Epoch 108/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 82.0665 - mse: 70.2800 - val_loss: 94.8869 - val_mse: 83.1509\n",
      "Epoch 109/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 80.8741 - mse: 69.0328 - val_loss: 82.3530 - val_mse: 70.4783\n",
      "Epoch 110/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 80.4712 - mse: 68.6229 - val_loss: 79.1095 - val_mse: 67.2505\n",
      "Epoch 111/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 81.1794 - mse: 69.3088 - val_loss: 82.4372 - val_mse: 70.5860\n",
      "Epoch 112/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 80.5667 - mse: 68.7178 - val_loss: 79.3489 - val_mse: 67.4975\n",
      "Epoch 113/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 80.4485 - mse: 68.6093 - val_loss: 86.2277 - val_mse: 74.4083\n",
      "Epoch 114/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 80.5016 - mse: 68.6541 - val_loss: 81.0723 - val_mse: 69.2049\n",
      "Epoch 115/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 79.4334 - mse: 67.6017 - val_loss: 78.8357 - val_mse: 67.0539\n",
      "Epoch 116/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 79.9568 - mse: 68.1528 - val_loss: 78.6657 - val_mse: 66.8233\n",
      "Epoch 117/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 80.0898 - mse: 68.2440 - val_loss: 85.1244 - val_mse: 73.3263\n",
      "Epoch 118/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 79.4852 - mse: 67.6277 - val_loss: 80.6984 - val_mse: 68.8163\n",
      "Epoch 119/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 79.8874 - mse: 68.0073 - val_loss: 83.9730 - val_mse: 72.1370\n",
      "Epoch 120/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 79.0272 - mse: 67.1199 - val_loss: 80.6943 - val_mse: 68.7943\n",
      "Epoch 121/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 79.0050 - mse: 67.0924 - val_loss: 84.6328 - val_mse: 72.6955\n",
      "Epoch 122/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 79.0287 - mse: 67.0618 - val_loss: 78.0757 - val_mse: 66.0795\n",
      "Epoch 123/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 79.0839 - mse: 67.1238 - val_loss: 81.1882 - val_mse: 69.2167\n",
      "Epoch 124/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 80.0118 - mse: 68.0823 - val_loss: 83.0687 - val_mse: 71.1247\n",
      "Epoch 125/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 80.0625 - mse: 68.0939 - val_loss: 82.4039 - val_mse: 70.4465\n",
      "Epoch 126/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 78.3453 - mse: 66.3769 - val_loss: 79.5141 - val_mse: 67.5283\n",
      "Epoch 127/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 79.2154 - mse: 67.2198 - val_loss: 76.8860 - val_mse: 64.9061\n",
      "Epoch 128/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 78.5107 - mse: 66.5296 - val_loss: 79.2707 - val_mse: 67.3070\n",
      "Epoch 129/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 79.4950 - mse: 67.5004 - val_loss: 83.8116 - val_mse: 71.8835\n",
      "Epoch 130/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 78.2790 - mse: 66.2943 - val_loss: 75.9415 - val_mse: 63.9353\n",
      "Epoch 131/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 77.8189 - mse: 65.7919 - val_loss: 81.8201 - val_mse: 69.7852\n",
      "Epoch 132/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 77.3842 - mse: 65.3597 - val_loss: 76.4731 - val_mse: 64.4223\n",
      "Epoch 133/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 77.5756 - mse: 65.5630 - val_loss: 80.9435 - val_mse: 68.9748\n",
      "Epoch 134/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 78.1834 - mse: 66.1924 - val_loss: 88.6582 - val_mse: 76.6152\n",
      "Epoch 135/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 78.4466 - mse: 66.4785 - val_loss: 79.3460 - val_mse: 67.3596\n",
      "Epoch 136/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 77.3788 - mse: 65.3929 - val_loss: 82.3073 - val_mse: 70.2875\n",
      "Epoch 137/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 78.1538 - mse: 66.1393 - val_loss: 84.6622 - val_mse: 72.6279\n",
      "Epoch 138/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 77.6922 - mse: 65.7123 - val_loss: 77.7801 - val_mse: 65.8217\n",
      "Epoch 139/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 77.0676 - mse: 65.0898 - val_loss: 78.9225 - val_mse: 66.9289\n",
      "Epoch 140/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 76.8031 - mse: 64.7888 - val_loss: 84.5526 - val_mse: 72.5891\n",
      "Epoch 141/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 77.4540 - mse: 65.4817 - val_loss: 80.9149 - val_mse: 68.9867\n",
      "Epoch 142/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 77.1093 - mse: 65.1302 - val_loss: 81.3814 - val_mse: 69.3521\n",
      "Epoch 143/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 76.8550 - mse: 64.8576 - val_loss: 80.4311 - val_mse: 68.4770\n",
      "Epoch 144/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 76.8571 - mse: 64.8608 - val_loss: 81.4098 - val_mse: 69.3723\n",
      "Epoch 145/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 77.3200 - mse: 65.2756 - val_loss: 76.4654 - val_mse: 64.4608\n",
      "Epoch 146/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 77.2147 - mse: 65.1618 - val_loss: 86.8552 - val_mse: 74.7585\n",
      "Epoch 147/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 76.4333 - mse: 64.3805 - val_loss: 75.7035 - val_mse: 63.6185\n",
      "Epoch 148/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 76.9480 - mse: 64.9012 - val_loss: 77.8376 - val_mse: 65.8295\n",
      "Epoch 149/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 76.9181 - mse: 64.8761 - val_loss: 87.3866 - val_mse: 75.3963\n",
      "Epoch 150/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 75.8412 - mse: 63.7999 - val_loss: 75.8039 - val_mse: 63.7865\n",
      "Epoch 151/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 76.8155 - mse: 64.7978 - val_loss: 79.7215 - val_mse: 67.7575\n",
      "Epoch 152/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 76.6082 - mse: 64.5514 - val_loss: 81.6152 - val_mse: 69.5717\n",
      "Epoch 153/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 76.6341 - mse: 64.5622 - val_loss: 78.2077 - val_mse: 66.1653\n",
      "Epoch 154/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 75.2892 - mse: 63.1758 - val_loss: 77.5699 - val_mse: 65.4731\n",
      "Epoch 155/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 76.5926 - mse: 64.4899 - val_loss: 75.5875 - val_mse: 63.4506\n",
      "Epoch 156/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 76.1703 - mse: 64.0610 - val_loss: 80.4036 - val_mse: 68.3460\n",
      "Epoch 157/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 75.5012 - mse: 63.3861 - val_loss: 78.4061 - val_mse: 66.3454\n",
      "Epoch 158/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 76.0674 - mse: 63.9771 - val_loss: 77.5544 - val_mse: 65.4249\n",
      "Epoch 159/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 75.4620 - mse: 63.3694 - val_loss: 76.7435 - val_mse: 64.6222\n",
      "Epoch 160/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 75.8863 - mse: 63.8065 - val_loss: 78.1759 - val_mse: 66.1103\n",
      "Epoch 161/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 76.5568 - mse: 64.4754 - val_loss: 75.5358 - val_mse: 63.5180\n",
      "Epoch 162/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 75.6305 - mse: 63.5924 - val_loss: 75.6225 - val_mse: 63.5675\n",
      "Epoch 163/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 75.9789 - mse: 63.9261 - val_loss: 75.9454 - val_mse: 63.8838\n",
      "Epoch 164/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 76.1281 - mse: 64.0442 - val_loss: 80.3672 - val_mse: 68.3688\n",
      "Epoch 165/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 76.0280 - mse: 63.9689 - val_loss: 84.7156 - val_mse: 72.5466\n",
      "Epoch 166/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 74.6720 - mse: 62.5717 - val_loss: 77.8691 - val_mse: 65.8052\n",
      "Epoch 167/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 75.5182 - mse: 63.3880 - val_loss: 77.9136 - val_mse: 65.7952\n",
      "Epoch 168/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 75.8950 - mse: 63.7445 - val_loss: 74.7644 - val_mse: 62.6573\n",
      "Epoch 169/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 74.1067 - mse: 61.9663 - val_loss: 81.1467 - val_mse: 69.0553\n",
      "Epoch 170/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 74.9610 - mse: 62.8546 - val_loss: 75.5022 - val_mse: 63.3904\n",
      "Epoch 171/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 76.1565 - mse: 64.0501 - val_loss: 73.4716 - val_mse: 61.3625\n",
      "Epoch 172/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 74.6632 - mse: 62.5359 - val_loss: 77.0255 - val_mse: 64.8339\n",
      "Epoch 173/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 74.7812 - mse: 62.6446 - val_loss: 77.7937 - val_mse: 65.6671\n",
      "Epoch 174/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 74.6456 - mse: 62.4865 - val_loss: 76.7485 - val_mse: 64.5989\n",
      "Epoch 175/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 74.8978 - mse: 62.7312 - val_loss: 79.0187 - val_mse: 66.8297\n",
      "Epoch 176/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 74.4672 - mse: 62.2972 - val_loss: 83.4044 - val_mse: 71.2096\n",
      "Epoch 177/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 74.3683 - mse: 62.2044 - val_loss: 80.2219 - val_mse: 68.0541\n",
      "Epoch 178/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 74.5111 - mse: 62.3501 - val_loss: 79.1069 - val_mse: 66.9403\n",
      "Epoch 179/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 74.9678 - mse: 62.8455 - val_loss: 76.3725 - val_mse: 64.2249\n",
      "Epoch 180/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 74.2615 - mse: 62.1005 - val_loss: 78.4328 - val_mse: 66.3088\n",
      "Epoch 181/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 73.7512 - mse: 61.6108 - val_loss: 76.1893 - val_mse: 64.0381\n",
      "Epoch 182/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 74.4301 - mse: 62.2877 - val_loss: 76.9090 - val_mse: 64.8061\n",
      "Epoch 183/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 74.9658 - mse: 62.8725 - val_loss: 72.8211 - val_mse: 60.7163\n",
      "Epoch 184/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 73.8424 - mse: 61.7475 - val_loss: 76.9633 - val_mse: 64.8907\n",
      "Epoch 185/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 74.9000 - mse: 62.8196 - val_loss: 78.3372 - val_mse: 66.3019\n",
      "Epoch 186/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 74.0124 - mse: 61.9281 - val_loss: 82.1435 - val_mse: 70.0971\n",
      "Epoch 187/500\n",
      "905/905 [==============================] - 2s 3ms/step - loss: 73.8966 - mse: 61.7981 - val_loss: 75.8148 - val_mse: 63.6836\n",
      "Epoch 188/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 74.3630 - mse: 62.2786 - val_loss: 74.4302 - val_mse: 62.3462\n",
      "Epoch 189/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 73.5044 - mse: 61.4295 - val_loss: 77.6157 - val_mse: 65.5391\n",
      "Epoch 190/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 74.0386 - mse: 61.9204 - val_loss: 78.0133 - val_mse: 65.9020\n",
      "Epoch 191/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 73.0179 - mse: 60.8704 - val_loss: 78.1158 - val_mse: 65.9426\n",
      "Epoch 192/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 73.6591 - mse: 61.4953 - val_loss: 79.9572 - val_mse: 67.7647\n",
      "Epoch 193/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 73.8900 - mse: 61.7357 - val_loss: 74.9265 - val_mse: 62.7949\n",
      "Epoch 194/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 73.6757 - mse: 61.5528 - val_loss: 79.1672 - val_mse: 67.0559\n",
      "Epoch 195/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 73.5169 - mse: 61.3937 - val_loss: 71.4513 - val_mse: 59.3666\n",
      "Epoch 196/500\n",
      "905/905 [==============================] - 2s 2ms/step - loss: 73.0652 - mse: 60.9648 - val_loss: 74.6841 - val_mse: 62.5928\n",
      "Epoch 197/500\n",
      "715/905 [======================>.......] - ETA: 0s - loss: 73.5526 - mse: 61.4568"
     ]
    }
   ],
   "source": [
    "# fit\n",
    "h_history_nn = h_model_nn.fit(h_X_nn_train_sc, h_y_nn_train, epochs = 500, verbose = 1, \n",
    "                          validation_data = (h_X_nn_test_sc, h_y_nn_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "034ebf42-97f6-4c45-8afe-b6aa38a878a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#breakfast hour\n",
    "# plot train and test loss (mse)\n",
    "fig, ax = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "ax[0].plot(h_history_nn.history['loss'], color = 'green')\n",
    "ax[1].plot(h_history_nn.history['val_loss'], color = 'blue')\n",
    "\n",
    "ax[0].set_title('Train')\n",
    "ax[0].set_xlabel('epochs')\n",
    "ax[0].set_ylabel('Loss = MSE')\n",
    "\n",
    "ax[1].set_title('Test')\n",
    "ax[1].set_xlabel('epochs')\n",
    "ax[1].set_ylabel('Loss = MSE');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12875aee-46ef-4b23-b129-06eeb40217af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://medium.com/towards-data-science/loss-functions-and-their-use-in-neural-networks-a470e703f1e9\n",
    "h_nn_y_true = h_y_nn_test\n",
    "h_nn_y_pred = h_model_nn.predict(h_X_nn_test_sc)\n",
    "\n",
    "# R2\n",
    "print(f'Test R2: {r2_score(h_nn_y_true, h_nn_y_pred)}')\n",
    "\n",
    "# MAE\n",
    "mae = MeanAbsoluteError()\n",
    "print(f'Mean Absolute Error: {mae(h_nn_y_true, h_nn_y_pred)}')\n",
    "\n",
    "# Huber\n",
    "huber = Huber()\n",
    "print(f'Huber Loss: {huber(h_nn_y_true, h_nn_y_pred)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e93ab764-1d3e-421a-ac8d-ddd2befb1385",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
