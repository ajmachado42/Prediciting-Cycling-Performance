{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ba2e640d-2908-4fd0-bdf3-78f0c301dd65",
   "metadata": {},
   "source": [
    "# Model Testing\n",
    "---\n",
    "---\n",
    "### *Commented out models that would take a long time to run; results are in the commented out portions too.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3237b7f-7fc2-4010-b60b-c7d880f28dd7",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Sources and Adaptations From\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9e3e2a61-2500-4f10-9091-992cc2485733",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Various modeling lessons and breakfast hours from DSI 523\n",
    "# https://medium.com/towards-data-science/loss-functions-and-their-use-in-neural-networks-a470e703f1e9\n",
    "# https://www.tensorflow.org/api_docs/python/tf/keras/losses/MeanAbsoluteError\n",
    "# https://www.tensorflow.org/api_docs/python/tf/keras/losses/Huber\n",
    "# https://towardsdatascience.com/what-is-batch-normalization-46058b4f583\n",
    "# https://www.tensorflow.org/tutorials/keras/save_and_load#save_the_entire_model\n",
    "# https://machinelearningmastery.com/xgboost-for-regression/\n",
    "# https://towardsdatascience.com/getting-started-with-xgboost-in-scikit-learn-f69f5f470a97"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9509e54f-20d0-4638-aebd-93d7a93be9f1",
   "metadata": {},
   "source": [
    "# Imports\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b828ad9a-87a1-4fcc-ab0b-1a27c6ba6159",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "pd.set_option('display.max_columns', None)\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import PolynomialFeatures, StandardScaler\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import Ridge, Lasso, ElasticNet\n",
    "from sklearn.ensemble import GradientBoostingRegressor, AdaBoostRegressor, VotingRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import AdaBoostRegressor, RandomForestRegressor\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.regularizers import l1, l2, l1_l2\n",
    "from tensorflow.keras.losses import MeanAbsoluteError, Huber"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ca0ce27d-3d8f-42de-9ea2-616dbf621a46",
   "metadata": {},
   "outputs": [],
   "source": [
    "a_df = pd.read_csv('../data/average/a_df.csv')\n",
    "h_df = pd.read_csv('../data/high/h_df.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16db770f-e93d-4bfb-9395-8176710ffbd9",
   "metadata": {},
   "source": [
    "# Average Cycling Performance Model Testing\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6ab7c15d-bf3f-4083-b508-3cd326ce0bd1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>elevation</th>\n",
       "      <th>dt</th>\n",
       "      <th>heart_rate</th>\n",
       "      <th>bearing</th>\n",
       "      <th>time_diff_s</th>\n",
       "      <th>total_time_s</th>\n",
       "      <th>ele_diff_m</th>\n",
       "      <th>total_ele_change_m</th>\n",
       "      <th>lat_lon</th>\n",
       "      <th>dist_diff_km</th>\n",
       "      <th>total_dist_km</th>\n",
       "      <th>temp</th>\n",
       "      <th>feels_like</th>\n",
       "      <th>pressure</th>\n",
       "      <th>humidity</th>\n",
       "      <th>dew_point</th>\n",
       "      <th>clouds</th>\n",
       "      <th>wind_speed</th>\n",
       "      <th>wind_deg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2022-07-20 16:07:45+00:00</td>\n",
       "      <td>38.773466</td>\n",
       "      <td>-121.363686</td>\n",
       "      <td>35.799999</td>\n",
       "      <td>1658333265</td>\n",
       "      <td>78</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>(38.77346634864807, -121.36368582956493)</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>297.65</td>\n",
       "      <td>297.17</td>\n",
       "      <td>1019</td>\n",
       "      <td>39</td>\n",
       "      <td>282.80</td>\n",
       "      <td>1</td>\n",
       "      <td>0.45</td>\n",
       "      <td>177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2022-07-20 16:07:46+00:00</td>\n",
       "      <td>38.773542</td>\n",
       "      <td>-121.363672</td>\n",
       "      <td>35.599998</td>\n",
       "      <td>1658333266</td>\n",
       "      <td>79</td>\n",
       "      <td>8.292053</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.200001</td>\n",
       "      <td>-0.2</td>\n",
       "      <td>(38.77354153431952, -121.36367183178663)</td>\n",
       "      <td>0.0084</td>\n",
       "      <td>0.0084</td>\n",
       "      <td>297.65</td>\n",
       "      <td>297.17</td>\n",
       "      <td>1019</td>\n",
       "      <td>39</td>\n",
       "      <td>282.80</td>\n",
       "      <td>1</td>\n",
       "      <td>0.45</td>\n",
       "      <td>177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2022-07-20 16:07:49+00:00</td>\n",
       "      <td>38.773630</td>\n",
       "      <td>-121.363682</td>\n",
       "      <td>35.200001</td>\n",
       "      <td>1658333269</td>\n",
       "      <td>82</td>\n",
       "      <td>-5.321180</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>-0.399998</td>\n",
       "      <td>-0.6</td>\n",
       "      <td>(38.77363029867411, -121.36368239298463)</td>\n",
       "      <td>0.0099</td>\n",
       "      <td>0.0183</td>\n",
       "      <td>297.65</td>\n",
       "      <td>297.17</td>\n",
       "      <td>1019</td>\n",
       "      <td>39</td>\n",
       "      <td>282.80</td>\n",
       "      <td>1</td>\n",
       "      <td>0.45</td>\n",
       "      <td>177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2022-07-20 16:07:51+00:00</td>\n",
       "      <td>38.773789</td>\n",
       "      <td>-121.363733</td>\n",
       "      <td>35.000000</td>\n",
       "      <td>1658333271</td>\n",
       "      <td>83</td>\n",
       "      <td>-13.956066</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>-0.200001</td>\n",
       "      <td>-0.8</td>\n",
       "      <td>(38.77378871664405, -121.36373268440366)</td>\n",
       "      <td>0.0181</td>\n",
       "      <td>0.0364</td>\n",
       "      <td>297.67</td>\n",
       "      <td>297.17</td>\n",
       "      <td>1019</td>\n",
       "      <td>38</td>\n",
       "      <td>282.43</td>\n",
       "      <td>1</td>\n",
       "      <td>0.45</td>\n",
       "      <td>177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2022-07-20 16:07:52+00:00</td>\n",
       "      <td>38.773786</td>\n",
       "      <td>-121.363766</td>\n",
       "      <td>35.000000</td>\n",
       "      <td>1658333272</td>\n",
       "      <td>83</td>\n",
       "      <td>-96.936537</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.8</td>\n",
       "      <td>(38.77378553152084, -121.36376612819731)</td>\n",
       "      <td>0.0029</td>\n",
       "      <td>0.0393</td>\n",
       "      <td>297.67</td>\n",
       "      <td>297.17</td>\n",
       "      <td>1019</td>\n",
       "      <td>38</td>\n",
       "      <td>282.43</td>\n",
       "      <td>1</td>\n",
       "      <td>0.45</td>\n",
       "      <td>177</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   timestamp   latitude   longitude  elevation          dt  \\\n",
       "0  2022-07-20 16:07:45+00:00  38.773466 -121.363686  35.799999  1658333265   \n",
       "1  2022-07-20 16:07:46+00:00  38.773542 -121.363672  35.599998  1658333266   \n",
       "2  2022-07-20 16:07:49+00:00  38.773630 -121.363682  35.200001  1658333269   \n",
       "3  2022-07-20 16:07:51+00:00  38.773789 -121.363733  35.000000  1658333271   \n",
       "4  2022-07-20 16:07:52+00:00  38.773786 -121.363766  35.000000  1658333272   \n",
       "\n",
       "   heart_rate    bearing  time_diff_s  total_time_s  ele_diff_m  \\\n",
       "0          78   0.000000            0             0    0.000000   \n",
       "1          79   8.292053            1             1   -0.200001   \n",
       "2          82  -5.321180            3             4   -0.399998   \n",
       "3          83 -13.956066            2             6   -0.200001   \n",
       "4          83 -96.936537            1             7    0.000000   \n",
       "\n",
       "   total_ele_change_m                                   lat_lon  dist_diff_km  \\\n",
       "0                 0.0  (38.77346634864807, -121.36368582956493)        0.0000   \n",
       "1                -0.2  (38.77354153431952, -121.36367183178663)        0.0084   \n",
       "2                -0.6  (38.77363029867411, -121.36368239298463)        0.0099   \n",
       "3                -0.8  (38.77378871664405, -121.36373268440366)        0.0181   \n",
       "4                -0.8  (38.77378553152084, -121.36376612819731)        0.0029   \n",
       "\n",
       "   total_dist_km    temp  feels_like  pressure  humidity  dew_point  clouds  \\\n",
       "0         0.0000  297.65      297.17      1019        39     282.80       1   \n",
       "1         0.0084  297.65      297.17      1019        39     282.80       1   \n",
       "2         0.0183  297.65      297.17      1019        39     282.80       1   \n",
       "3         0.0364  297.67      297.17      1019        38     282.43       1   \n",
       "4         0.0393  297.67      297.17      1019        38     282.43       1   \n",
       "\n",
       "   wind_speed  wind_deg  \n",
       "0        0.45       177  \n",
       "1        0.45       177  \n",
       "2        0.45       177  \n",
       "3        0.45       177  \n",
       "4        0.45       177  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4acc5f0b-46be-4d35-87ce-5e50e80274f6",
   "metadata": {
    "tags": []
   },
   "source": [
    "## X, y, train_test_split\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1422d14a-0130-44c4-afe3-74ee147cd7a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['timestamp', 'latitude', 'longitude', 'elevation', 'dt', 'heart_rate',\n",
       "       'bearing', 'time_diff_s', 'total_time_s', 'ele_diff_m',\n",
       "       'total_ele_change_m', 'lat_lon', 'dist_diff_km', 'total_dist_km',\n",
       "       'temp', 'feels_like', 'pressure', 'humidity', 'dew_point', 'clouds',\n",
       "       'wind_speed', 'wind_deg'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a7112d65-747f-47d3-a69d-5a905518bddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "a_features = ['elevation', 'bearing', 'time_diff_s', 'total_time_s', 'ele_diff_m',\n",
    "       'total_ele_change_m', 'dist_diff_km', 'total_dist_km',\n",
    "       'temp', 'feels_like', 'pressure', 'humidity', 'dew_point', 'clouds',\n",
    "       'wind_speed', 'wind_deg']\n",
    "a_X = a_df[a_features]\n",
    "a_y = a_df['heart_rate']\n",
    "\n",
    "a_X_train, a_X_test, a_y_train, a_y_test = train_test_split(a_X, a_y, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "58b0ddc5-1911-439c-89ec-342d06ab9dfe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "130.21652421652422"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a_y.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5eab97a-12cc-46e7-b1bb-b0222df5cd69",
   "metadata": {},
   "source": [
    "### StandardScaler X_train and X_test for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8902c045-7211-4157-9248-2fb9e0426418",
   "metadata": {},
   "outputs": [],
   "source": [
    "a_ss = StandardScaler()\n",
    "a_X_train_sc = a_ss.fit_transform(a_X_train)\n",
    "a_X_test_sc = a_ss.transform(a_X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04b67ac0-6d16-4efe-b05c-b03bcea45e88",
   "metadata": {},
   "source": [
    "### Polynomial X_train and X_test for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4f429dcf-6b97-424d-ac99-0f12184d3266",
   "metadata": {},
   "outputs": [],
   "source": [
    "a_poly = PolynomialFeatures()\n",
    "a_X_train_sc_p = a_poly.fit_transform(a_X_train_sc)\n",
    "a_X_test_sc_p = a_poly.fit_transform(a_X_test_sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e10c0a5a-9580-4cb1-8a6b-d50c38e4879a",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Linear Regression\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2308b699-e547-4eb1-914d-5d06f13712bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Regression Train R2 Score: 0.5260233155015783\n",
      "Linear Regression Test R2 Score: 0.5322513167399172\n"
     ]
    }
   ],
   "source": [
    "a_lr = LinearRegression()\n",
    "a_lr.fit(a_X_train_sc, a_y_train)\n",
    "print(f'Linear Regression Train R2 Score: {a_lr.score(a_X_train_sc, a_y_train)}')\n",
    "print(f'Linear Regression Test R2 Score: {a_lr.score(a_X_test_sc, a_y_test)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a3eba4a-d843-40fe-894f-437fc51e4da6",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Linear Regression Pipeline (StandardScaler, Polynomial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7dbc010d-fd67-4801-ad74-b6108a94f87b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR Pipe Train R2 Score: 0.8082482755481244\n",
      "LR Pipe Test R2 Score: 0.7906546450250773\n"
     ]
    }
   ],
   "source": [
    "a_lr_pipe = Pipeline([\n",
    "    ('a_ss', StandardScaler()),\n",
    "    ('a_poly', PolynomialFeatures()),\n",
    "    ('a_lr', LinearRegression())\n",
    "])\n",
    "\n",
    "a_lr_pipe.fit(a_X_train, a_y_train)\n",
    "print(f'LR Pipe Train R2 Score: {a_lr_pipe.score(a_X_train, a_y_train)}')\n",
    "print(f'LR Pipe Test R2 Score: {a_lr_pipe.score(a_X_test, a_y_test)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe5836ba-85f8-42db-af96-34667942de0f",
   "metadata": {},
   "source": [
    "## Regressor Boosting\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c3242f8-96db-41b8-9fca-e0b32a8aa4cc",
   "metadata": {
    "tags": []
   },
   "source": [
    "### AdaBoost with Linear Regression Base Estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "968bf2bf-2df7-48f1-9bfc-0f59053ef06c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ada LR Train R2 Score: 0.8046423913777858\n",
      "Ada LR Test R2 Score: 0.776694380832758\n"
     ]
    }
   ],
   "source": [
    "a_ada_lr = AdaBoostRegressor(base_estimator = LinearRegression(), random_state = 42)\n",
    "\n",
    "a_ada_lr.fit(a_X_train_sc_p, a_y_train)\n",
    "print(f'Ada LR Train R2 Score: {a_ada_lr.score(a_X_train_sc_p, a_y_train)}')\n",
    "print(f'Ada LR Test R2 Score: {a_ada_lr.score(a_X_test_sc_p, a_y_test)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4dcc69d-c9c3-47cb-a6e9-a1e24411836d",
   "metadata": {
    "tags": []
   },
   "source": [
    "### AdaBoost with Decision Tree Base Estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fc0afdc7-652b-4526-a213-bfca4d321bb9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n0.868164587298871\\n{'base_estimator__max_depth': 5, 'n_estimators': 200}\\nAda DT Train R2 Score: 0.8756108134812447\\nAda DT Test R2 Score: 0.8730399805443708\\n\""
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "a_ada_dt = AdaBoostRegressor(base_estimator = DecisionTreeRegressor(), random_state = 42)\n",
    "\n",
    "a_ada_dt_params = {\n",
    "    'n_estimators': [50, 100, 150, 200],\n",
    "    'base_estimator__max_depth': [1, 2, 5]\n",
    "}\n",
    "\n",
    "a_gs_ada_dt = GridSearchCV(a_ada_dt, param_grid = a_ada_dt_params, cv = 5)\n",
    "a_gs_ada_dt.fit(a_X_train_sc, a_y_train)\n",
    "print(a_gs_ada_dt.best_score_)\n",
    "print(a_gs_ada_dt.best_params_)\n",
    "print(f'Ada DT Train R2 Score: {a_gs_ada_dt.score(a_X_train_sc, a_y_train)}')\n",
    "print(f'Ada DT Test R2 Score: {a_gs_ada_dt.score(a_X_test_sc, a_y_test)}')\n",
    "'''\n",
    "\n",
    "'''\n",
    "0.868164587298871\n",
    "{'base_estimator__max_depth': 5, 'n_estimators': 200}\n",
    "Ada DT Train R2 Score: 0.8756108134812447\n",
    "Ada DT Test R2 Score: 0.8730399805443708\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6155407f-7539-4cbe-bca7-8b766fd70b94",
   "metadata": {
    "tags": []
   },
   "source": [
    "### AdaBoost with Random Forest Base Estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5de35077-25ad-44a6-bb79-dc018699bd4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n0.844253394135492\\n{'base_estimator__max_depth': 5, 'n_estimators': 50}\\nAda DT Train R2 Score: 0.8526320077631239\\nAda DT Test R2 Score: 0.8512220020729748\\n\""
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "a_ada_rf = AdaBoostRegressor(base_estimator = RandomForestRegressor(), random_state = 42)\n",
    "\n",
    "a_ada_rf_params = {\n",
    "    'n_estimators': [50, 100, 150, 200],\n",
    "    'base_estimator__max_depth': [1, 2, 5]\n",
    "}\n",
    "\n",
    "a_gs_ada_rf = GridSearchCV(a_ada_rf, param_grid = a_ada_rf_params, cv = 5)\n",
    "a_gs_ada_rf.fit(a_X_train_sc, a_y_train)\n",
    "print(a_gs_ada_rf.best_score_)\n",
    "print(a_gs_ada_rf.best_params_)\n",
    "print(f'Ada DT Train R2 Score: {a_gs_ada_rf.score(a_X_train_sc, a_y_train)}')\n",
    "print(f'Ada DT Test R2 Score: {a_gs_ada_rf.score(a_X_test_sc, a_y_test)}')\n",
    "'''\n",
    "'''\n",
    "0.844253394135492\n",
    "{'base_estimator__max_depth': 5, 'n_estimators': 50}\n",
    "Ada DT Train R2 Score: 0.8526320077631239\n",
    "Ada DT Test R2 Score: 0.8512220020729748\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c920337-6425-47be-9aaa-60769c0e1f00",
   "metadata": {},
   "source": [
    "### Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f14bbf6-8fe7-4203-ba97-93d56b5a5234",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "'''\n",
    "# Instantiate\n",
    "a_gboost = GradientBoostingRegressor()\n",
    "\n",
    "a_gboost_params = {\n",
    "    'max_depth': [2, 3, 4],\n",
    "    'n_estimators': [100, 125, 150],\n",
    "    'learning_rate': [0.8, .1, .12]\n",
    "}\n",
    "\n",
    "# Gridsearch\n",
    "a_gb_gs = GridSearchCV(a_gboost, param_grid = a_gboost_params, cv = 5)\n",
    "a_gb_gs.fit(a_X_train_sc, a_y_train)\n",
    "\n",
    "# Metrics\n",
    "print(f'GridSearch Best Score: {a_gb_gs.best_score_}')\n",
    "print(a_gb_gs.best_params_)\n",
    "print(f'GBoost Train R2 Score: {a_gb_gs.score(a_X_train_sc, a_y_train)}')\n",
    "print(f'GBoost Test R2 Score: {a_gb_gs.score(a_X_test_sc, a_y_test)}')\n",
    "\n",
    "a_gb_y_true = a_y_test\n",
    "a_gb_y_pred = a_gb_gs.predict(a_X_test_sc)\n",
    "\n",
    "# MAE\n",
    "print(f'Mean Absolute Error: {mean_absolute_error(a_gb_y_true, a_gb_y_pred)}')\n",
    "\n",
    "# MSE\n",
    "print(f'Mean Squared Error: {mean_squared_error(a_gb_y_true, a_gb_y_pred)}')\n",
    "'''\n",
    "'''\n",
    "GridSearch Best Score: 0.964003581281308\n",
    "{'learning_rate': 0.8, 'max_depth': 4, 'n_estimators': 150}\n",
    "GBoost Train R2 Score: 0.9888934084696182\n",
    "GBoost Test R2 Score: 0.9709606783777225\n",
    "Mean Absolute Error: 2.7824393977965642\n",
    "Mean Squared Error: 14.658521217024555\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4181f707-d03f-4ee2-b37b-1118d1248c1f",
   "metadata": {},
   "source": [
    "### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bbad08d0-11cc-4eec-8800-6c20b255c9d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GridSearch Best Score: 0.9659966627636546\n",
      "{'learning_rate': 0.8, 'max_depth': 4, 'n_estimators': 150}\n",
      "XGBoost Train R2 Score: 0.9888381686104007\n",
      "XGBoost Test R2 Score: 0.9719382323708556\n",
      "Mean Absolute Error: 2.7619688805285496\n",
      "Mean Squared Error: 14.165069746789941\n"
     ]
    }
   ],
   "source": [
    "# Instantiate\n",
    "a_xgboost = XGBRegressor()\n",
    "\n",
    "# Gridsearch\n",
    "a_xgboost_params = {\n",
    "    'max_depth': [2, 3, 4],\n",
    "    'n_estimators': [100, 125, 150],\n",
    "    'learning_rate': [0.8, .1, .12]\n",
    "}\n",
    "\n",
    "a_xgb_gs = GridSearchCV(a_xgboost, param_grid = a_xgboost_params, cv = 5)\n",
    "a_xgb_gs.fit(a_X_train_sc, a_y_train)\n",
    "\n",
    "# Metrics\n",
    "print(f'GridSearch Best Score: {a_xgb_gs.best_score_}')\n",
    "print(a_xgb_gs.best_params_)\n",
    "print(f'XGBoost Train R2 Score: {a_xgb_gs.score(a_X_train_sc, a_y_train)}')\n",
    "print(f'XGBoost Test R2 Score: {a_xgb_gs.score(a_X_test_sc, a_y_test)}')\n",
    "\n",
    "a_xgb_y_true = a_y_test\n",
    "a_xgb_y_pred = a_xgb_gs.predict(a_X_test_sc)\n",
    "\n",
    "# MAE\n",
    "print(f'Mean Absolute Error: {mean_absolute_error(a_xgb_y_true, a_xgb_y_pred)}')\n",
    "\n",
    "# MSE\n",
    "print(f'Mean Squared Error: {mean_squared_error(a_xgb_y_true, a_xgb_y_pred)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d8ca1c8-b384-47d3-996b-6f29e207438b",
   "metadata": {},
   "source": [
    "## Neural Net Regressor\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22cacd20-80c3-48ac-bea0-c84c0c8cd227",
   "metadata": {},
   "source": [
    "### X, y, train_test_split, StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "72562d7c-5ea8-442a-9d86-9e02f13c5004",
   "metadata": {},
   "outputs": [],
   "source": [
    "a_X_nn = a_df[a_features]\n",
    "a_y_nn = a_df['heart_rate']\n",
    "\n",
    "a_X_nn = np.array(a_X_nn)\n",
    "a_y_nn = np.array(a_y_nn)\n",
    "\n",
    "a_X_nn_train, a_X_nn_test, a_y_nn_train, a_y_nn_test = train_test_split(a_X_nn, a_y_nn, random_state = 42)\n",
    "\n",
    "a_ss_nn = StandardScaler()\n",
    "a_X_nn_train_sc = a_ss_nn.fit_transform(a_X_nn_train)\n",
    "a_X_nn_test_sc = a_ss.transform(a_X_nn_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2407b7a2-6e73-4b0e-a006-93ffbc16cbe4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16,)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a_X_nn[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ef54f2ba-d2bd-4c98-b754-af095d7e5d76",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-11 13:05:35.608628: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# Instantiate\n",
    "a_model_nn = Sequential()\n",
    "\n",
    "# Layers\n",
    "a_model_nn.add(Dense(128, input_dim = 16, activation = 'relu'))\n",
    "\n",
    "a_model_nn.add(BatchNormalization())\n",
    "a_model_nn.add(Dense(64, activation = 'relu', kernel_regularizer = l2(.1)))\n",
    "a_model_nn.add(Dense(128, activation = 'relu', kernel_regularizer = l2(.5)))\n",
    "a_model_nn.add(Dense(64, activation = 'relu', kernel_regularizer = l2(.1)))\n",
    "a_model_nn.add(Dense(128, activation = 'relu', kernel_regularizer = l2(.5))) \n",
    "a_model_nn.add(Dense(64, activation = 'relu', kernel_regularizer = l2(.1))) \n",
    "a_model_nn.add(Dense(1, kernel_regularizer = l2(.5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d60e4577-0412-443b-94e0-ea16b121a107",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile\n",
    "a_model_nn.compile(optimizer = 'adam', loss = 'mse', metrics = ['mse'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b676e67c-ea0c-4e85-88ea-8c8361d2fc68",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "354/354 [==============================] - 2s 3ms/step - loss: 1401.4193 - mse: 1309.4623 - val_loss: 854.5279 - val_mse: 773.6292\n",
      "Epoch 2/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 238.7970 - mse: 163.1531 - val_loss: 207.3080 - val_mse: 136.1577\n",
      "Epoch 3/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 207.8374 - mse: 140.1891 - val_loss: 196.7780 - val_mse: 132.4665\n",
      "Epoch 4/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 191.4227 - mse: 129.9038 - val_loss: 230.4197 - val_mse: 171.6844\n",
      "Epoch 5/500\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 178.9811 - mse: 122.5710 - val_loss: 225.9859 - val_mse: 171.9776\n",
      "Epoch 6/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 160.0215 - mse: 108.0152 - val_loss: 201.5130 - val_mse: 151.6079\n",
      "Epoch 7/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 152.9096 - mse: 104.8138 - val_loss: 124.5703 - val_mse: 78.2793\n",
      "Epoch 8/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 145.1072 - mse: 100.4835 - val_loss: 125.7973 - val_mse: 82.7066\n",
      "Epoch 9/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 139.9454 - mse: 98.4255 - val_loss: 121.2765 - val_mse: 81.2127\n",
      "Epoch 10/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 134.2884 - mse: 95.5705 - val_loss: 114.9565 - val_mse: 77.5641\n",
      "Epoch 11/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 127.6618 - mse: 91.4671 - val_loss: 116.8951 - val_mse: 81.8178\n",
      "Epoch 12/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 126.8564 - mse: 92.9183 - val_loss: 117.9926 - val_mse: 85.1297\n",
      "Epoch 13/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 121.4118 - mse: 89.4571 - val_loss: 106.4602 - val_mse: 75.3683\n",
      "Epoch 14/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 114.3693 - mse: 84.1777 - val_loss: 112.5683 - val_mse: 83.1478\n",
      "Epoch 15/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 115.3959 - mse: 86.8284 - val_loss: 112.6046 - val_mse: 84.8140\n",
      "Epoch 16/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 112.6993 - mse: 85.5681 - val_loss: 100.2965 - val_mse: 73.8584\n",
      "Epoch 17/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 107.0403 - mse: 81.2173 - val_loss: 93.2392 - val_mse: 68.0792\n",
      "Epoch 18/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 106.1524 - mse: 81.5297 - val_loss: 86.4390 - val_mse: 62.3729\n",
      "Epoch 19/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 103.0877 - mse: 79.5567 - val_loss: 94.6626 - val_mse: 71.6276\n",
      "Epoch 20/500\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 101.0136 - mse: 78.4897 - val_loss: 92.5204 - val_mse: 70.4750\n",
      "Epoch 21/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 102.6449 - mse: 81.0070 - val_loss: 91.8412 - val_mse: 70.5850\n",
      "Epoch 22/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 99.3113 - mse: 78.4828 - val_loss: 93.1565 - val_mse: 72.7404\n",
      "Epoch 23/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 96.4741 - mse: 76.3545 - val_loss: 108.5991 - val_mse: 88.8872\n",
      "Epoch 24/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 90.7517 - mse: 71.3248 - val_loss: 80.5011 - val_mse: 61.3937\n",
      "Epoch 25/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 93.4333 - mse: 74.6360 - val_loss: 107.2942 - val_mse: 88.7082\n",
      "Epoch 26/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 92.7605 - mse: 74.5232 - val_loss: 78.8350 - val_mse: 60.8540\n",
      "Epoch 27/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 91.0158 - mse: 73.3046 - val_loss: 83.0646 - val_mse: 65.6400\n",
      "Epoch 28/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 89.4963 - mse: 72.2615 - val_loss: 83.6690 - val_mse: 66.7255\n",
      "Epoch 29/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 85.8124 - mse: 69.0131 - val_loss: 73.6244 - val_mse: 57.0739\n",
      "Epoch 30/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 87.7017 - mse: 71.3332 - val_loss: 73.2501 - val_mse: 57.0573\n",
      "Epoch 31/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 86.2298 - mse: 70.2383 - val_loss: 70.9636 - val_mse: 55.1816\n",
      "Epoch 32/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 84.5239 - mse: 68.9210 - val_loss: 77.9827 - val_mse: 62.5965\n",
      "Epoch 33/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 83.8308 - mse: 68.5425 - val_loss: 78.7353 - val_mse: 63.5345\n",
      "Epoch 34/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 82.2310 - mse: 67.2384 - val_loss: 72.1044 - val_mse: 57.2490\n",
      "Epoch 35/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 78.9663 - mse: 64.3008 - val_loss: 84.2819 - val_mse: 69.7275\n",
      "Epoch 36/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 80.5841 - mse: 66.1974 - val_loss: 75.7911 - val_mse: 61.4932\n",
      "Epoch 37/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 82.7663 - mse: 68.6276 - val_loss: 95.1229 - val_mse: 81.1288\n",
      "Epoch 38/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 79.4408 - mse: 65.5490 - val_loss: 75.8236 - val_mse: 62.0061\n",
      "Epoch 39/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 79.5626 - mse: 65.9182 - val_loss: 66.1335 - val_mse: 52.5701\n",
      "Epoch 40/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 77.6207 - mse: 64.2006 - val_loss: 71.0059 - val_mse: 57.7397\n",
      "Epoch 41/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 79.6668 - mse: 66.4580 - val_loss: 69.3260 - val_mse: 56.2460\n",
      "Epoch 42/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 80.5736 - mse: 67.5391 - val_loss: 82.9269 - val_mse: 70.0891\n",
      "Epoch 43/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 78.3090 - mse: 65.4956 - val_loss: 91.0398 - val_mse: 78.3588\n",
      "Epoch 44/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 73.7276 - mse: 61.0936 - val_loss: 66.3376 - val_mse: 53.8094\n",
      "Epoch 45/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 73.4033 - mse: 60.9599 - val_loss: 68.5690 - val_mse: 56.1788\n",
      "Epoch 46/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 75.4415 - mse: 63.1562 - val_loss: 62.5116 - val_mse: 50.2942\n",
      "Epoch 47/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 75.8529 - mse: 63.7274 - val_loss: 90.7175 - val_mse: 78.7505\n",
      "Epoch 48/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 72.9771 - mse: 61.0110 - val_loss: 68.1537 - val_mse: 56.2773\n",
      "Epoch 49/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 75.1197 - mse: 63.2998 - val_loss: 66.6110 - val_mse: 54.8667\n",
      "Epoch 50/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 73.3643 - mse: 61.6692 - val_loss: 73.6246 - val_mse: 62.0245\n",
      "Epoch 51/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 73.7255 - mse: 62.1624 - val_loss: 90.1824 - val_mse: 78.7375\n",
      "Epoch 52/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 74.1189 - mse: 62.6928 - val_loss: 62.3948 - val_mse: 51.0273\n",
      "Epoch 53/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 72.2506 - mse: 60.9157 - val_loss: 57.2782 - val_mse: 45.9908\n",
      "Epoch 54/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 73.1231 - mse: 61.9026 - val_loss: 64.8850 - val_mse: 53.7501\n",
      "Epoch 55/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 72.0241 - mse: 60.9083 - val_loss: 64.3158 - val_mse: 53.2760\n",
      "Epoch 56/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 74.2694 - mse: 63.2584 - val_loss: 61.2479 - val_mse: 50.2783\n",
      "Epoch 57/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 69.9567 - mse: 59.0287 - val_loss: 61.1352 - val_mse: 50.2456\n",
      "Epoch 58/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 69.1799 - mse: 58.3357 - val_loss: 61.1943 - val_mse: 50.4144\n",
      "Epoch 59/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 70.3529 - mse: 59.6017 - val_loss: 64.6919 - val_mse: 54.0570\n",
      "Epoch 60/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 69.3512 - mse: 58.6843 - val_loss: 61.0020 - val_mse: 50.4097\n",
      "Epoch 61/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 68.3189 - mse: 57.7250 - val_loss: 82.9684 - val_mse: 72.4795\n",
      "Epoch 62/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 70.7164 - mse: 60.2160 - val_loss: 57.1982 - val_mse: 46.7542\n",
      "Epoch 63/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 67.1808 - mse: 56.7589 - val_loss: 63.1851 - val_mse: 52.7880\n",
      "Epoch 64/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 67.0402 - mse: 56.6863 - val_loss: 61.7255 - val_mse: 51.3654\n",
      "Epoch 65/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 69.5663 - mse: 59.2584 - val_loss: 71.5998 - val_mse: 61.2805\n",
      "Epoch 66/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 66.6143 - mse: 56.3998 - val_loss: 63.8044 - val_mse: 53.6428\n",
      "Epoch 67/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 68.4293 - mse: 58.2826 - val_loss: 57.7768 - val_mse: 47.6494\n",
      "Epoch 68/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 66.5605 - mse: 56.4687 - val_loss: 67.5981 - val_mse: 57.5868\n",
      "Epoch 69/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 66.6381 - mse: 56.6216 - val_loss: 56.4865 - val_mse: 46.4958\n",
      "Epoch 70/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 66.3264 - mse: 56.3460 - val_loss: 75.3105 - val_mse: 65.4207\n",
      "Epoch 71/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 66.7795 - mse: 56.8386 - val_loss: 61.4125 - val_mse: 51.4801\n",
      "Epoch 72/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 69.1418 - mse: 59.2667 - val_loss: 57.7227 - val_mse: 47.8550\n",
      "Epoch 73/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 68.6268 - mse: 58.7948 - val_loss: 69.8440 - val_mse: 60.0933\n",
      "Epoch 74/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 64.4702 - mse: 54.6682 - val_loss: 58.8533 - val_mse: 49.0614\n",
      "Epoch 75/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 66.2147 - mse: 56.4757 - val_loss: 63.1478 - val_mse: 53.5035\n",
      "Epoch 76/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 66.1558 - mse: 56.4618 - val_loss: 52.4531 - val_mse: 42.7893\n",
      "Epoch 77/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 65.0905 - mse: 55.4369 - val_loss: 68.7546 - val_mse: 59.0508\n",
      "Epoch 78/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 64.1032 - mse: 54.4872 - val_loss: 62.6800 - val_mse: 53.1407\n",
      "Epoch 79/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 66.2167 - mse: 56.6499 - val_loss: 58.2832 - val_mse: 48.7392\n",
      "Epoch 80/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 63.7461 - mse: 54.2176 - val_loss: 56.9957 - val_mse: 47.4789\n",
      "Epoch 81/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 61.4514 - mse: 51.9603 - val_loss: 55.9423 - val_mse: 46.4822\n",
      "Epoch 82/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 62.5760 - mse: 53.1224 - val_loss: 57.1578 - val_mse: 47.7262\n",
      "Epoch 83/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 66.6766 - mse: 57.2160 - val_loss: 57.3511 - val_mse: 47.9562\n",
      "Epoch 84/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 63.6153 - mse: 54.2231 - val_loss: 56.1811 - val_mse: 46.8323\n",
      "Epoch 85/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 63.5008 - mse: 54.1528 - val_loss: 55.8012 - val_mse: 46.4911\n",
      "Epoch 86/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 61.9087 - mse: 52.5916 - val_loss: 69.4241 - val_mse: 60.1768\n",
      "Epoch 87/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 65.7848 - mse: 56.4895 - val_loss: 56.1174 - val_mse: 46.8651\n",
      "Epoch 88/500\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 62.5535 - mse: 53.2797 - val_loss: 55.7770 - val_mse: 46.5194\n",
      "Epoch 89/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 63.0430 - mse: 53.7874 - val_loss: 61.3996 - val_mse: 52.1439\n",
      "Epoch 90/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 60.8671 - mse: 51.6591 - val_loss: 60.7630 - val_mse: 51.5284\n",
      "Epoch 91/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 64.1845 - mse: 54.9884 - val_loss: 57.0600 - val_mse: 47.8820\n",
      "Epoch 92/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 60.9357 - mse: 51.7787 - val_loss: 54.4927 - val_mse: 45.3078\n",
      "Epoch 93/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 61.5611 - mse: 52.4088 - val_loss: 55.4346 - val_mse: 46.3098\n",
      "Epoch 94/500\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 63.6976 - mse: 54.5950 - val_loss: 55.8480 - val_mse: 46.7573\n",
      "Epoch 95/500\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 60.9661 - mse: 51.8781 - val_loss: 55.9524 - val_mse: 46.8907\n",
      "Epoch 96/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 62.4263 - mse: 53.3638 - val_loss: 54.7734 - val_mse: 45.7450\n",
      "Epoch 97/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 61.3564 - mse: 52.3267 - val_loss: 56.1948 - val_mse: 47.1799\n",
      "Epoch 98/500\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 60.7223 - mse: 51.7131 - val_loss: 69.3968 - val_mse: 60.4646\n",
      "Epoch 99/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 62.0764 - mse: 53.0892 - val_loss: 62.6119 - val_mse: 53.6061\n",
      "Epoch 100/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 61.0085 - mse: 52.0432 - val_loss: 76.6418 - val_mse: 67.7724\n",
      "Epoch 101/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 62.3724 - mse: 53.4118 - val_loss: 56.7641 - val_mse: 47.8149\n",
      "Epoch 102/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 62.6734 - mse: 53.7323 - val_loss: 55.9673 - val_mse: 47.0697\n",
      "Epoch 103/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 62.1390 - mse: 53.2365 - val_loss: 52.3404 - val_mse: 43.4728\n",
      "Epoch 104/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 62.0989 - mse: 53.2292 - val_loss: 56.5590 - val_mse: 47.6712\n",
      "Epoch 105/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 61.7347 - mse: 52.8784 - val_loss: 52.6355 - val_mse: 43.7653\n",
      "Epoch 106/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 60.2240 - mse: 51.3687 - val_loss: 60.4571 - val_mse: 51.6410\n",
      "Epoch 107/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 59.8541 - mse: 51.0085 - val_loss: 57.7319 - val_mse: 48.9339\n",
      "Epoch 108/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 60.9858 - mse: 52.1687 - val_loss: 60.6716 - val_mse: 51.8371\n",
      "Epoch 109/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 59.4149 - mse: 50.6071 - val_loss: 61.9783 - val_mse: 53.2091\n",
      "Epoch 110/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 60.5975 - mse: 51.8023 - val_loss: 65.3009 - val_mse: 56.5585\n",
      "Epoch 111/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 60.2606 - mse: 51.4698 - val_loss: 57.9355 - val_mse: 49.1737\n",
      "Epoch 112/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 60.1053 - mse: 51.3309 - val_loss: 56.9797 - val_mse: 48.2158\n",
      "Epoch 113/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 59.5775 - mse: 50.8087 - val_loss: 67.7418 - val_mse: 58.9214\n",
      "Epoch 114/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 59.9905 - mse: 51.2221 - val_loss: 57.7386 - val_mse: 49.0050\n",
      "Epoch 115/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 58.9054 - mse: 50.1531 - val_loss: 58.0022 - val_mse: 49.2953\n",
      "Epoch 116/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 59.8377 - mse: 51.0774 - val_loss: 55.6973 - val_mse: 46.9129\n",
      "Epoch 117/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 58.7033 - mse: 49.9489 - val_loss: 55.7030 - val_mse: 46.9474\n",
      "Epoch 118/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 59.2050 - mse: 50.4506 - val_loss: 53.4963 - val_mse: 44.7862\n",
      "Epoch 119/500\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 57.5685 - mse: 48.8348 - val_loss: 58.0039 - val_mse: 49.2402\n",
      "Epoch 120/500\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 56.7178 - mse: 47.9609 - val_loss: 53.9178 - val_mse: 45.1577\n",
      "Epoch 121/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 58.6691 - mse: 49.9156 - val_loss: 52.9724 - val_mse: 44.2365\n",
      "Epoch 122/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 59.5601 - mse: 50.7977 - val_loss: 51.2106 - val_mse: 42.4410\n",
      "Epoch 123/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 57.2988 - mse: 48.5319 - val_loss: 55.6915 - val_mse: 46.8975\n",
      "Epoch 124/500\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 57.9340 - mse: 49.1766 - val_loss: 62.4398 - val_mse: 53.6381\n",
      "Epoch 125/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 57.5731 - mse: 48.8286 - val_loss: 57.2268 - val_mse: 48.5160\n",
      "Epoch 126/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 58.0947 - mse: 49.3484 - val_loss: 50.8838 - val_mse: 42.1542\n",
      "Epoch 127/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 56.8093 - mse: 48.0638 - val_loss: 57.9710 - val_mse: 49.1808\n",
      "Epoch 128/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 58.5757 - mse: 49.8334 - val_loss: 55.7738 - val_mse: 47.0696\n",
      "Epoch 129/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 58.7165 - mse: 49.9808 - val_loss: 60.9902 - val_mse: 52.2891\n",
      "Epoch 130/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 57.7981 - mse: 49.0504 - val_loss: 72.1919 - val_mse: 63.3556\n",
      "Epoch 131/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 58.4486 - mse: 49.6874 - val_loss: 55.4365 - val_mse: 46.6677\n",
      "Epoch 132/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 57.6016 - mse: 48.8423 - val_loss: 52.5659 - val_mse: 43.7875\n",
      "Epoch 133/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 57.8332 - mse: 49.0487 - val_loss: 53.6991 - val_mse: 44.9359\n",
      "Epoch 134/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 57.7672 - mse: 48.9758 - val_loss: 62.3581 - val_mse: 53.6489\n",
      "Epoch 135/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 57.7646 - mse: 48.9715 - val_loss: 62.9352 - val_mse: 54.0641\n",
      "Epoch 136/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 59.3513 - mse: 50.5357 - val_loss: 53.3772 - val_mse: 44.5493\n",
      "Epoch 137/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 56.8823 - mse: 48.0792 - val_loss: 55.0837 - val_mse: 46.3340\n",
      "Epoch 138/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 57.1073 - mse: 48.3132 - val_loss: 50.5125 - val_mse: 41.7400\n",
      "Epoch 139/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 55.8341 - mse: 47.0430 - val_loss: 52.5538 - val_mse: 43.7675\n",
      "Epoch 140/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 55.1174 - mse: 46.3122 - val_loss: 52.0403 - val_mse: 43.2422\n",
      "Epoch 141/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 57.1119 - mse: 48.3119 - val_loss: 57.5096 - val_mse: 48.7101\n",
      "Epoch 142/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 56.1214 - mse: 47.2968 - val_loss: 55.3224 - val_mse: 46.4779\n",
      "Epoch 143/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 56.9160 - mse: 48.0969 - val_loss: 51.3759 - val_mse: 42.5915\n",
      "Epoch 144/500\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 55.6121 - mse: 46.8063 - val_loss: 53.9921 - val_mse: 45.1930\n",
      "Epoch 145/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 57.4054 - mse: 48.5851 - val_loss: 48.8220 - val_mse: 40.0331\n",
      "Epoch 146/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 57.1997 - mse: 48.4008 - val_loss: 56.9809 - val_mse: 48.1441\n",
      "Epoch 147/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 56.2877 - mse: 47.4752 - val_loss: 53.3453 - val_mse: 44.4855\n",
      "Epoch 148/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 56.1264 - mse: 47.2941 - val_loss: 56.3381 - val_mse: 47.5441\n",
      "Epoch 149/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 54.5543 - mse: 45.7266 - val_loss: 54.9402 - val_mse: 46.1426\n",
      "Epoch 150/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 54.6641 - mse: 45.8240 - val_loss: 54.3947 - val_mse: 45.5260\n",
      "Epoch 151/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 54.3182 - mse: 45.4861 - val_loss: 52.8094 - val_mse: 43.9824\n",
      "Epoch 152/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 54.2701 - mse: 45.4433 - val_loss: 53.7116 - val_mse: 44.8918\n",
      "Epoch 153/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 56.7463 - mse: 47.9021 - val_loss: 56.0754 - val_mse: 47.2717\n",
      "Epoch 154/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 57.4219 - mse: 48.5464 - val_loss: 56.3687 - val_mse: 47.5301\n",
      "Epoch 155/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 56.2665 - mse: 47.3721 - val_loss: 59.7898 - val_mse: 50.8350\n",
      "Epoch 156/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 57.4540 - mse: 48.5395 - val_loss: 56.5471 - val_mse: 47.6307\n",
      "Epoch 157/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 55.2573 - mse: 46.3199 - val_loss: 51.7270 - val_mse: 42.7609\n",
      "Epoch 158/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 55.1832 - mse: 46.2154 - val_loss: 49.9756 - val_mse: 41.0056\n",
      "Epoch 159/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 53.9512 - mse: 44.9893 - val_loss: 60.6728 - val_mse: 51.6472\n",
      "Epoch 160/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 54.2899 - mse: 45.3288 - val_loss: 51.6184 - val_mse: 42.6287\n",
      "Epoch 161/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 53.9389 - mse: 44.9664 - val_loss: 49.9951 - val_mse: 41.0461\n",
      "Epoch 162/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 54.3328 - mse: 45.3736 - val_loss: 47.5134 - val_mse: 38.5270\n",
      "Epoch 163/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 53.3527 - mse: 44.3659 - val_loss: 47.3441 - val_mse: 38.3457\n",
      "Epoch 164/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 54.6791 - mse: 45.6681 - val_loss: 52.3370 - val_mse: 43.3173\n",
      "Epoch 165/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 53.2127 - mse: 44.1913 - val_loss: 56.9633 - val_mse: 47.9716\n",
      "Epoch 166/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 54.5265 - mse: 45.4812 - val_loss: 51.3635 - val_mse: 42.2806\n",
      "Epoch 167/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 54.9677 - mse: 45.9134 - val_loss: 56.5347 - val_mse: 47.4751\n",
      "Epoch 168/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 55.7028 - mse: 46.6616 - val_loss: 50.1026 - val_mse: 41.0222\n",
      "Epoch 169/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 54.6798 - mse: 45.6135 - val_loss: 53.7252 - val_mse: 44.6435\n",
      "Epoch 170/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 54.8573 - mse: 45.7852 - val_loss: 50.5206 - val_mse: 41.4513\n",
      "Epoch 171/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 54.3271 - mse: 45.2471 - val_loss: 51.7863 - val_mse: 42.7452\n",
      "Epoch 172/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 53.4109 - mse: 44.3245 - val_loss: 50.7800 - val_mse: 41.6286\n",
      "Epoch 173/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 53.7344 - mse: 44.6143 - val_loss: 50.4580 - val_mse: 41.3330\n",
      "Epoch 174/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 53.7201 - mse: 44.5825 - val_loss: 50.6356 - val_mse: 41.5156\n",
      "Epoch 175/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 54.1536 - mse: 45.0337 - val_loss: 46.9457 - val_mse: 37.8508\n",
      "Epoch 176/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 54.2193 - mse: 45.0951 - val_loss: 51.7347 - val_mse: 42.5892\n",
      "Epoch 177/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 52.5472 - mse: 43.4118 - val_loss: 53.3476 - val_mse: 44.2167\n",
      "Epoch 178/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 52.1727 - mse: 43.0085 - val_loss: 50.8065 - val_mse: 41.6781\n",
      "Epoch 179/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 53.2072 - mse: 44.0443 - val_loss: 50.9323 - val_mse: 41.7156\n",
      "Epoch 180/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 53.7124 - mse: 44.5343 - val_loss: 49.6780 - val_mse: 40.4963\n",
      "Epoch 181/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 52.0067 - mse: 42.8348 - val_loss: 61.5024 - val_mse: 52.3073\n",
      "Epoch 182/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 53.3177 - mse: 44.1458 - val_loss: 52.2200 - val_mse: 43.0256\n",
      "Epoch 183/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 53.4836 - mse: 44.2821 - val_loss: 51.2382 - val_mse: 42.0541\n",
      "Epoch 184/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 53.6951 - mse: 44.5015 - val_loss: 53.0515 - val_mse: 43.8193\n",
      "Epoch 185/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 52.7046 - mse: 43.4995 - val_loss: 52.4399 - val_mse: 43.2573\n",
      "Epoch 186/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 53.0633 - mse: 43.8433 - val_loss: 55.4899 - val_mse: 46.2928\n",
      "Epoch 187/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 51.4970 - mse: 42.2944 - val_loss: 51.2219 - val_mse: 42.0300\n",
      "Epoch 188/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 51.9998 - mse: 42.7682 - val_loss: 55.7848 - val_mse: 46.5485\n",
      "Epoch 189/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 52.3319 - mse: 43.1147 - val_loss: 52.6942 - val_mse: 43.4380\n",
      "Epoch 190/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 54.1079 - mse: 44.8878 - val_loss: 52.8255 - val_mse: 43.5544\n",
      "Epoch 191/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 52.6754 - mse: 43.4460 - val_loss: 54.6338 - val_mse: 45.3778\n",
      "Epoch 192/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 51.8482 - mse: 42.6062 - val_loss: 51.2535 - val_mse: 42.0034\n",
      "Epoch 193/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 53.0191 - mse: 43.8003 - val_loss: 48.9813 - val_mse: 39.7537\n",
      "Epoch 194/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 50.8625 - mse: 41.6318 - val_loss: 61.6644 - val_mse: 52.5229\n",
      "Epoch 195/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 52.9463 - mse: 43.7216 - val_loss: 50.4072 - val_mse: 41.1525\n",
      "Epoch 196/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 53.2896 - mse: 44.0758 - val_loss: 52.7500 - val_mse: 43.4960\n",
      "Epoch 197/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 51.5945 - mse: 42.3543 - val_loss: 48.6981 - val_mse: 39.4927\n",
      "Epoch 198/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 52.4049 - mse: 43.1563 - val_loss: 52.6162 - val_mse: 43.3308\n",
      "Epoch 199/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 51.5968 - mse: 42.3425 - val_loss: 48.7467 - val_mse: 39.4954\n",
      "Epoch 200/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 51.9032 - mse: 42.6536 - val_loss: 50.9237 - val_mse: 41.6535\n",
      "Epoch 201/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 50.5263 - mse: 41.2515 - val_loss: 48.6659 - val_mse: 39.3815\n",
      "Epoch 202/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 51.4766 - mse: 42.2071 - val_loss: 49.8883 - val_mse: 40.5908\n",
      "Epoch 203/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 52.4747 - mse: 43.2003 - val_loss: 49.5674 - val_mse: 40.2737\n",
      "Epoch 204/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 52.2732 - mse: 42.9967 - val_loss: 48.5708 - val_mse: 39.2842\n",
      "Epoch 205/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 53.3931 - mse: 44.0880 - val_loss: 51.3137 - val_mse: 41.9913\n",
      "Epoch 206/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 51.0591 - mse: 41.7334 - val_loss: 49.2555 - val_mse: 39.9231\n",
      "Epoch 207/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 53.1880 - mse: 43.8270 - val_loss: 46.5012 - val_mse: 37.1621\n",
      "Epoch 208/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 51.5512 - mse: 42.1823 - val_loss: 46.9601 - val_mse: 37.5894\n",
      "Epoch 209/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 50.3653 - mse: 41.0272 - val_loss: 47.1747 - val_mse: 37.8543\n",
      "Epoch 210/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 52.1982 - mse: 42.8483 - val_loss: 65.0110 - val_mse: 55.5843\n",
      "Epoch 211/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 50.5038 - mse: 41.1448 - val_loss: 47.9722 - val_mse: 38.6142\n",
      "Epoch 212/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 51.0198 - mse: 41.6836 - val_loss: 46.8495 - val_mse: 37.5236\n",
      "Epoch 213/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 52.5189 - mse: 43.1896 - val_loss: 47.4680 - val_mse: 38.1061\n",
      "Epoch 214/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 50.7716 - mse: 41.4309 - val_loss: 49.6130 - val_mse: 40.2467\n",
      "Epoch 215/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 51.1046 - mse: 41.7474 - val_loss: 55.9101 - val_mse: 46.5874\n",
      "Epoch 216/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 50.1103 - mse: 40.7680 - val_loss: 53.1393 - val_mse: 43.8466\n",
      "Epoch 217/500\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 50.9396 - mse: 41.5898 - val_loss: 49.8052 - val_mse: 40.4267\n",
      "Epoch 218/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 49.5087 - mse: 40.1321 - val_loss: 49.1823 - val_mse: 39.8342\n",
      "Epoch 219/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 50.8252 - mse: 41.4647 - val_loss: 48.7711 - val_mse: 39.3920\n",
      "Epoch 220/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 51.4136 - mse: 42.0452 - val_loss: 48.2574 - val_mse: 38.8886\n",
      "Epoch 221/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 50.1693 - mse: 40.8147 - val_loss: 47.6222 - val_mse: 38.2505\n",
      "Epoch 222/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 51.5087 - mse: 42.1436 - val_loss: 50.2547 - val_mse: 40.9032\n",
      "Epoch 223/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 50.6120 - mse: 41.2414 - val_loss: 49.3751 - val_mse: 40.0078\n",
      "Epoch 224/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 50.1919 - mse: 40.8247 - val_loss: 46.7909 - val_mse: 37.3682\n",
      "Epoch 225/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 51.5369 - mse: 42.1504 - val_loss: 49.8493 - val_mse: 40.4837\n",
      "Epoch 226/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 50.4558 - mse: 41.0684 - val_loss: 48.1428 - val_mse: 38.7744\n",
      "Epoch 227/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 50.2508 - mse: 40.8492 - val_loss: 47.8780 - val_mse: 38.4660\n",
      "Epoch 228/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 50.6496 - mse: 41.2712 - val_loss: 51.4359 - val_mse: 42.0506\n",
      "Epoch 229/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 50.0155 - mse: 40.6266 - val_loss: 45.1118 - val_mse: 35.7221\n",
      "Epoch 230/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 49.8311 - mse: 40.4195 - val_loss: 54.0951 - val_mse: 44.7458\n",
      "Epoch 231/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 50.2331 - mse: 40.8464 - val_loss: 49.1402 - val_mse: 39.7517\n",
      "Epoch 232/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 49.7475 - mse: 40.3360 - val_loss: 49.8246 - val_mse: 40.4142\n",
      "Epoch 233/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 49.2963 - mse: 39.8888 - val_loss: 51.9621 - val_mse: 42.5420\n",
      "Epoch 234/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 50.9431 - mse: 41.5237 - val_loss: 51.3366 - val_mse: 41.8738\n",
      "Epoch 235/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 51.2418 - mse: 41.8116 - val_loss: 46.9459 - val_mse: 37.5053\n",
      "Epoch 236/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 50.1614 - mse: 40.7420 - val_loss: 47.7041 - val_mse: 38.3312\n",
      "Epoch 237/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 50.1702 - mse: 40.7500 - val_loss: 47.4050 - val_mse: 37.9906\n",
      "Epoch 238/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 50.9908 - mse: 41.5669 - val_loss: 48.2934 - val_mse: 38.8494\n",
      "Epoch 239/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 49.9760 - mse: 40.5312 - val_loss: 46.9061 - val_mse: 37.4770\n",
      "Epoch 240/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 49.9254 - mse: 40.5017 - val_loss: 49.7672 - val_mse: 40.2930\n",
      "Epoch 241/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 49.8015 - mse: 40.3589 - val_loss: 46.7522 - val_mse: 37.3197\n",
      "Epoch 242/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 50.7106 - mse: 41.2695 - val_loss: 53.1704 - val_mse: 43.6729\n",
      "Epoch 243/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 50.6104 - mse: 41.1554 - val_loss: 54.3548 - val_mse: 44.9362\n",
      "Epoch 244/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 49.2528 - mse: 39.7864 - val_loss: 48.0543 - val_mse: 38.6142\n",
      "Epoch 245/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 48.9556 - mse: 39.5161 - val_loss: 53.1953 - val_mse: 43.8089\n",
      "Epoch 246/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 49.0841 - mse: 39.6371 - val_loss: 46.8721 - val_mse: 37.4461\n",
      "Epoch 247/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 49.6414 - mse: 40.1996 - val_loss: 51.8685 - val_mse: 42.5090\n",
      "Epoch 248/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 49.6624 - mse: 40.2545 - val_loss: 54.5870 - val_mse: 45.2182\n",
      "Epoch 249/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 50.8973 - mse: 41.4684 - val_loss: 45.4541 - val_mse: 36.0201\n",
      "Epoch 250/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 49.8035 - mse: 40.3708 - val_loss: 48.8487 - val_mse: 39.4288\n",
      "Epoch 251/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 48.8220 - mse: 39.3939 - val_loss: 46.6041 - val_mse: 37.1614\n",
      "Epoch 252/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 49.7447 - mse: 40.2988 - val_loss: 50.3031 - val_mse: 40.8927\n",
      "Epoch 253/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 51.3506 - mse: 41.8938 - val_loss: 51.0743 - val_mse: 41.5376\n",
      "Epoch 254/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 49.3237 - mse: 39.8504 - val_loss: 47.8214 - val_mse: 38.4055\n",
      "Epoch 255/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 49.5437 - mse: 40.0895 - val_loss: 48.7359 - val_mse: 39.2318\n",
      "Epoch 256/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 49.7810 - mse: 40.3247 - val_loss: 50.1172 - val_mse: 40.6130\n",
      "Epoch 257/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 49.2788 - mse: 39.8218 - val_loss: 48.2527 - val_mse: 38.8102\n",
      "Epoch 258/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 51.2627 - mse: 41.7938 - val_loss: 49.5070 - val_mse: 40.0481\n",
      "Epoch 259/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 50.6566 - mse: 41.1736 - val_loss: 47.4671 - val_mse: 37.9741\n",
      "Epoch 260/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 49.2885 - mse: 39.8139 - val_loss: 48.0100 - val_mse: 38.5225\n",
      "Epoch 261/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 48.2443 - mse: 38.7690 - val_loss: 48.3294 - val_mse: 38.8776\n",
      "Epoch 262/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 49.7839 - mse: 40.3048 - val_loss: 49.1944 - val_mse: 39.7547\n",
      "Epoch 263/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 48.0783 - mse: 38.5956 - val_loss: 46.5738 - val_mse: 37.0780\n",
      "Epoch 264/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 49.4434 - mse: 39.9679 - val_loss: 46.7769 - val_mse: 37.2887\n",
      "Epoch 265/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 49.5699 - mse: 40.0728 - val_loss: 50.1624 - val_mse: 40.6818\n",
      "Epoch 266/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 49.8749 - mse: 40.3620 - val_loss: 44.4474 - val_mse: 34.9558\n",
      "Epoch 267/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 50.6081 - mse: 41.1136 - val_loss: 45.7952 - val_mse: 36.3210\n",
      "Epoch 268/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 49.4254 - mse: 39.9429 - val_loss: 48.0875 - val_mse: 38.6213\n",
      "Epoch 269/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 49.2855 - mse: 39.8025 - val_loss: 51.7086 - val_mse: 42.2743\n",
      "Epoch 270/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 47.4456 - mse: 37.9360 - val_loss: 45.4201 - val_mse: 35.9336\n",
      "Epoch 271/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 48.9712 - mse: 39.4886 - val_loss: 46.3810 - val_mse: 36.9341\n",
      "Epoch 272/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 48.5607 - mse: 39.0692 - val_loss: 47.8502 - val_mse: 38.3053\n",
      "Epoch 273/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 48.1761 - mse: 38.6882 - val_loss: 52.9621 - val_mse: 43.4696\n",
      "Epoch 274/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 50.0111 - mse: 40.5317 - val_loss: 52.7343 - val_mse: 43.2420\n",
      "Epoch 275/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 48.8090 - mse: 39.3489 - val_loss: 45.0839 - val_mse: 35.6097\n",
      "Epoch 276/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 49.0698 - mse: 39.5946 - val_loss: 50.5075 - val_mse: 40.9882\n",
      "Epoch 277/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 49.1997 - mse: 39.7116 - val_loss: 46.5799 - val_mse: 37.0609\n",
      "Epoch 278/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 48.4062 - mse: 38.8921 - val_loss: 44.7842 - val_mse: 35.2653\n",
      "Epoch 279/500\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 47.7012 - mse: 38.1854 - val_loss: 46.4705 - val_mse: 36.9902\n",
      "Epoch 280/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 48.4815 - mse: 39.0020 - val_loss: 44.1935 - val_mse: 34.7322\n",
      "Epoch 281/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 49.2992 - mse: 39.8189 - val_loss: 49.2735 - val_mse: 39.7851\n",
      "Epoch 282/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 48.7106 - mse: 39.2283 - val_loss: 44.7194 - val_mse: 35.2384\n",
      "Epoch 283/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 47.8462 - mse: 38.3483 - val_loss: 51.0595 - val_mse: 41.5557\n",
      "Epoch 284/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 48.3471 - mse: 38.8737 - val_loss: 50.5545 - val_mse: 41.1404\n",
      "Epoch 285/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 48.4073 - mse: 38.9339 - val_loss: 60.3109 - val_mse: 50.7577\n",
      "Epoch 286/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 48.5655 - mse: 39.0822 - val_loss: 59.2386 - val_mse: 49.6934\n",
      "Epoch 287/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 50.1314 - mse: 40.6350 - val_loss: 58.9544 - val_mse: 49.4987\n",
      "Epoch 288/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 49.4101 - mse: 39.8638 - val_loss: 44.9007 - val_mse: 35.3694\n",
      "Epoch 289/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 47.7953 - mse: 38.2808 - val_loss: 45.1555 - val_mse: 35.6664\n",
      "Epoch 290/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 49.3261 - mse: 39.8170 - val_loss: 46.6508 - val_mse: 37.1348\n",
      "Epoch 291/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 47.9437 - mse: 38.4348 - val_loss: 44.8545 - val_mse: 35.3031\n",
      "Epoch 292/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 47.3051 - mse: 37.7684 - val_loss: 45.6986 - val_mse: 36.1895\n",
      "Epoch 293/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 48.1877 - mse: 38.6671 - val_loss: 45.3656 - val_mse: 35.8389\n",
      "Epoch 294/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 47.5950 - mse: 38.0673 - val_loss: 46.2337 - val_mse: 36.7226\n",
      "Epoch 295/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 48.8350 - mse: 39.3285 - val_loss: 48.7688 - val_mse: 39.2987\n",
      "Epoch 296/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 47.3183 - mse: 37.8264 - val_loss: 48.1364 - val_mse: 38.6156\n",
      "Epoch 297/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 47.5887 - mse: 38.0694 - val_loss: 47.1499 - val_mse: 37.6623\n",
      "Epoch 298/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 48.3745 - mse: 38.8604 - val_loss: 46.6049 - val_mse: 37.0396\n",
      "Epoch 299/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 48.7646 - mse: 39.2182 - val_loss: 48.2920 - val_mse: 38.7496\n",
      "Epoch 300/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 48.3616 - mse: 38.8271 - val_loss: 50.7247 - val_mse: 41.1673\n",
      "Epoch 301/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 48.2025 - mse: 38.6610 - val_loss: 49.3273 - val_mse: 39.8327\n",
      "Epoch 302/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 48.4180 - mse: 38.8888 - val_loss: 44.1956 - val_mse: 34.6512\n",
      "Epoch 303/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 48.1748 - mse: 38.6464 - val_loss: 44.8706 - val_mse: 35.3666\n",
      "Epoch 304/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 48.1831 - mse: 38.6282 - val_loss: 47.3801 - val_mse: 37.8934\n",
      "Epoch 305/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 48.8093 - mse: 39.3032 - val_loss: 47.0071 - val_mse: 37.5134\n",
      "Epoch 306/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 48.4670 - mse: 38.9437 - val_loss: 50.4000 - val_mse: 40.8706\n",
      "Epoch 307/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 46.8477 - mse: 37.3163 - val_loss: 47.9555 - val_mse: 38.4025\n",
      "Epoch 308/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 47.5986 - mse: 38.0656 - val_loss: 46.0009 - val_mse: 36.4464\n",
      "Epoch 309/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 47.4302 - mse: 37.8878 - val_loss: 48.3355 - val_mse: 38.7850\n",
      "Epoch 310/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 46.7952 - mse: 37.2364 - val_loss: 47.6050 - val_mse: 38.1141\n",
      "Epoch 311/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 47.8979 - mse: 38.3866 - val_loss: 55.4054 - val_mse: 45.9331\n",
      "Epoch 312/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 48.5737 - mse: 39.0501 - val_loss: 47.2044 - val_mse: 37.6497\n",
      "Epoch 313/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 47.2074 - mse: 37.6541 - val_loss: 48.7308 - val_mse: 39.1811\n",
      "Epoch 314/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 47.9897 - mse: 38.4602 - val_loss: 48.2392 - val_mse: 38.7593\n",
      "Epoch 315/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 47.3185 - mse: 37.7993 - val_loss: 46.7541 - val_mse: 37.2680\n",
      "Epoch 316/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 48.1178 - mse: 38.6076 - val_loss: 45.1410 - val_mse: 35.6694\n",
      "Epoch 317/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 46.4902 - mse: 36.9951 - val_loss: 44.8334 - val_mse: 35.3262\n",
      "Epoch 318/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 47.0870 - mse: 37.5982 - val_loss: 46.6028 - val_mse: 37.1059\n",
      "Epoch 319/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 48.7706 - mse: 39.2751 - val_loss: 47.4095 - val_mse: 37.9421\n",
      "Epoch 320/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 48.0034 - mse: 38.4894 - val_loss: 47.2059 - val_mse: 37.6511\n",
      "Epoch 321/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 47.8203 - mse: 38.3030 - val_loss: 48.3474 - val_mse: 38.8193\n",
      "Epoch 322/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 47.9695 - mse: 38.4530 - val_loss: 48.0798 - val_mse: 38.5362\n",
      "Epoch 323/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 47.9170 - mse: 38.4026 - val_loss: 56.0606 - val_mse: 46.5691\n",
      "Epoch 324/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 46.6737 - mse: 37.1466 - val_loss: 45.6468 - val_mse: 36.1248\n",
      "Epoch 325/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 46.8749 - mse: 37.3367 - val_loss: 46.2090 - val_mse: 36.6859\n",
      "Epoch 326/500\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 46.5110 - mse: 36.9682 - val_loss: 51.1932 - val_mse: 41.6150\n",
      "Epoch 327/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 47.5049 - mse: 37.9784 - val_loss: 46.2753 - val_mse: 36.7664\n",
      "Epoch 328/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 47.7072 - mse: 38.1704 - val_loss: 60.0714 - val_mse: 50.6271\n",
      "Epoch 329/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 47.9634 - mse: 38.4367 - val_loss: 46.7585 - val_mse: 37.2049\n",
      "Epoch 330/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 47.6646 - mse: 38.1352 - val_loss: 44.4764 - val_mse: 34.9381\n",
      "Epoch 331/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 47.5895 - mse: 38.0617 - val_loss: 45.9552 - val_mse: 36.4033\n",
      "Epoch 332/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 46.8709 - mse: 37.3496 - val_loss: 45.8796 - val_mse: 36.3893\n",
      "Epoch 333/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 48.0624 - mse: 38.5252 - val_loss: 42.5608 - val_mse: 33.0200\n",
      "Epoch 334/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 47.5943 - mse: 38.0548 - val_loss: 44.8064 - val_mse: 35.2917\n",
      "Epoch 335/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 46.9329 - mse: 37.4178 - val_loss: 47.2349 - val_mse: 37.7143\n",
      "Epoch 336/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 46.8726 - mse: 37.3593 - val_loss: 43.9094 - val_mse: 34.3936\n",
      "Epoch 337/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 46.1517 - mse: 36.6392 - val_loss: 46.5339 - val_mse: 37.0497\n",
      "Epoch 338/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 46.7232 - mse: 37.1996 - val_loss: 43.4704 - val_mse: 33.9301\n",
      "Epoch 339/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 47.7882 - mse: 38.2556 - val_loss: 46.4988 - val_mse: 36.9604\n",
      "Epoch 340/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 48.1808 - mse: 38.6649 - val_loss: 54.0160 - val_mse: 44.4434\n",
      "Epoch 341/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 46.5440 - mse: 37.0286 - val_loss: 44.4796 - val_mse: 34.9757\n",
      "Epoch 342/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 46.9450 - mse: 37.4262 - val_loss: 44.7884 - val_mse: 35.2653\n",
      "Epoch 343/500\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 48.1417 - mse: 38.6056 - val_loss: 43.2195 - val_mse: 33.6918\n",
      "Epoch 344/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 46.5674 - mse: 36.9924 - val_loss: 56.3268 - val_mse: 46.6932\n",
      "Epoch 345/500\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 47.8737 - mse: 38.3191 - val_loss: 43.3847 - val_mse: 33.8610\n",
      "Epoch 346/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 47.6499 - mse: 38.1130 - val_loss: 45.6466 - val_mse: 36.1288\n",
      "Epoch 347/500\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 47.1256 - mse: 37.5884 - val_loss: 42.9843 - val_mse: 33.4425\n",
      "Epoch 348/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 46.5505 - mse: 37.0364 - val_loss: 49.7650 - val_mse: 40.2931\n",
      "Epoch 349/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 47.7827 - mse: 38.2564 - val_loss: 45.0755 - val_mse: 35.5598\n",
      "Epoch 350/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 46.2028 - mse: 36.6479 - val_loss: 48.9500 - val_mse: 39.4026\n",
      "Epoch 351/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 48.0111 - mse: 38.4351 - val_loss: 57.3254 - val_mse: 47.6847\n",
      "Epoch 352/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 47.9847 - mse: 38.4198 - val_loss: 47.4563 - val_mse: 37.8669\n",
      "Epoch 353/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 45.3923 - mse: 35.8368 - val_loss: 44.3390 - val_mse: 34.7843\n",
      "Epoch 354/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 47.1251 - mse: 37.5824 - val_loss: 47.2156 - val_mse: 37.6670\n",
      "Epoch 355/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 46.6238 - mse: 37.0566 - val_loss: 54.1028 - val_mse: 44.5389\n",
      "Epoch 356/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 46.6136 - mse: 37.0505 - val_loss: 47.2422 - val_mse: 37.6737\n",
      "Epoch 357/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 47.3015 - mse: 37.7352 - val_loss: 46.7723 - val_mse: 37.2523\n",
      "Epoch 358/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 47.0213 - mse: 37.4652 - val_loss: 51.1451 - val_mse: 41.5104\n",
      "Epoch 359/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 47.2891 - mse: 37.7056 - val_loss: 45.4251 - val_mse: 35.8616\n",
      "Epoch 360/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 46.9938 - mse: 37.4214 - val_loss: 50.8163 - val_mse: 41.2107\n",
      "Epoch 361/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 46.8305 - mse: 37.2579 - val_loss: 47.3333 - val_mse: 37.7216\n",
      "Epoch 362/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 46.3060 - mse: 36.7339 - val_loss: 46.5601 - val_mse: 36.9457\n",
      "Epoch 363/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 47.2795 - mse: 37.6922 - val_loss: 45.0904 - val_mse: 35.5202\n",
      "Epoch 364/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 46.6157 - mse: 37.0280 - val_loss: 44.1079 - val_mse: 34.5082\n",
      "Epoch 365/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 46.7388 - mse: 37.1357 - val_loss: 55.8334 - val_mse: 46.2881\n",
      "Epoch 366/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 48.2224 - mse: 38.6247 - val_loss: 45.0699 - val_mse: 35.4521\n",
      "Epoch 367/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 45.9503 - mse: 36.3424 - val_loss: 50.0101 - val_mse: 40.4773\n",
      "Epoch 368/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 46.7382 - mse: 37.1554 - val_loss: 47.2925 - val_mse: 37.7146\n",
      "Epoch 369/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 46.1433 - mse: 36.5676 - val_loss: 44.5533 - val_mse: 34.9860\n",
      "Epoch 370/500\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 46.8107 - mse: 37.2254 - val_loss: 44.5796 - val_mse: 34.9906\n",
      "Epoch 371/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 46.2071 - mse: 36.6096 - val_loss: 45.1998 - val_mse: 35.5807\n",
      "Epoch 372/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 46.1766 - mse: 36.5996 - val_loss: 45.6909 - val_mse: 36.0894\n",
      "Epoch 373/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 46.5611 - mse: 36.9508 - val_loss: 43.8199 - val_mse: 34.2122\n",
      "Epoch 374/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 47.0027 - mse: 37.4022 - val_loss: 50.6848 - val_mse: 41.0202\n",
      "Epoch 375/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 47.4034 - mse: 37.7867 - val_loss: 49.1276 - val_mse: 39.4961\n",
      "Epoch 376/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 45.8220 - mse: 36.2054 - val_loss: 44.0084 - val_mse: 34.3930\n",
      "Epoch 377/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 46.3116 - mse: 36.7138 - val_loss: 44.2052 - val_mse: 34.6216\n",
      "Epoch 378/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 46.6967 - mse: 37.1038 - val_loss: 44.6884 - val_mse: 35.0971\n",
      "Epoch 379/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 45.3137 - mse: 35.7132 - val_loss: 43.2180 - val_mse: 33.6149\n",
      "Epoch 380/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 44.3551 - mse: 34.7553 - val_loss: 45.8792 - val_mse: 36.2855\n",
      "Epoch 381/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 46.6167 - mse: 37.0266 - val_loss: 48.3708 - val_mse: 38.7538\n",
      "Epoch 382/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 48.0620 - mse: 38.4838 - val_loss: 45.7976 - val_mse: 36.2632\n",
      "Epoch 383/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 46.3916 - mse: 36.8077 - val_loss: 47.2584 - val_mse: 37.6428\n",
      "Epoch 384/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 45.6705 - mse: 36.0675 - val_loss: 46.3723 - val_mse: 36.7264\n",
      "Epoch 385/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 46.3604 - mse: 36.7781 - val_loss: 44.1073 - val_mse: 34.5198\n",
      "Epoch 386/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 46.3015 - mse: 36.6965 - val_loss: 49.5799 - val_mse: 40.0233\n",
      "Epoch 387/500\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 45.8603 - mse: 36.2738 - val_loss: 42.9140 - val_mse: 33.3147\n",
      "Epoch 388/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 45.5784 - mse: 35.9889 - val_loss: 42.9763 - val_mse: 33.3501\n",
      "Epoch 389/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 46.0724 - mse: 36.4804 - val_loss: 53.6875 - val_mse: 44.0550\n",
      "Epoch 390/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 46.8470 - mse: 37.2645 - val_loss: 44.5875 - val_mse: 34.9878\n",
      "Epoch 391/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 44.8780 - mse: 35.2914 - val_loss: 47.3891 - val_mse: 37.8473\n",
      "Epoch 392/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 45.5323 - mse: 35.9541 - val_loss: 42.9033 - val_mse: 33.3206\n",
      "Epoch 393/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 46.7224 - mse: 37.1401 - val_loss: 47.7486 - val_mse: 38.1884\n",
      "Epoch 394/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 45.1518 - mse: 35.5637 - val_loss: 44.6616 - val_mse: 35.0930\n",
      "Epoch 395/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 46.1378 - mse: 36.5552 - val_loss: 46.5018 - val_mse: 36.9085\n",
      "Epoch 396/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 47.4420 - mse: 37.8416 - val_loss: 46.8027 - val_mse: 37.2094\n",
      "Epoch 397/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 45.3929 - mse: 35.7963 - val_loss: 43.1184 - val_mse: 33.5090\n",
      "Epoch 398/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 48.6176 - mse: 39.0172 - val_loss: 50.1885 - val_mse: 40.5042\n",
      "Epoch 399/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 45.0909 - mse: 35.4984 - val_loss: 44.4994 - val_mse: 34.8954\n",
      "Epoch 400/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 45.5880 - mse: 36.0017 - val_loss: 46.4531 - val_mse: 36.8402\n",
      "Epoch 401/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 45.8269 - mse: 36.2555 - val_loss: 52.9907 - val_mse: 43.3797\n",
      "Epoch 402/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 45.5457 - mse: 35.9700 - val_loss: 43.3299 - val_mse: 33.7684\n",
      "Epoch 403/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 45.9121 - mse: 36.3128 - val_loss: 46.0419 - val_mse: 36.4249\n",
      "Epoch 404/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 46.0398 - mse: 36.4606 - val_loss: 44.1194 - val_mse: 34.5227\n",
      "Epoch 405/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 47.0310 - mse: 37.4386 - val_loss: 44.6986 - val_mse: 35.1212\n",
      "Epoch 406/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 45.6316 - mse: 36.0277 - val_loss: 47.3926 - val_mse: 37.7935\n",
      "Epoch 407/500\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 46.5612 - mse: 36.9521 - val_loss: 47.1565 - val_mse: 37.5093\n",
      "Epoch 408/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 43.9464 - mse: 34.3187 - val_loss: 46.6490 - val_mse: 37.0308\n",
      "Epoch 409/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 46.0837 - mse: 36.4943 - val_loss: 43.6600 - val_mse: 34.0678\n",
      "Epoch 410/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 45.1744 - mse: 35.5700 - val_loss: 46.6224 - val_mse: 36.9995\n",
      "Epoch 411/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 45.8010 - mse: 36.1895 - val_loss: 43.5656 - val_mse: 33.9388\n",
      "Epoch 412/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 44.5951 - mse: 34.9639 - val_loss: 42.9796 - val_mse: 33.3535\n",
      "Epoch 413/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 46.2432 - mse: 36.6166 - val_loss: 45.5941 - val_mse: 36.0378\n",
      "Epoch 414/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 45.4679 - mse: 35.8891 - val_loss: 46.7329 - val_mse: 37.1367\n",
      "Epoch 415/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 45.0058 - mse: 35.4295 - val_loss: 44.5391 - val_mse: 34.9127\n",
      "Epoch 416/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 45.6302 - mse: 36.0066 - val_loss: 45.0429 - val_mse: 35.4149\n",
      "Epoch 417/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 46.9758 - mse: 37.3482 - val_loss: 43.3321 - val_mse: 33.7228\n",
      "Epoch 418/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 45.3955 - mse: 35.7703 - val_loss: 47.4001 - val_mse: 37.7483\n",
      "Epoch 419/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 44.2004 - mse: 34.6083 - val_loss: 45.6155 - val_mse: 36.0350\n",
      "Epoch 420/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 45.8790 - mse: 36.3094 - val_loss: 43.3126 - val_mse: 33.7379\n",
      "Epoch 421/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 45.9450 - mse: 36.3408 - val_loss: 46.2561 - val_mse: 36.6467\n",
      "Epoch 422/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 46.3636 - mse: 36.7405 - val_loss: 43.7252 - val_mse: 34.1088\n",
      "Epoch 423/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 46.4290 - mse: 36.8419 - val_loss: 47.3462 - val_mse: 37.7266\n",
      "Epoch 424/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 44.7364 - mse: 35.1525 - val_loss: 41.9891 - val_mse: 32.3852\n",
      "Epoch 425/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 44.0628 - mse: 34.4930 - val_loss: 43.7748 - val_mse: 34.2043\n",
      "Epoch 426/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 44.4104 - mse: 34.8440 - val_loss: 49.5938 - val_mse: 40.0367\n",
      "Epoch 427/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 45.5217 - mse: 35.9337 - val_loss: 45.1751 - val_mse: 35.5777\n",
      "Epoch 428/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 45.5096 - mse: 35.9364 - val_loss: 46.4822 - val_mse: 36.9508\n",
      "Epoch 429/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 47.4120 - mse: 37.8453 - val_loss: 44.2832 - val_mse: 34.6958\n",
      "Epoch 430/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 46.0171 - mse: 36.4272 - val_loss: 44.2994 - val_mse: 34.7001\n",
      "Epoch 431/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 46.2454 - mse: 36.6393 - val_loss: 44.0723 - val_mse: 34.4706\n",
      "Epoch 432/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 44.9245 - mse: 35.3498 - val_loss: 45.4493 - val_mse: 35.9121\n",
      "Epoch 433/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 45.1170 - mse: 35.5424 - val_loss: 43.4039 - val_mse: 33.8144\n",
      "Epoch 434/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 46.2629 - mse: 36.6598 - val_loss: 46.1879 - val_mse: 36.5901\n",
      "Epoch 435/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 45.3396 - mse: 35.7350 - val_loss: 48.5625 - val_mse: 39.0361\n",
      "Epoch 436/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 44.7194 - mse: 35.1306 - val_loss: 43.7596 - val_mse: 34.1882\n",
      "Epoch 437/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 45.1170 - mse: 35.5203 - val_loss: 43.7646 - val_mse: 34.1880\n",
      "Epoch 438/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 45.4804 - mse: 35.9057 - val_loss: 43.5387 - val_mse: 33.9270\n",
      "Epoch 439/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 46.8924 - mse: 37.2841 - val_loss: 55.7341 - val_mse: 46.0879\n",
      "Epoch 440/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 45.7735 - mse: 36.1822 - val_loss: 45.7631 - val_mse: 36.1873\n",
      "Epoch 441/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 44.6175 - mse: 35.0166 - val_loss: 41.4245 - val_mse: 31.8409\n",
      "Epoch 442/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 44.9353 - mse: 35.3672 - val_loss: 45.9647 - val_mse: 36.3842\n",
      "Epoch 443/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 44.6716 - mse: 35.1015 - val_loss: 45.7119 - val_mse: 36.1186\n",
      "Epoch 444/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 45.4678 - mse: 35.8881 - val_loss: 41.4367 - val_mse: 31.8581\n",
      "Epoch 445/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 44.9091 - mse: 35.3428 - val_loss: 42.4829 - val_mse: 32.9595\n",
      "Epoch 446/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 44.4349 - mse: 34.8817 - val_loss: 42.3209 - val_mse: 32.7740\n",
      "Epoch 447/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 44.4898 - mse: 34.9442 - val_loss: 51.8370 - val_mse: 42.2832\n",
      "Epoch 448/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 43.6946 - mse: 34.1471 - val_loss: 49.9897 - val_mse: 40.4985\n",
      "Epoch 449/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 45.9404 - mse: 36.4050 - val_loss: 41.4547 - val_mse: 31.9175\n",
      "Epoch 450/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 46.0275 - mse: 36.4750 - val_loss: 44.4200 - val_mse: 34.8348\n",
      "Epoch 451/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 43.2948 - mse: 33.7340 - val_loss: 46.3323 - val_mse: 36.7642\n",
      "Epoch 452/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 45.8255 - mse: 36.2680 - val_loss: 47.0674 - val_mse: 37.4796\n",
      "Epoch 453/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 44.3235 - mse: 34.7715 - val_loss: 45.3869 - val_mse: 35.9027\n",
      "Epoch 454/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 44.4431 - mse: 34.9320 - val_loss: 43.5784 - val_mse: 34.1113\n",
      "Epoch 455/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 45.4948 - mse: 35.9817 - val_loss: 44.1588 - val_mse: 34.6439\n",
      "Epoch 456/500\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 45.2573 - mse: 35.7401 - val_loss: 42.9526 - val_mse: 33.4536\n",
      "Epoch 457/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 44.9782 - mse: 35.4446 - val_loss: 44.7951 - val_mse: 35.2235\n",
      "Epoch 458/500\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 43.9558 - mse: 34.4074 - val_loss: 44.3652 - val_mse: 34.7996\n",
      "Epoch 459/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 44.1151 - mse: 34.5466 - val_loss: 44.3004 - val_mse: 34.7108\n",
      "Epoch 460/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 45.3952 - mse: 35.8134 - val_loss: 44.4449 - val_mse: 34.8443\n",
      "Epoch 461/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 44.0790 - mse: 34.5091 - val_loss: 45.2061 - val_mse: 35.6122\n",
      "Epoch 462/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 44.4132 - mse: 34.8637 - val_loss: 44.8030 - val_mse: 35.2315\n",
      "Epoch 463/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 43.8669 - mse: 34.3152 - val_loss: 42.3115 - val_mse: 32.7791\n",
      "Epoch 464/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 44.7996 - mse: 35.2504 - val_loss: 43.8550 - val_mse: 34.2950\n",
      "Epoch 465/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 43.7866 - mse: 34.2556 - val_loss: 42.9758 - val_mse: 33.4699\n",
      "Epoch 466/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 44.5276 - mse: 35.0075 - val_loss: 42.0434 - val_mse: 32.4898\n",
      "Epoch 467/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 44.2942 - mse: 34.7550 - val_loss: 44.3916 - val_mse: 34.8411\n",
      "Epoch 468/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 45.2302 - mse: 35.6514 - val_loss: 41.3260 - val_mse: 31.7578\n",
      "Epoch 469/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 44.8582 - mse: 35.3095 - val_loss: 43.3328 - val_mse: 33.7595\n",
      "Epoch 470/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 44.5567 - mse: 34.9835 - val_loss: 45.7266 - val_mse: 36.1688\n",
      "Epoch 471/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 48.0626 - mse: 38.5314 - val_loss: 44.7357 - val_mse: 35.2029\n",
      "Epoch 472/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 46.9789 - mse: 37.4689 - val_loss: 42.9787 - val_mse: 33.4983\n",
      "Epoch 473/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 44.6266 - mse: 35.1140 - val_loss: 51.6131 - val_mse: 42.0499\n",
      "Epoch 474/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 44.3321 - mse: 34.8060 - val_loss: 41.7185 - val_mse: 32.1936\n",
      "Epoch 475/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 44.3743 - mse: 34.8278 - val_loss: 42.1344 - val_mse: 32.5763\n",
      "Epoch 476/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 43.0942 - mse: 33.5461 - val_loss: 50.6441 - val_mse: 41.1535\n",
      "Epoch 477/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 43.7603 - mse: 34.2213 - val_loss: 44.3767 - val_mse: 34.8541\n",
      "Epoch 478/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 43.7188 - mse: 34.1849 - val_loss: 42.3540 - val_mse: 32.8059\n",
      "Epoch 479/500\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 44.2667 - mse: 34.7483 - val_loss: 57.1884 - val_mse: 47.5947\n",
      "Epoch 480/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 43.5166 - mse: 33.9746 - val_loss: 42.2198 - val_mse: 32.6867\n",
      "Epoch 481/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 44.1918 - mse: 34.6539 - val_loss: 40.4173 - val_mse: 30.8965\n",
      "Epoch 482/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 43.4790 - mse: 33.9693 - val_loss: 42.6197 - val_mse: 33.1455\n",
      "Epoch 483/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 44.6118 - mse: 35.1142 - val_loss: 42.9766 - val_mse: 33.4777\n",
      "Epoch 484/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 44.0564 - mse: 34.5547 - val_loss: 42.7698 - val_mse: 33.2877\n",
      "Epoch 485/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 44.6309 - mse: 35.1438 - val_loss: 47.3935 - val_mse: 37.9551\n",
      "Epoch 486/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 45.4049 - mse: 35.8975 - val_loss: 42.2342 - val_mse: 32.7002\n",
      "Epoch 487/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 44.0728 - mse: 34.5300 - val_loss: 42.0894 - val_mse: 32.5698\n",
      "Epoch 488/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 43.8960 - mse: 34.3629 - val_loss: 43.8216 - val_mse: 34.2952\n",
      "Epoch 489/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 43.6177 - mse: 34.0939 - val_loss: 43.2731 - val_mse: 33.7387\n",
      "Epoch 490/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 45.3442 - mse: 35.7985 - val_loss: 45.3602 - val_mse: 35.8590\n",
      "Epoch 491/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 44.1857 - mse: 34.6608 - val_loss: 41.4165 - val_mse: 31.8761\n",
      "Epoch 492/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 43.6158 - mse: 34.0905 - val_loss: 49.7909 - val_mse: 40.3126\n",
      "Epoch 493/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 44.3305 - mse: 34.7857 - val_loss: 42.9960 - val_mse: 33.4269\n",
      "Epoch 494/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 43.8415 - mse: 34.2998 - val_loss: 42.2774 - val_mse: 32.7540\n",
      "Epoch 495/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 43.6684 - mse: 34.1533 - val_loss: 41.9243 - val_mse: 32.4121\n",
      "Epoch 496/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 43.4382 - mse: 33.9193 - val_loss: 52.2944 - val_mse: 42.7085\n",
      "Epoch 497/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 43.9561 - mse: 34.4157 - val_loss: 46.1795 - val_mse: 36.6681\n",
      "Epoch 498/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 43.8437 - mse: 34.3091 - val_loss: 41.5572 - val_mse: 32.0420\n",
      "Epoch 499/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 45.0148 - mse: 35.4816 - val_loss: 49.6273 - val_mse: 40.1118\n",
      "Epoch 500/500\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 44.4520 - mse: 34.8861 - val_loss: 43.5974 - val_mse: 34.0414\n"
     ]
    }
   ],
   "source": [
    "# fit\n",
    "a_history_nn = a_model_nn.fit(a_X_nn_train_sc, a_y_nn_train, epochs = 500, verbose = 1, \n",
    "                          validation_data = (a_X_nn_test_sc, a_y_nn_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b6adc2a6-f40b-4657-a166-7c4c4ddec36d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x360 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#breakfast hour\n",
    "# plot train and test loss (mse)\n",
    "fig, ax = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "ax[0].plot(a_history_nn.history['loss'], color = 'green')\n",
    "ax[1].plot(a_history_nn.history['val_loss'], color = 'blue')\n",
    "\n",
    "ax[0].set_title('Train')\n",
    "ax[0].set_xlabel('epochs')\n",
    "ax[0].set_ylabel('Loss = MSE')\n",
    "\n",
    "ax[1].set_title('Test')\n",
    "ax[1].set_xlabel('epochs')\n",
    "ax[1].set_ylabel('Loss = MSE');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f7a59700-0e54-4489-b2d9-6ac968862dab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 0s 956us/step\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'metrics' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/5f/3tr4k1qn1fx81cd1l2y9j2100000gn/T/ipykernel_50486/3575261667.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# R2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mR2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mr2_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma_nn_y_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma_nn_y_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'R2: {R2}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'metrics' is not defined"
     ]
    }
   ],
   "source": [
    "# https://medium.com/towards-data-science/loss-functions-and-their-use-in-neural-networks-a470e703f1e9\n",
    "a_nn_y_true = a_y_nn_test\n",
    "a_nn_y_pred = a_model_nn.predict(a_X_nn_test_sc)\n",
    "\n",
    "# R2\n",
    "R2 = metrics.r2_score(a_nn_y_true, a_nn_y_pred)\n",
    "print(f'R2: {R2}')\n",
    "\n",
    "# MAE\n",
    "mae = MeanAbsoluteError()\n",
    "print(f'Mean Absolute Error: {mae(a_nn_y_true, a_nn_y_pred)}')\n",
    "\n",
    "# Huber\n",
    "huber = Huber()\n",
    "print(f'Huber Loss: {huber(a_nn_y_true, a_nn_y_pred)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8edaf62c-ecf7-4fee-975b-fee70de28981",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model\n",
    "# a_model_nn.save('../data/average/a_model_nn.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d0f289d-1569-471e-a074-5c8321fcb722",
   "metadata": {},
   "source": [
    "# Average Cycling Performance Best Model\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a227fa4d-a3fb-4bbe-8fdb-4839b2e441de",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
